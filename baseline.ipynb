{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install https://github.com/pyg-team/pytorch_geometric.git","metadata":{"id":"xSkgt1zf-raF","outputId":"59f4a52f-5eb4-41e5-9fba-07432989fe78","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git","metadata":{"id":"5oR2D2Us-xSQ","outputId":"7086cadf-a7fe-4d75-f271-6339bee8164d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd hackaton/","metadata":{"id":"tEhfPly6-7UK","outputId":"3078ee06-6312-4fca-f5f9-888fa628c80a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n","metadata":{"id":"PxBvwB0_6xI8","outputId":"5933387c-2cfb-474f-d842-f36a3e2d2a73","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh datasets","metadata":{"id":"1rockhiQ7Nny","outputId":"2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import random_split\n# Load utility functions from cloned repository\nfrom src.loadData import GraphDataset\nfrom src.utils import set_seed\nfrom src.models import GNN\nimport argparse\n\n# Set the random seed\nset_seed()\n","metadata":{"id":"lAQuCuIoBbq5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_zeros(data):\n    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n    return data","metadata":{"id":"Dyf0I2-t9IcW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total += data.y.size(0)\n\n    # Save checkpoints if required\n    if save_checkpoints:\n        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n        torch.save(model.state_dict(), checkpoint_file)\n        print(f\"Checkpoint saved at {checkpoint_file}\")\n\n    return total_loss / len(data_loader),  correct / total","metadata":{"id":"3jKvoQYI9Zbc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(data_loader, model, device, calculate_accuracy=False):\n    model.eval()\n    correct = 0\n    total = 0\n    predictions = []\n    total_loss = 0\n    criterion = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n            data = data.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if calculate_accuracy:\n                correct += (pred == data.y).sum().item()\n                total += data.y.size(0)\n                total_loss += criterion(output, data.y).item()\n            else:\n                predictions.extend(pred.cpu().numpy())\n    if calculate_accuracy:\n        accuracy = correct / total\n        return  total_loss / len(data_loader),accuracy\n    return predictions","metadata":{"id":"8peFiIS19ZpK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_predictions(predictions, test_path):\n    script_dir = os.getcwd() \n    submission_folder = os.path.join(script_dir, \"submission\")\n    test_dir_name = os.path.basename(os.path.dirname(test_path))\n    \n    os.makedirs(submission_folder, exist_ok=True)\n    \n    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n    \n    test_graph_ids = list(range(len(predictions)))\n    output_df = pd.DataFrame({\n        \"id\": test_graph_ids,\n        \"pred\": predictions\n    })\n    \n    output_df.to_csv(output_csv_path, index=False)\n    print(f\"Predictions saved to {output_csv_path}\")","metadata":{"id":"WanuZKxy9Zs-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_progress(train_losses, train_accuracies, output_dir):\n    epochs = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(12, 6))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Loss per Epoch')\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training Accuracy per Epoch')\n\n    # Save plots in the current directory\n    os.makedirs(output_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n    plt.close()","metadata":{"id":"uyHIJS5U9ZzB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_user_input(prompt, default=None, required=False, type_cast=str):\n\n    while True:\n        user_input = input(f\"{prompt} [{default}]: \")\n        \n        if user_input == \"\" and required:\n            print(\"This field is required. Please enter a value.\")\n            continue\n        \n        if user_input == \"\" and default is not None:\n            return default\n        \n        if user_input == \"\" and not required:\n            return None\n        \n        try:\n            return type_cast(user_input)\n        except ValueError:\n            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_arguments():\n    args = {}\n    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n\n    \n    return argparse.Namespace(**args)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def populate_args(args):\n    print(\"Arguments received:\")\n    for key, value in vars(args).items():\n        print(f\"{key}: {value}\")\nargs = get_arguments()\npopulate_args(args)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NoisyCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, p_noisy):\n        super().__init__()\n        self.p = p_noisy\n        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        losses = self.ce(logits, targets)\n        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n        return (losses * weights).mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"script_dir = os.getcwd() \n# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n    \nif args.gnn == 'gin':\n    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\nelif args.gnn == 'gin-virtual':\n    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\nelif args.gnn == 'gcn':\n    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\nelif args.gnn == 'gcn-virtual':\n    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\nelse:\n    raise ValueError('Invalid GNN type')\n    \noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# criterion = torch.nn.CrossEntropyLoss()\nif args.baseline_mode == 2:\n    criterion = NoisyCrossEntropyLoss(args.noise_prob)\nelse:\n    criterion = torch.nn.CrossEntropyLoss()","metadata":{"id":"lHX55XGECXBr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dir_name = os.path.basename(os.path.dirname(args.test_path))\nlogs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\nlog_file = os.path.join(logs_folder, \"training.log\")\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\nlogging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\nlogging.getLogger().addHandler(logging.StreamHandler())\n\ncheckpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\ncheckpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\nos.makedirs(checkpoints_folder, exist_ok=True)\n","metadata":{"id":"BTYT5jYuChPb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.path.exists(checkpoint_path) and not args.train_path:\n    model.load_state_dict(torch.load(checkpoint_path))\n    print(f\"Loaded best model from {checkpoint_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if args.train_path:\n    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n    val_size = int(0.2 * len(full_dataset))\n    train_size = len(full_dataset) - val_size\n\n    \n    generator = torch.Generator().manual_seed(12)\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n\n    num_epochs = args.epochs\n    best_val_accuracy = 0.0   \n\n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n\n    if num_checkpoints > 1:\n        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n    else:\n        checkpoint_intervals = [num_epochs]\n\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train(\n            train_loader, model, optimizer, criterion, device,\n            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n            current_epoch=epoch\n        )\n\n        val_loss,val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n        train_losses.append(train_loss)\n        train_accuracies.append(train_acc)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n\n        \n        if val_acc > best_val_accuracy:\n            best_val_accuracy = val_acc\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f\"Best model updated and saved at {checkpoint_path}\")\n\n    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ndel train_dataset\ndel train_loader\ndel full_dataset\ndel val_dataset\ndel val_loader\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = GraphDataset(args.test_path, transform=add_zeros)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n    ","metadata":{"id":"xsXZIj4Mdu3I","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(checkpoint_path))\npredictions = evaluate(test_loader, model, device, calculate_accuracy=False)\nsave_predictions(predictions, args.test_path)","metadata":{"id":"x1OnGq_nCmTr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}