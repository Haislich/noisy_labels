{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from functools import partial\n",
    "import optuna\n",
    "import gc\n",
    "from typing import Literal\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    save_checkpoints,\n",
    "    checkpoint_path,\n",
    "    current_epoch,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return total_loss / len(data_loader), accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\"id\": test_graph_ids, \"pred\": predictions})\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy per Epoch\")\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (\n",
    "            1\n",
    "            - torch.nn.functional.one_hot(targets, num_classes=logits.size(1))\n",
    "            .float()\n",
    "            .sum(dim=1)\n",
    "        )\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCELoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 6, alpha: float = 0.1, beta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # CCE\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "\n",
    "        # RCE\n",
    "        pred = F.softmax(logits, dim=1).clamp(min=1e-6, max=1 - 1e-6)\n",
    "        one_hot = F.one_hot(targets, self.num_classes).float()\n",
    "        rce = -(1 - one_hot) * torch.log(1 - pred)\n",
    "        rce = rce.sum(dim=1)\n",
    "        return (self.alpha * ce + self.beta * rce).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3521734483.py, line 56)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef forward(self,logits: torch.Tensor), :\u001b[39m\n                                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class NCODLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        embedding_dimensions: int = 300,\n",
    "        total_epochs: int = 150,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        label_counts = {}\n",
    "        for elem in dataset:\n",
    "            y = elem.y.item()\n",
    "            label_counts[y] = label_counts.get(y, 0) + 1\n",
    "\n",
    "        self.num_elements = len(dataset)\n",
    "        self.num_classes = len(label_counts)\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.total_epochs = total_epochs\n",
    "\n",
    "        # u_i: Learnable noise discount scalar per sample\n",
    "        # This parameter is updated during training to reflect the estimated noisiness of each sample.\n",
    "        # If u_i grows, the loss becomes less sensitive to this sample.\n",
    "        # Initialized near zero (assumes most samples are initially clean).\n",
    "        self.u = nn.Parameter(torch.empty(self.num_elements, 1))\n",
    "        torch.nn.init.normal_(self.u, mean=1e-8, std=1e-9)\n",
    "\n",
    "        # Buffer for past feature embeddings, used for centroid computation\n",
    "        # Shape: [num_samples, embedding_dimensions]\n",
    "        # These are updated each time a sample appears in a batch\n",
    "        self.past_embeddings = torch.rand((self.num_elements, embedding_dimensions))\n",
    "\n",
    "        # Per-class centroids. used to compute soft labels\n",
    "        # Initialized randomly and updated each epoch using confident samples\n",
    "        self.centroids = torch.rand((self.num_classes, embedding_dimensions))\n",
    "\n",
    "        # Bins: stores indices for each class neded for centroid computation\n",
    "        # self.bins[i] will contain the list of dataset indices where label == i\n",
    "        self.bins = torch.tensor(sorted(list(label_counts.items())), dtype = torch.uint32)\n",
    "        # ratio_consistency: strength of consistency regularization between model outputs\n",
    "        # Helps enforce prediction stability across augmentations or dropout\n",
    "        # self.ratio_consistency = ratio_consistency\n",
    "\n",
    "        # ratio_balance: strength of class balance regularization (KL divergence to uniform prior)\n",
    "        # Helps prevent the model from collapsing to over-predicting certain classes under label noise\n",
    "        # Tune if class imbalance becomes a problem or you're using soft labels heavily.\n",
    "        # self.ratio_balance = ratio_balance\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        indexes: torch.Tensor,\n",
    "        embeddings: torch.Tensor,\n",
    "        epoch: int,\n",
    "    ):\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        if epoch ==0:\n",
    "            percent = math.ceil((50 - (50 / self.total_epochs) * epoch) + 50)\n",
    "            u_class= self.u.numpy()[indexes.numpy()]\n",
    "            bottom_k = int((len(u_class) / 100) * percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_checkpoints,\n",
    "    checkpoints_dir,\n",
    "    run_name,\n",
    "    best_model_path,\n",
    "    logs_dir,\n",
    "    *,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):  # -> float | Any:\n",
    "    logging.info(\"#\" * 80)\n",
    "    # Hyperparameter search space\n",
    "    logging.info(\"Start case study with parameters:\")\n",
    "    gnn_type = trial.suggest_categorical(\n",
    "        \"gnn_type\", [\"gin\", \"gin-virtual\", \"gcn\", \"gcn-virtual\"]\n",
    "    )\n",
    "    drop_ratio = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 3, 6)\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 300, 600])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 40, 80, step=20)\n",
    "\n",
    "    logging.info(f\"{gnn_type=}\")\n",
    "    logging.info(f\"{drop_ratio=}\")\n",
    "    logging.info(f\"{num_layers=}\")\n",
    "    logging.info(f\"{embedding_dim=}\")\n",
    "    logging.info(f\"{num_epochs=}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNN(\n",
    "        gnn_type=\"gin\" if \"gin\" in gnn_type else \"gcn\",\n",
    "        num_class=6,\n",
    "        num_layer=num_layers,\n",
    "        emb_dim=embedding_dim,\n",
    "        drop_ratio=drop_ratio,\n",
    "        virtual_node=\"virtual\" in gnn_type,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = SCELoss()  # torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Prepare checkpoints\n",
    "    checkpoint_epochs = [\n",
    "        int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)\n",
    "    ]\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_epochs),\n",
    "            checkpoint_path=os.path.join(checkpoints_dir, f\"model_{run_name}\"),\n",
    "            current_epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        msg = (\n",
    "            f\"[{run_name}] Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "        logging.info(msg)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            logging.info(f\"[{run_name}] Best model updated at {best_model_path}\")\n",
    "\n",
    "    plot_training_progress(\n",
    "        train_losses, train_accuracies, os.path.join(logs_dir, \"train_plots\")\n",
    "    )\n",
    "    plot_training_progress(\n",
    "        val_losses, val_accuracies, os.path.join(logs_dir, \"val_plots\")\n",
    "    )\n",
    "    logging.info(f\"Case study end, {best_val_accuracy}\")\n",
    "    logging.info(\"#\" * 80)\n",
    "    logging.info(\"\\n\")\n",
    "\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_study(\n",
    "    dataset_name: Literal[\"A\", \"B\", \"C\", \"D\"],\n",
    "    n_trials: int = 30,\n",
    "    resume_if_exists: bool = True,\n",
    "    num_checkpoints: int = 10,\n",
    "    default_batch_size: int = 32,\n",
    "):\n",
    "    script_root = os.getcwd()\n",
    "    train_path = f\"./datasets/{dataset_name}/train.json.gz\"\n",
    "    run_name = dataset_name\n",
    "\n",
    "    logs_dir = os.path.join(script_root, \"logs\", run_name)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_dir, \"training.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "    checkpoints_dir = os.path.join(script_root, \"checkpoints\", run_name)\n",
    "    best_model_path = os.path.join(checkpoints_dir, f\"model_{run_name}_best.pth\")\n",
    "    summary_csv_path = os.path.join(logs_dir, f\"optuna_summary_{dataset_name}.csv\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,  # type:ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,  # type:ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"--- Starting Optuna optimization for dataset {dataset_name} ---\")\n",
    "\n",
    "    study = optuna.create_study(study_name=run_name, direction=\"maximize\")\n",
    "\n",
    "    obj = partial(\n",
    "        objective,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_checkpoints=num_checkpoints,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        run_name=run_name,\n",
    "        best_model_path=best_model_path,\n",
    "        logs_dir=logs_dir,\n",
    "    )\n",
    "    study.optimize(obj, n_trials=n_trials)\n",
    "\n",
    "    all_trials = []\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            row = {\"accuracy\": trial.value}\n",
    "            row.update(trial.params)\n",
    "            all_trials.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_trials)\n",
    "    results_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    logging.info(f\"\\nAll trials saved to: {summary_csv_path}\")\n",
    "    logging.info(f\"\\nBest result for dataset {dataset_name}:\")\n",
    "    display(results_df.sort_values(\"accuracy\", ascending=False))\n",
    "    logging.info(f\"\\nBest Params for {dataset_name}:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Starting Optuna optimization for dataset A ---\n",
      "[I 2025-05-25 19:03:44,730] A new study created in memory with name: A\n",
      "################################################################################\n",
      "Start case study with parameters:\n",
      "gnn_type='gcn'\n",
      "drop_ratio=0.25287334651446963\n",
      "num_layers=6\n",
      "embedding_dim=64\n",
      "num_epochs=60\n",
      "Epoch:   0%|          | 0/60 [00:00<?, ?it/s][A] Epoch 1/60 | Train Loss: 1.0281, Acc: 0.3729 | Val Loss: 1.5257, Acc: 0.4034\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   2%|▏         | 1/60 [00:28<28:05, 28.57s/it][A] Epoch 2/60 | Train Loss: 0.9751, Acc: 0.4359 | Val Loss: 1.5691, Acc: 0.4047\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   3%|▎         | 2/60 [00:52<25:05, 25.97s/it][A] Epoch 3/60 | Train Loss: 0.9505, Acc: 0.4662 | Val Loss: 1.3864, Acc: 0.5084\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   5%|▌         | 3/60 [01:16<23:52, 25.14s/it][A] Epoch 4/60 | Train Loss: 0.9289, Acc: 0.4940 | Val Loss: 1.3738, Acc: 0.5018\n",
      "Epoch:   7%|▋         | 4/60 [01:41<23:07, 24.78s/it][A] Epoch 5/60 | Train Loss: 0.9122, Acc: 0.5131 | Val Loss: 1.3284, Acc: 0.5381\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   8%|▊         | 5/60 [02:04<22:24, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 6/60 | Train Loss: 0.8982, Acc: 0.5244 | Val Loss: 1.3568, Acc: 0.5204\n",
      "Epoch:  10%|█         | 6/60 [02:28<21:50, 24.28s/it][A] Epoch 7/60 | Train Loss: 0.8879, Acc: 0.5356 | Val Loss: 1.3109, Acc: 0.5461\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  12%|█▏        | 7/60 [02:52<21:21, 24.18s/it][A] Epoch 8/60 | Train Loss: 0.8719, Acc: 0.5527 | Val Loss: 1.3124, Acc: 0.5408\n",
      "Epoch:  13%|█▎        | 8/60 [03:16<20:47, 24.00s/it][A] Epoch 9/60 | Train Loss: 0.8634, Acc: 0.5622 | Val Loss: 1.2893, Acc: 0.5598\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  15%|█▌        | 9/60 [03:40<20:18, 23.90s/it][A] Epoch 10/60 | Train Loss: 0.8571, Acc: 0.5725 | Val Loss: 1.2592, Acc: 0.5833\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  17%|█▋        | 10/60 [04:03<19:52, 23.84s/it][A] Epoch 11/60 | Train Loss: 0.8453, Acc: 0.5848 | Val Loss: 1.2863, Acc: 0.5785\n",
      "Epoch:  18%|█▊        | 11/60 [04:27<19:27, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_12.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 12/60 | Train Loss: 0.8378, Acc: 0.5944 | Val Loss: 1.2404, Acc: 0.5949\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  20%|██        | 12/60 [04:51<19:02, 23.80s/it][A] Epoch 13/60 | Train Loss: 0.8320, Acc: 0.5979 | Val Loss: 1.2442, Acc: 0.5891\n",
      "Epoch:  22%|██▏       | 13/60 [05:15<18:36, 23.76s/it][A] Epoch 14/60 | Train Loss: 0.8212, Acc: 0.6082 | Val Loss: 1.2530, Acc: 0.5971\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  23%|██▎       | 14/60 [05:39<18:15, 23.81s/it][A] Epoch 15/60 | Train Loss: 0.8158, Acc: 0.6103 | Val Loss: 1.2216, Acc: 0.6157\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  25%|██▌       | 15/60 [06:03<17:55, 23.90s/it][A] Epoch 16/60 | Train Loss: 0.8094, Acc: 0.6148 | Val Loss: 1.1910, Acc: 0.6268\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  27%|██▋       | 16/60 [06:32<18:40, 25.47s/it][A] Epoch 17/60 | Train Loss: 0.8087, Acc: 0.6184 | Val Loss: 1.2578, Acc: 0.5851\n",
      "Epoch:  28%|██▊       | 17/60 [06:56<17:56, 25.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_18.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 18/60 | Train Loss: 0.8045, Acc: 0.6246 | Val Loss: 1.3284, Acc: 0.5430\n",
      "Epoch:  30%|███       | 18/60 [07:20<17:19, 24.75s/it][A] Epoch 19/60 | Train Loss: 0.7921, Acc: 0.6372 | Val Loss: 1.2166, Acc: 0.6228\n",
      "Epoch:  32%|███▏      | 19/60 [07:44<16:47, 24.58s/it][A] Epoch 20/60 | Train Loss: 0.7906, Acc: 0.6380 | Val Loss: 1.2074, Acc: 0.6241\n",
      "Epoch:  33%|███▎      | 20/60 [08:08<16:16, 24.41s/it][A] Epoch 21/60 | Train Loss: 0.7842, Acc: 0.6408 | Val Loss: 1.2286, Acc: 0.6033\n",
      "Epoch:  35%|███▌      | 21/60 [08:32<15:48, 24.31s/it][A] Epoch 22/60 | Train Loss: 0.7861, Acc: 0.6390 | Val Loss: 1.2004, Acc: 0.6170\n",
      "Epoch:  37%|███▋      | 22/60 [08:56<15:22, 24.27s/it][A] Epoch 23/60 | Train Loss: 0.7784, Acc: 0.6472 | Val Loss: 2.8310, Acc: 0.1800\n",
      "Epoch:  38%|███▊      | 23/60 [09:21<14:57, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_24.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 24/60 | Train Loss: 0.7767, Acc: 0.6473 | Val Loss: 1.2099, Acc: 0.6232\n",
      "Epoch:  40%|████      | 24/60 [09:45<14:32, 24.24s/it][A] Epoch 25/60 | Train Loss: 0.7767, Acc: 0.6499 | Val Loss: 1.1719, Acc: 0.6449\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  42%|████▏     | 25/60 [10:09<14:07, 24.21s/it][A] Epoch 26/60 | Train Loss: 0.7698, Acc: 0.6576 | Val Loss: 1.1952, Acc: 0.6268\n",
      "Epoch:  43%|████▎     | 26/60 [10:33<13:40, 24.13s/it][A] Epoch 27/60 | Train Loss: 0.7643, Acc: 0.6544 | Val Loss: 1.2019, Acc: 0.6316\n",
      "Epoch:  45%|████▌     | 27/60 [10:57<13:16, 24.15s/it][A] Epoch 28/60 | Train Loss: 0.7608, Acc: 0.6575 | Val Loss: 1.1805, Acc: 0.6383\n",
      "Epoch:  47%|████▋     | 28/60 [11:21<12:50, 24.07s/it][A] Epoch 29/60 | Train Loss: 0.7586, Acc: 0.6625 | Val Loss: 1.2206, Acc: 0.6201\n",
      "Epoch:  48%|████▊     | 29/60 [11:45<12:26, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 30/60 | Train Loss: 0.7543, Acc: 0.6676 | Val Loss: 1.1779, Acc: 0.6494\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  50%|█████     | 30/60 [12:09<12:01, 24.06s/it][A] Epoch 31/60 | Train Loss: 0.7487, Acc: 0.6683 | Val Loss: 1.1851, Acc: 0.6512\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  52%|█████▏    | 31/60 [12:33<11:38, 24.09s/it][A] Epoch 32/60 | Train Loss: 0.7481, Acc: 0.6687 | Val Loss: 1.1797, Acc: 0.6489\n",
      "Epoch:  53%|█████▎    | 32/60 [12:57<11:13, 24.06s/it][A] Epoch 33/60 | Train Loss: 0.7447, Acc: 0.6710 | Val Loss: 1.2078, Acc: 0.6330\n",
      "Epoch:  55%|█████▌    | 33/60 [13:21<10:49, 24.04s/it][A] Epoch 34/60 | Train Loss: 0.7461, Acc: 0.6702 | Val Loss: 1.1931, Acc: 0.6454\n",
      "Epoch:  57%|█████▋    | 34/60 [13:45<10:26, 24.08s/it][A] Epoch 35/60 | Train Loss: 0.7445, Acc: 0.6743 | Val Loss: 1.2342, Acc: 0.6285\n",
      "Epoch:  58%|█████▊    | 35/60 [14:09<10:01, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_36.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 36/60 | Train Loss: 0.7373, Acc: 0.6725 | Val Loss: 1.2229, Acc: 0.6396\n",
      "Epoch:  60%|██████    | 36/60 [14:34<09:38, 24.09s/it][A] Epoch 37/60 | Train Loss: 0.7388, Acc: 0.6759 | Val Loss: 1.2060, Acc: 0.6379\n",
      "Epoch:  62%|██████▏   | 37/60 [14:58<09:14, 24.11s/it][A] Epoch 38/60 | Train Loss: 0.7333, Acc: 0.6793 | Val Loss: 1.2007, Acc: 0.6534\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  63%|██████▎   | 38/60 [15:22<08:49, 24.08s/it][A] Epoch 39/60 | Train Loss: 0.7364, Acc: 0.6745 | Val Loss: 1.2344, Acc: 0.6206\n",
      "Epoch:  65%|██████▌   | 39/60 [15:46<08:24, 24.04s/it][A] Epoch 40/60 | Train Loss: 0.7317, Acc: 0.6824 | Val Loss: 1.2005, Acc: 0.6454\n",
      "Epoch:  67%|██████▋   | 40/60 [16:10<08:00, 24.00s/it][A] Epoch 41/60 | Train Loss: 0.7284, Acc: 0.6811 | Val Loss: 1.1910, Acc: 0.6582\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  68%|██████▊   | 41/60 [16:34<07:37, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_42.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 42/60 | Train Loss: 0.7188, Acc: 0.6883 | Val Loss: 1.2442, Acc: 0.6321\n",
      "Epoch:  70%|███████   | 42/60 [16:58<07:14, 24.14s/it][A] Epoch 43/60 | Train Loss: 0.7267, Acc: 0.6804 | Val Loss: 1.1977, Acc: 0.6427\n",
      "Epoch:  72%|███████▏  | 43/60 [17:22<06:51, 24.21s/it][A] Epoch 44/60 | Train Loss: 0.7218, Acc: 0.6896 | Val Loss: 1.2284, Acc: 0.6410\n",
      "Epoch:  73%|███████▎  | 44/60 [17:47<06:27, 24.25s/it][A] Epoch 45/60 | Train Loss: 0.7210, Acc: 0.6861 | Val Loss: 1.2241, Acc: 0.6458\n",
      "Epoch:  75%|███████▌  | 45/60 [18:11<06:02, 24.17s/it][A] Epoch 46/60 | Train Loss: 0.7188, Acc: 0.6836 | Val Loss: 1.2336, Acc: 0.6316\n",
      "Epoch:  77%|███████▋  | 46/60 [18:35<05:37, 24.13s/it][A] Epoch 47/60 | Train Loss: 0.7192, Acc: 0.6881 | Val Loss: 1.1901, Acc: 0.6560\n",
      "Epoch:  78%|███████▊  | 47/60 [18:59<05:14, 24.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_48.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 48/60 | Train Loss: 0.7072, Acc: 0.6916 | Val Loss: 1.1997, Acc: 0.6520\n",
      "Epoch:  80%|████████  | 48/60 [19:23<04:50, 24.20s/it][A] Epoch 49/60 | Train Loss: 0.7080, Acc: 0.6947 | Val Loss: 1.2127, Acc: 0.6591\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  82%|████████▏ | 49/60 [19:47<04:25, 24.18s/it][A] Epoch 50/60 | Train Loss: 0.7094, Acc: 0.6950 | Val Loss: 1.2300, Acc: 0.6361\n",
      "Epoch:  83%|████████▎ | 50/60 [20:12<04:02, 24.24s/it][A] Epoch 51/60 | Train Loss: 0.7096, Acc: 0.6906 | Val Loss: 1.2283, Acc: 0.6538\n",
      "Epoch:  85%|████████▌ | 51/60 [20:36<03:38, 24.32s/it][A] Epoch 52/60 | Train Loss: 0.7103, Acc: 0.6989 | Val Loss: 1.2005, Acc: 0.6618\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  87%|████████▋ | 52/60 [21:01<03:14, 24.32s/it][A] Epoch 53/60 | Train Loss: 0.7035, Acc: 0.6961 | Val Loss: 1.1878, Acc: 0.6582\n",
      "Epoch:  88%|████████▊ | 53/60 [21:25<02:50, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_54.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 54/60 | Train Loss: 0.7056, Acc: 0.6951 | Val Loss: 1.2223, Acc: 0.6472\n",
      "Epoch:  90%|█████████ | 54/60 [21:49<02:26, 24.34s/it][A] Epoch 55/60 | Train Loss: 0.7032, Acc: 0.6966 | Val Loss: 1.2284, Acc: 0.6480\n",
      "Epoch:  92%|█████████▏| 55/60 [22:14<02:01, 24.32s/it][A] Epoch 56/60 | Train Loss: 0.7048, Acc: 0.7001 | Val Loss: 1.2085, Acc: 0.6591\n",
      "Epoch:  93%|█████████▎| 56/60 [22:38<01:37, 24.28s/it][A] Epoch 57/60 | Train Loss: 0.6983, Acc: 0.7032 | Val Loss: 1.1923, Acc: 0.6582\n",
      "Epoch:  95%|█████████▌| 57/60 [23:02<01:12, 24.26s/it][A] Epoch 58/60 | Train Loss: 0.7023, Acc: 0.6975 | Val Loss: 1.2379, Acc: 0.6485\n",
      "Epoch:  97%|█████████▋| 58/60 [23:26<00:48, 24.29s/it][A] Epoch 59/60 | Train Loss: 0.6996, Acc: 0.7011 | Val Loss: 1.2274, Acc: 0.6485\n",
      "Epoch:  98%|█████████▊| 59/60 [23:51<00:24, 24.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_60.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 60/60 | Train Loss: 0.6943, Acc: 0.7039 | Val Loss: 1.2384, Acc: 0.6538\n",
      "Epoch: 100%|██████████| 60/60 [24:15<00:00, 24.26s/it]\n",
      "Case study end, 0.661790780141844\n",
      "################################################################################\n",
      "\n",
      "\n",
      "[I 2025-05-25 19:28:00,865] Trial 0 finished with value: 0.661790780141844 and parameters: {'gnn_type': 'gcn', 'dropout': 0.25287334651446963, 'num_layers': 6, 'embedding_dim': 64, 'num_epochs': 60}. Best is trial 0 with value: 0.661790780141844.\n",
      "################################################################################\n",
      "Start case study with parameters:\n",
      "gnn_type='gcn-virtual'\n",
      "drop_ratio=0.5780210233709472\n",
      "num_layers=3\n",
      "embedding_dim=128\n",
      "num_epochs=80\n",
      "Epoch:   0%|          | 0/80 [00:00<?, ?it/s][A] Epoch 1/80 | Train Loss: 1.0746, Acc: 0.2698 | Val Loss: 1.7512, Acc: 0.2766\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   1%|▏         | 1/80 [00:24<32:47, 24.90s/it][A] Epoch 2/80 | Train Loss: 1.0407, Acc: 0.3370 | Val Loss: 1.6389, Acc: 0.3174\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   2%|▎         | 2/80 [00:49<32:21, 24.89s/it][A] Epoch 3/80 | Train Loss: 1.0335, Acc: 0.3515 | Val Loss: 1.7881, Acc: 0.2535\n",
      "Epoch:   4%|▍         | 3/80 [01:14<32:00, 24.94s/it][A] Epoch 4/80 | Train Loss: 1.0259, Acc: 0.3689 | Val Loss: 1.5633, Acc: 0.3759\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   5%|▌         | 4/80 [01:39<31:31, 24.88s/it][A] Epoch 5/80 | Train Loss: 1.0122, Acc: 0.3860 | Val Loss: 3.0380, Acc: 0.2026\n",
      "Epoch:   6%|▋         | 5/80 [02:04<31:04, 24.86s/it][A] Epoch 6/80 | Train Loss: 1.0122, Acc: 0.3871 | Val Loss: 1.6967, Acc: 0.3630\n",
      "Epoch:   8%|▊         | 6/80 [02:29<30:40, 24.87s/it][A] Epoch 7/80 | Train Loss: 1.0059, Acc: 0.3954 | Val Loss: 1.5391, Acc: 0.4051\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:   9%|▉         | 7/80 [02:54<30:15, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 8/80 | Train Loss: 1.0047, Acc: 0.3936 | Val Loss: 1.5603, Acc: 0.3812\n",
      "Epoch:  10%|█         | 8/80 [03:18<29:49, 24.86s/it][A] Epoch 9/80 | Train Loss: 1.0013, Acc: 0.3947 | Val Loss: 1.5094, Acc: 0.4087\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  11%|█▏        | 9/80 [03:43<29:25, 24.87s/it][A] Epoch 10/80 | Train Loss: 0.9980, Acc: 0.3986 | Val Loss: 1.5234, Acc: 0.4149\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  12%|█▎        | 10/80 [04:08<29:00, 24.86s/it][A] Epoch 11/80 | Train Loss: 0.9934, Acc: 0.4037 | Val Loss: 1.5472, Acc: 0.3980\n",
      "Epoch:  14%|█▍        | 11/80 [04:33<28:33, 24.84s/it][A] Epoch 12/80 | Train Loss: 0.9908, Acc: 0.4071 | Val Loss: 1.4922, Acc: 0.4153\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  15%|█▌        | 12/80 [04:58<28:09, 24.85s/it][A] Epoch 13/80 | Train Loss: 0.9924, Acc: 0.4040 | Val Loss: 1.5199, Acc: 0.4171\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  16%|█▋        | 13/80 [05:23<27:45, 24.86s/it][A] Epoch 14/80 | Train Loss: 0.9839, Acc: 0.4154 | Val Loss: 1.6522, Acc: 0.3506\n",
      "Epoch:  18%|█▊        | 14/80 [05:48<27:20, 24.86s/it][A] Epoch 15/80 | Train Loss: 0.9783, Acc: 0.4270 | Val Loss: 1.5309, Acc: 0.3825\n",
      "Epoch:  19%|█▉        | 15/80 [06:12<26:54, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 16/80 | Train Loss: 0.9693, Acc: 0.4447 | Val Loss: 1.9858, Acc: 0.2558\n",
      "Epoch:  20%|██        | 16/80 [06:37<26:27, 24.81s/it][A] Epoch 17/80 | Train Loss: 0.9594, Acc: 0.4549 | Val Loss: 1.4957, Acc: 0.4158\n",
      "Epoch:  21%|██▏       | 17/80 [07:02<26:01, 24.79s/it][A] Epoch 18/80 | Train Loss: 0.9546, Acc: 0.4665 | Val Loss: 1.4976, Acc: 0.4211\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  22%|██▎       | 18/80 [07:27<25:38, 24.81s/it][A] Epoch 19/80 | Train Loss: 0.9442, Acc: 0.4720 | Val Loss: 1.4047, Acc: 0.4889\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  24%|██▍       | 19/80 [07:52<25:12, 24.79s/it][A] Epoch 20/80 | Train Loss: 0.9397, Acc: 0.4820 | Val Loss: 1.5031, Acc: 0.4424\n",
      "Epoch:  25%|██▌       | 20/80 [08:16<24:47, 24.80s/it][A] Epoch 21/80 | Train Loss: 0.9291, Acc: 0.4941 | Val Loss: 1.4374, Acc: 0.4854\n",
      "Epoch:  26%|██▋       | 21/80 [08:41<24:23, 24.81s/it][A] Epoch 22/80 | Train Loss: 0.9183, Acc: 0.5031 | Val Loss: 1.4570, Acc: 0.4809\n",
      "Epoch:  28%|██▊       | 22/80 [09:06<23:57, 24.78s/it][A] Epoch 23/80 | Train Loss: 0.9190, Acc: 0.5019 | Val Loss: 1.4056, Acc: 0.4934\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  29%|██▉       | 23/80 [09:31<23:32, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_24.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 24/80 | Train Loss: 0.9138, Acc: 0.5198 | Val Loss: 1.3514, Acc: 0.5337\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  30%|███       | 24/80 [09:56<23:11, 24.85s/it][A] Epoch 25/80 | Train Loss: 0.9053, Acc: 0.5178 | Val Loss: 1.3792, Acc: 0.4934\n",
      "Epoch:  31%|███▏      | 25/80 [10:21<22:47, 24.86s/it][A] Epoch 26/80 | Train Loss: 0.9021, Acc: 0.5262 | Val Loss: 1.3660, Acc: 0.5195\n",
      "Epoch:  32%|███▎      | 26/80 [10:45<22:17, 24.77s/it][A] Epoch 27/80 | Train Loss: 0.8976, Acc: 0.5339 | Val Loss: 1.3901, Acc: 0.5000\n",
      "Epoch:  34%|███▍      | 27/80 [11:10<21:50, 24.73s/it][A] Epoch 28/80 | Train Loss: 0.8985, Acc: 0.5355 | Val Loss: 1.3459, Acc: 0.5324\n",
      "Epoch:  35%|███▌      | 28/80 [11:34<21:23, 24.68s/it][A] Epoch 29/80 | Train Loss: 0.8894, Acc: 0.5409 | Val Loss: 1.3850, Acc: 0.5022\n",
      "Epoch:  36%|███▋      | 29/80 [11:59<20:57, 24.65s/it][A] Epoch 30/80 | Train Loss: 0.8843, Acc: 0.5450 | Val Loss: 1.3316, Acc: 0.5381\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  38%|███▊      | 30/80 [12:23<20:30, 24.60s/it][A] Epoch 31/80 | Train Loss: 0.8787, Acc: 0.5502 | Val Loss: 1.3329, Acc: 0.5363\n",
      "Epoch:  39%|███▉      | 31/80 [12:48<20:02, 24.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_32.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 32/80 | Train Loss: 0.8827, Acc: 0.5485 | Val Loss: 1.3244, Acc: 0.5395\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  40%|████      | 32/80 [13:12<19:37, 24.54s/it][A] Epoch 33/80 | Train Loss: 0.8766, Acc: 0.5523 | Val Loss: 1.5650, Acc: 0.4020\n",
      "Epoch:  41%|████▏     | 33/80 [13:37<19:13, 24.53s/it][A] Epoch 34/80 | Train Loss: 0.8761, Acc: 0.5493 | Val Loss: 1.2962, Acc: 0.5612\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  42%|████▎     | 34/80 [14:01<18:47, 24.50s/it][A] Epoch 35/80 | Train Loss: 0.8766, Acc: 0.5564 | Val Loss: 1.3050, Acc: 0.5523\n",
      "Epoch:  44%|████▍     | 35/80 [14:26<18:24, 24.54s/it][A] Epoch 36/80 | Train Loss: 0.8716, Acc: 0.5580 | Val Loss: 1.3145, Acc: 0.5439\n",
      "Epoch:  45%|████▌     | 36/80 [14:50<17:58, 24.52s/it][A] Epoch 37/80 | Train Loss: 0.8724, Acc: 0.5598 | Val Loss: 1.3138, Acc: 0.5523\n",
      "Epoch:  46%|████▋     | 37/80 [15:15<17:32, 24.49s/it][A] Epoch 38/80 | Train Loss: 0.8641, Acc: 0.5650 | Val Loss: 1.3144, Acc: 0.5567\n",
      "Epoch:  48%|████▊     | 38/80 [15:39<17:11, 24.56s/it][A] Epoch 39/80 | Train Loss: 0.8631, Acc: 0.5713 | Val Loss: 1.3347, Acc: 0.5359\n",
      "Epoch:  49%|████▉     | 39/80 [16:04<16:45, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 40/80 | Train Loss: 0.8588, Acc: 0.5727 | Val Loss: 1.3103, Acc: 0.5607\n",
      "Epoch:  50%|█████     | 40/80 [16:28<16:21, 24.53s/it][A] Epoch 41/80 | Train Loss: 0.8547, Acc: 0.5755 | Val Loss: 1.2959, Acc: 0.5643\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  51%|█████▏    | 41/80 [16:53<15:57, 24.55s/it][A] Epoch 42/80 | Train Loss: 0.8577, Acc: 0.5718 | Val Loss: 1.3398, Acc: 0.5465\n",
      "Epoch:  52%|█████▎    | 42/80 [17:18<15:31, 24.52s/it][A] Epoch 43/80 | Train Loss: 0.8552, Acc: 0.5685 | Val Loss: 1.3235, Acc: 0.5629\n",
      "Epoch:  54%|█████▍    | 43/80 [17:42<15:05, 24.48s/it][A] Epoch 44/80 | Train Loss: 0.8587, Acc: 0.5757 | Val Loss: 1.2784, Acc: 0.5807\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  55%|█████▌    | 44/80 [18:07<14:43, 24.53s/it][A] Epoch 45/80 | Train Loss: 0.8517, Acc: 0.5792 | Val Loss: 1.2828, Acc: 0.5745\n",
      "Epoch:  56%|█████▋    | 45/80 [18:31<14:19, 24.55s/it][A] Epoch 46/80 | Train Loss: 0.8494, Acc: 0.5793 | Val Loss: 1.2884, Acc: 0.5736\n",
      "Epoch:  57%|█████▊    | 46/80 [18:56<13:54, 24.55s/it][A] Epoch 47/80 | Train Loss: 0.8475, Acc: 0.5827 | Val Loss: 1.2820, Acc: 0.5780\n",
      "Epoch:  59%|█████▉    | 47/80 [19:20<13:30, 24.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_48.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 48/80 | Train Loss: 0.8476, Acc: 0.5883 | Val Loss: 1.3084, Acc: 0.5634\n",
      "Epoch:  60%|██████    | 48/80 [19:45<13:06, 24.57s/it][A] Epoch 49/80 | Train Loss: 0.8446, Acc: 0.5867 | Val Loss: 1.2684, Acc: 0.5887\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  61%|██████▏   | 49/80 [20:09<12:40, 24.53s/it][A] Epoch 50/80 | Train Loss: 0.8471, Acc: 0.5861 | Val Loss: 1.2751, Acc: 0.5833\n",
      "Epoch:  62%|██████▎   | 50/80 [20:34<12:15, 24.52s/it][A] Epoch 51/80 | Train Loss: 0.8417, Acc: 0.5878 | Val Loss: 1.3062, Acc: 0.5625\n",
      "Epoch:  64%|██████▍   | 51/80 [20:58<11:50, 24.50s/it][A] Epoch 52/80 | Train Loss: 0.8362, Acc: 0.5959 | Val Loss: 1.2777, Acc: 0.5869\n",
      "Epoch:  65%|██████▌   | 52/80 [21:23<11:24, 24.46s/it][A] Epoch 53/80 | Train Loss: 0.8378, Acc: 0.5960 | Val Loss: 1.3429, Acc: 0.5532\n",
      "Epoch:  66%|██████▋   | 53/80 [21:47<11:00, 24.47s/it][A] Epoch 54/80 | Train Loss: 0.8420, Acc: 0.5993 | Val Loss: 1.3228, Acc: 0.5709\n",
      "Epoch:  68%|██████▊   | 54/80 [22:12<10:36, 24.50s/it][A] Epoch 55/80 | Train Loss: 0.8356, Acc: 0.5943 | Val Loss: 1.2962, Acc: 0.5758\n",
      "Epoch:  69%|██████▉   | 55/80 [22:36<10:13, 24.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_56.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 56/80 | Train Loss: 0.8374, Acc: 0.5984 | Val Loss: 1.3023, Acc: 0.5740\n",
      "Epoch:  70%|███████   | 56/80 [23:01<09:48, 24.52s/it][A] Epoch 57/80 | Train Loss: 0.8301, Acc: 0.6034 | Val Loss: 1.2920, Acc: 0.5833\n",
      "Epoch:  71%|███████▏  | 57/80 [23:25<09:23, 24.51s/it][A] Epoch 58/80 | Train Loss: 0.8302, Acc: 0.6078 | Val Loss: 1.3277, Acc: 0.5638\n",
      "Epoch:  72%|███████▎  | 58/80 [23:50<09:00, 24.57s/it][A] Epoch 59/80 | Train Loss: 0.8372, Acc: 0.6017 | Val Loss: 1.2725, Acc: 0.5882\n",
      "Epoch:  74%|███████▍  | 59/80 [24:14<08:35, 24.54s/it][A] Epoch 60/80 | Train Loss: 0.8325, Acc: 0.6035 | Val Loss: 1.3120, Acc: 0.5691\n",
      "Epoch:  75%|███████▌  | 60/80 [24:39<08:12, 24.63s/it][A] Epoch 61/80 | Train Loss: 0.8320, Acc: 0.6055 | Val Loss: 1.2430, Acc: 0.5922\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  76%|███████▋  | 61/80 [25:04<07:48, 24.67s/it][A] Epoch 62/80 | Train Loss: 0.8289, Acc: 0.6058 | Val Loss: 1.2855, Acc: 0.5816\n",
      "Epoch:  78%|███████▊  | 62/80 [25:29<07:24, 24.67s/it][A] Epoch 63/80 | Train Loss: 0.8258, Acc: 0.6055 | Val Loss: 1.2986, Acc: 0.5869\n",
      "Epoch:  79%|███████▉  | 63/80 [25:53<06:58, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_64.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 64/80 | Train Loss: 0.8236, Acc: 0.6037 | Val Loss: 1.3194, Acc: 0.5824\n",
      "Epoch:  80%|████████  | 64/80 [26:18<06:34, 24.63s/it][A] Epoch 65/80 | Train Loss: 0.8290, Acc: 0.6065 | Val Loss: 1.3039, Acc: 0.5842\n",
      "Epoch:  81%|████████▏ | 65/80 [26:43<06:09, 24.62s/it][A] Epoch 66/80 | Train Loss: 0.8271, Acc: 0.6072 | Val Loss: 1.3047, Acc: 0.5723\n",
      "Epoch:  82%|████████▎ | 66/80 [27:07<05:43, 24.56s/it][A] Epoch 67/80 | Train Loss: 0.8295, Acc: 0.6056 | Val Loss: 1.2873, Acc: 0.5878\n",
      "Epoch:  84%|████████▍ | 67/80 [27:31<05:18, 24.50s/it][A] Epoch 68/80 | Train Loss: 0.8243, Acc: 0.6103 | Val Loss: 1.3116, Acc: 0.5785\n",
      "Epoch:  85%|████████▌ | 68/80 [27:56<04:53, 24.48s/it][A] Epoch 69/80 | Train Loss: 0.8203, Acc: 0.6157 | Val Loss: 1.2936, Acc: 0.5891\n",
      "Epoch:  86%|████████▋ | 69/80 [28:20<04:29, 24.54s/it][A] Epoch 70/80 | Train Loss: 0.8198, Acc: 0.6134 | Val Loss: 1.3313, Acc: 0.5705\n",
      "Epoch:  88%|████████▊ | 70/80 [28:45<04:06, 24.63s/it][A] Epoch 71/80 | Train Loss: 0.8181, Acc: 0.6203 | Val Loss: 1.2932, Acc: 0.5891\n",
      "Epoch:  89%|████████▉ | 71/80 [29:10<03:42, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_72.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 72/80 | Train Loss: 0.8214, Acc: 0.6178 | Val Loss: 1.2660, Acc: 0.6042\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  90%|█████████ | 72/80 [29:35<03:17, 24.74s/it][A] Epoch 73/80 | Train Loss: 0.8118, Acc: 0.6210 | Val Loss: 1.2565, Acc: 0.5975\n",
      "Epoch:  91%|█████████▏| 73/80 [30:00<02:53, 24.73s/it][A] Epoch 74/80 | Train Loss: 0.8118, Acc: 0.6177 | Val Loss: 1.2838, Acc: 0.5909\n",
      "Epoch:  92%|█████████▎| 74/80 [30:24<02:28, 24.68s/it][A] Epoch 75/80 | Train Loss: 0.8151, Acc: 0.6202 | Val Loss: 1.2899, Acc: 0.5966\n",
      "Epoch:  94%|█████████▍| 75/80 [30:49<02:03, 24.63s/it][A] Epoch 76/80 | Train Loss: 0.8119, Acc: 0.6210 | Val Loss: 1.2850, Acc: 0.5997\n",
      "Epoch:  95%|█████████▌| 76/80 [31:13<01:38, 24.59s/it][A] Epoch 77/80 | Train Loss: 0.8118, Acc: 0.6236 | Val Loss: 1.2792, Acc: 0.5913\n",
      "Epoch:  96%|█████████▋| 77/80 [31:38<01:13, 24.61s/it][A] Epoch 78/80 | Train Loss: 0.8140, Acc: 0.6208 | Val Loss: 1.2616, Acc: 0.6051\n",
      "[A] Best model updated at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_best.pth\n",
      "Epoch:  98%|█████████▊| 78/80 [32:03<00:49, 24.63s/it][A] Epoch 79/80 | Train Loss: 0.8064, Acc: 0.6187 | Val Loss: 1.2462, Acc: 0.6042\n",
      "Epoch:  99%|█████████▉| 79/80 [32:27<00:24, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_80.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[A] Epoch 80/80 | Train Loss: 0.8088, Acc: 0.6268 | Val Loss: 1.3899, Acc: 0.5288\n",
      "Epoch: 100%|██████████| 80/80 [32:52<00:00, 24.66s/it]\n",
      "Case study end, 0.6050531914893617\n",
      "################################################################################\n",
      "\n",
      "\n",
      "[I 2025-05-25 20:00:53,617] Trial 1 finished with value: 0.6050531914893617 and parameters: {'gnn_type': 'gcn-virtual', 'dropout': 0.5780210233709472, 'num_layers': 3, 'embedding_dim': 128, 'num_epochs': 80}. Best is trial 0 with value: 0.661790780141844.\n",
      "\n",
      "All trials saved to: /home/haislich/Documents/noisy_labels/hackaton/logs/A/optuna_summary_A.csv\n",
      "\n",
      "Best result for dataset A:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>gnn_type</th>\n",
       "      <th>dropout</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>num_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.661791</td>\n",
       "      <td>gcn</td>\n",
       "      <td>0.252873</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.605053</td>\n",
       "      <td>gcn-virtual</td>\n",
       "      <td>0.578021</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy     gnn_type   dropout  num_layers  embedding_dim  num_epochs\n",
       "0  0.661791          gcn  0.252873           6             64          60\n",
       "1  0.605053  gcn-virtual  0.578021           3            128          80"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for A:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gnn_type: gcn\n",
      "  dropout: 0.25287334651446963\n",
      "  num_layers: 6\n",
      "  embedding_dim: 64\n",
      "  num_epochs: 60\n"
     ]
    }
   ],
   "source": [
    "case_study(\"A\", 2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
