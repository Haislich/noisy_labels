{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'hackaton/'\n",
      "/home/haislich/Documents/noisy_labels/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PxBvwB0_6xI8",
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1rockhiQ7Nny",
    "outputId": "2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 2 haislich haislich 4,0K 25 mag 11.19 A\n",
      "drwxr-xr-x 2 haislich haislich 4,0K 25 mag 11.19 B\n",
      "drwxr-xr-x 2 haislich haislich 4,0K 25 mag 11.19 C\n",
      "drwxr-xr-x 2 haislich haislich 4,0K 25 mag 11.19 D\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs:   0%|          | 0/74 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred=tensor([2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 3, 2, 2, 2, 2, 2, 2], device='cuda:0'), data.y=None\n",
      "type(pred)=<class 'torch.Tensor'>, type(data.y)=<class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[32m     29\u001b[39m model.load_state_dict(torch.load(checkpoint_path))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m predictions = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_accuracy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m save_predictions(predictions, args.test_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data_loader, model, device, calculate_accuracy)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pred)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data.y)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# print(f\"{pred data. }\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m correct += \u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m().item()\n\u001b[32m     20\u001b[39m total += data.y.size(\u001b[32m0\u001b[39m)\n\u001b[32m     21\u001b[39m total_loss += criterion(output, data.y).item()\n",
      "\u001b[31mAttributeError\u001b[39m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = \"\"#input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\", default = \"./datasets/B/train.json.gz\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\",default = \"./datasets/B/test.json.gz\")\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: ./datasets/A/train.json.gz\n",
      "test_path: ./datasets/A/test.json.gz\n",
      "num_checkpoints: None\n",
      "device: 1\n",
      "gnn: gin\n",
      "drop_ratio: 0.0\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 10\n",
      "baseline_mode: 1\n",
      "noise_prob: 0.2\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lHX55XGECXBr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "if args.baseline_mode == 2:\n",
    "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "BTYT5jYuChPb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:32<00:00,  8.66batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.96batch/s]\n",
      "Epoch 1/10, Loss: 1.6972, Train Acc: 0.3193, Val Acc: 0.3435\n",
      "Epoch 1/10, Loss: 1.6972, Train Acc: 0.3193, Val Acc: 0.3435\n",
      "Epoch 1/10, Loss: 1.6972, Train Acc: 0.3193, Val Acc: 0.3435\n",
      "Epoch 1/10, Loss: 1.6972, Train Acc: 0.3193, Val Acc: 0.3435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6972, Train Acc: 0.3193, Val Acc: 0.3435\n",
      "Best model updated and saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/model_A_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.05batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.85batch/s]\n",
      "Epoch 2/10, Loss: 1.6355, Train Acc: 0.3535, Val Acc: 0.2961\n",
      "Epoch 2/10, Loss: 1.6355, Train Acc: 0.3535, Val Acc: 0.2961\n",
      "Epoch 2/10, Loss: 1.6355, Train Acc: 0.3535, Val Acc: 0.2961\n",
      "Epoch 2/10, Loss: 1.6355, Train Acc: 0.3535, Val Acc: 0.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.6355, Train Acc: 0.3535, Val Acc: 0.2961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.91batch/s]\n",
      "Epoch 3/10, Loss: 1.6329, Train Acc: 0.3583, Val Acc: 0.3107\n",
      "Epoch 3/10, Loss: 1.6329, Train Acc: 0.3583, Val Acc: 0.3107\n",
      "Epoch 3/10, Loss: 1.6329, Train Acc: 0.3583, Val Acc: 0.3107\n",
      "Epoch 3/10, Loss: 1.6329, Train Acc: 0.3583, Val Acc: 0.3107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.6329, Train Acc: 0.3583, Val Acc: 0.3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.02batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.76batch/s]\n",
      "Epoch 4/10, Loss: 1.6166, Train Acc: 0.3718, Val Acc: 0.3746\n",
      "Epoch 4/10, Loss: 1.6166, Train Acc: 0.3718, Val Acc: 0.3746\n",
      "Epoch 4/10, Loss: 1.6166, Train Acc: 0.3718, Val Acc: 0.3746\n",
      "Epoch 4/10, Loss: 1.6166, Train Acc: 0.3718, Val Acc: 0.3746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.6166, Train Acc: 0.3718, Val Acc: 0.3746\n",
      "Best model updated and saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/model_A_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.01batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.90batch/s]\n",
      "Epoch 5/10, Loss: 1.5921, Train Acc: 0.3795, Val Acc: 0.3719\n",
      "Epoch 5/10, Loss: 1.5921, Train Acc: 0.3795, Val Acc: 0.3719\n",
      "Epoch 5/10, Loss: 1.5921, Train Acc: 0.3795, Val Acc: 0.3719\n",
      "Epoch 5/10, Loss: 1.5921, Train Acc: 0.3795, Val Acc: 0.3719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.5921, Train Acc: 0.3795, Val Acc: 0.3719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.87batch/s]\n",
      "Epoch 6/10, Loss: 1.5716, Train Acc: 0.3912, Val Acc: 0.3147\n",
      "Epoch 6/10, Loss: 1.5716, Train Acc: 0.3912, Val Acc: 0.3147\n",
      "Epoch 6/10, Loss: 1.5716, Train Acc: 0.3912, Val Acc: 0.3147\n",
      "Epoch 6/10, Loss: 1.5716, Train Acc: 0.3912, Val Acc: 0.3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.5716, Train Acc: 0.3912, Val Acc: 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.04batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.93batch/s]\n",
      "Epoch 7/10, Loss: 1.5678, Train Acc: 0.3921, Val Acc: 0.3790\n",
      "Epoch 7/10, Loss: 1.5678, Train Acc: 0.3921, Val Acc: 0.3790\n",
      "Epoch 7/10, Loss: 1.5678, Train Acc: 0.3921, Val Acc: 0.3790\n",
      "Epoch 7/10, Loss: 1.5678, Train Acc: 0.3921, Val Acc: 0.3790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.5678, Train Acc: 0.3921, Val Acc: 0.3790\n",
      "Best model updated and saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/model_A_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:32<00:00,  8.56batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.85batch/s]\n",
      "Epoch 8/10, Loss: 1.5557, Train Acc: 0.4002, Val Acc: 0.3905\n",
      "Epoch 8/10, Loss: 1.5557, Train Acc: 0.4002, Val Acc: 0.3905\n",
      "Epoch 8/10, Loss: 1.5557, Train Acc: 0.4002, Val Acc: 0.3905\n",
      "Epoch 8/10, Loss: 1.5557, Train Acc: 0.4002, Val Acc: 0.3905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.5557, Train Acc: 0.4002, Val Acc: 0.3905\n",
      "Best model updated and saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/model_A_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:27<00:00, 10.08batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.87batch/s]\n",
      "Epoch 9/10, Loss: 1.5439, Train Acc: 0.4007, Val Acc: 0.3870\n",
      "Epoch 9/10, Loss: 1.5439, Train Acc: 0.4007, Val Acc: 0.3870\n",
      "Epoch 9/10, Loss: 1.5439, Train Acc: 0.4007, Val Acc: 0.3870\n",
      "Epoch 9/10, Loss: 1.5439, Train Acc: 0.4007, Val Acc: 0.3870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.5439, Train Acc: 0.4007, Val Acc: 0.3870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 282/282 [00:28<00:00, 10.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.95batch/s]\n",
      "Epoch 10/10, Loss: 1.5474, Train Acc: 0.4002, Val Acc: 0.3737\n",
      "Epoch 10/10, Loss: 1.5474, Train Acc: 0.4002, Val Acc: 0.3737\n",
      "Epoch 10/10, Loss: 1.5474, Train Acc: 0.4002, Val Acc: 0.3737\n",
      "Epoch 10/10, Loss: 1.5474, Train Acc: 0.4002, Val Acc: 0.3737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.5474, Train Acc: 0.4002, Val Acc: 0.3737\n"
     ]
    }
   ],
   "source": [
    "if args.train_path:\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    best_val_accuracy = 0.0   \n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer, criterion, device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
    "            current_epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss,val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11739"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs:   0%|          | 0/74 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model.load_state_dict(torch.load(checkpoint_path))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m predictions = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_accuracy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m save_predictions(predictions, args.test_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data_loader, model, device, calculate_accuracy)\u001b[39m\n\u001b[32m     12\u001b[39m pred = output.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m calculate_accuracy:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     correct += \u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m().item()\n\u001b[32m     16\u001b[39m     total += data.y.size(\u001b[32m0\u001b[39m)\n\u001b[32m     17\u001b[39m     total_loss += criterion(output, data.y).item()\n",
      "\u001b[31mAttributeError\u001b[39m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=True)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs:   0%|          | 0/74 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 150304], edge_attr=[150304, 7], num_nodes=7701, x=[7701], batch=[7701], ptr=[33])\n",
      "pred=tensor([2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 3, 2, 2, 2, 2, 2, 2], device='cuda:0'), data.y=None\n",
      "type(pred)=<class 'torch.Tensor'>, type(data.y)=<class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[32m     32\u001b[39m model.load_state_dict(torch.load(checkpoint_path))\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m predictions = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_accuracy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m save_predictions(predictions, args.test_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data_loader, model, device, calculate_accuracy)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pred)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data.y)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(f\"{pred data. }\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m correct += \u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m().item()\n\u001b[32m     23\u001b[39m total += data.y.size(\u001b[32m0\u001b[39m)\n\u001b[32m     24\u001b[39m total_loss += criterion(output, data.y).item()\n",
      "\u001b[31mAttributeError\u001b[39m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     predictions = []\n",
    "#     total_loss = 0\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     with torch.no_grad():\n",
    "#         for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "#             print(data)\n",
    "#             data = data.to(device)\n",
    "#             output = model(data)\n",
    "#             pred = output.argmax(dim=1)\n",
    "            \n",
    "#             if calculate_accuracy:\n",
    "#                 print(f\"{pred=}, {data.y=}\")\n",
    "#                 print(f\"{type(pred)=}, {type(data.y)=}\")\n",
    "#                 # print(f\"{pred data. }\")\n",
    "\n",
    "#                 correct += (pred == data.y).sum().item()\n",
    "#                 total += data.y.size(0)\n",
    "#                 total_loss += criterion(output, data.y).item()\n",
    "#             else:\n",
    "#                 predictions.extend(pred.cpu().numpy())\n",
    "#     if calculate_accuracy:\n",
    "#         accuracy = correct / total\n",
    "#         return  total_loss / len(data_loader),accuracy\n",
    "#     return predictions\n",
    "\n",
    "# model.load_state_dict(torch.load(checkpoint_path))\n",
    "# predictions = evaluate(test_loader, model, device, calculate_accuracy=True)\n",
    "# save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
