{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'hackaton/'\n",
      "/home/valerio/Desktop/noisy_labels/hackaton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valerio/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PxBvwB0_6xI8",
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1rockhiQ7Nny",
    "outputId": "2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxrwxr-x 6 valerio valerio 4.0K May 25 13:03 data\n"
     ]
    }
   ],
   "source": [
    "!ls -lh datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, scheduler, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            pred = logits.argmax(dim=1)\n",
    "\n",
    "            if calculate_accuracy:\n",
    "                loss = criterion(logits, data.y)\n",
    "                total_loss += loss.item()\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "\n",
    "                all_preds.extend(pred.cpu().tolist())\n",
    "                all_labels.extend(data.y.cpu().tolist())\n",
    "            else:\n",
    "                all_preds.extend(pred.cpu().tolist())\n",
    "\n",
    "    if calculate_accuracy:\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = correct / total\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        return avg_loss, accuracy, macro_f1\n",
    "\n",
    "    return all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
    "\n",
    "    while True:\n",
    "        user_input = \"\"#input(f\"{prompt} [{default}]: \")\n",
    "        \n",
    "        if user_input == \"\" and required:\n",
    "            print(\"This field is required. Please enter a value.\")\n",
    "            continue\n",
    "        \n",
    "        if user_input == \"\" and default is not None:\n",
    "            return default\n",
    "        \n",
    "        if user_input == \"\" and not required:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return type_cast(user_input)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    args = {}\n",
    "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\", default = \"./datasets/data/C/train.json.gz\")\n",
    "    args['test_path'] = get_user_input(\"Path to the test dataset\",default = \"./datasets/data/C/test.json.gz\")\n",
    "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
    "    args['device'] = get_user_input(\"Which GPU to use if any\", default=0, type_cast=int)\n",
    "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin-virtual')\n",
    "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
    "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
    "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
    "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
    "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=50, type_cast=int)\n",
    "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n",
    "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
    "\n",
    "    \n",
    "    return argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments received:\n",
      "train_path: ./datasets/data/C/train.json.gz\n",
      "test_path: ./datasets/data/C/test.json.gz\n",
      "num_checkpoints: None\n",
      "device: 0\n",
      "gnn: gin-virtual\n",
      "drop_ratio: 0.0\n",
      "num_layer: 5\n",
      "emb_dim: 300\n",
      "batch_size: 32\n",
      "epochs: 50\n",
      "baseline_mode: 1\n",
      "noise_prob: 0.2\n"
     ]
    }
   ],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHX55XGECXBr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "script_dir = os.getcwd() \n",
    "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
    "    \n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False).to(device)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
    "else:\n",
    "    raise ValueError('Invalid GNN type')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if args.baseline_mode == 2:\n",
    "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "BTYT5jYuChPb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from collections import Counter\n",
    "# import torch\n",
    "# from torch.utils.data import random_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Imposta root_dir alla cartella che contiene A/, B/, C/, D/\n",
    "# root_dir = '/home/valerio/Desktop/noisy_labels/hackaton/datasets/data'\n",
    "# # oppure, se passi --train_path uguale a questo path:\n",
    "# # root_dir = args.train_path\n",
    "\n",
    "# log_file = \"stats.log\"\n",
    "# # Pulisci il log all’avvio\n",
    "# with open(log_file, \"w\") as f:\n",
    "#     f.write(\"\")\n",
    "\n",
    "# for subname in sorted(os.listdir(root_dir)):\n",
    "#     subdir = os.path.join(root_dir, subname)\n",
    "#     train_path = os.path.join(subdir, \"train.json.gz\")\n",
    "\n",
    "#     if not os.path.isdir(subdir) or not os.path.isfile(train_path):\n",
    "#         print(f\"Skipping {subdir}, not a valid dataset directory or missing train.json.gz\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"Processing dataset {subname}\")\n",
    "\n",
    "#     # Carico e splitto\n",
    "#     full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "#     val_size = int(0.2 * len(full_dataset))\n",
    "#     train_size = len(full_dataset) - val_size\n",
    "#     generator = torch.Generator().manual_seed(12)\n",
    "#     train_dataset, val_dataset = random_split(\n",
    "#         full_dataset, [train_size, val_size], generator=generator\n",
    "#     )\n",
    "\n",
    "#     # Calcolo distribuzione\n",
    "#     labels = [d.y.item() for d in train_dataset]\n",
    "#     counts = Counter(labels)\n",
    "#     total = len(labels)\n",
    "\n",
    "#     with open(log_file, \"a\") as f:\n",
    "#         f.write(f\"=== Dataset {os.path.basename(subdir)} ===\\n\")\n",
    "#         for cls in sorted(counts):\n",
    "#             cnt = counts[cls]\n",
    "#             pct = cnt / total * 100\n",
    "#             f.write(f\"Label {cls}: {cnt} esempi ({pct:.1f}%)\\n\")\n",
    "\n",
    "#         f.write(\"\\n--- Esempi struttura grafo ---\\n\")\n",
    "#         for cls in sorted(counts):\n",
    "#             sample = next(d for d in train_dataset if d.y.item() == cls)\n",
    "#             f.write(\n",
    "#                 f\"Classe {cls}: num_nodes={sample.num_nodes}, \"\n",
    "#                 f\"num_edges={sample.num_edges}\\n\"\n",
    "#             )\n",
    "\n",
    "#         # Statistiche aggregate\n",
    "#         f.write(\"\\n--- Statistiche aggregate per classe ---\\n\")\n",
    "#         f.write(\"cls\\tn_samp\\tnodes_mean\\tnodes_std\\tedges_mean\\tedges_std\\n\")\n",
    "#         for cls in sorted(counts):\n",
    "#             samples = [d for d in train_dataset if d.y.item() == cls]\n",
    "#             nodes = [d.num_nodes for d in samples]\n",
    "#             edges = [d.num_edges for d in samples]\n",
    "#             f.write(\n",
    "#                 f\"{cls}\\t{counts[cls]}\\t\"\n",
    "#                 f\"{np.mean(nodes):.1f}\\t{np.std(nodes):.1f}\\t\"\n",
    "#                 f\"{np.mean(edges):.1f}\\t{np.std(edges):.1f}\\n\"\n",
    "#             )\n",
    "\n",
    "#         f.write(\"\\n\\n\")\n",
    "\n",
    "#     # Una volta processata questa cartella, non serve cercare altri train.json.gz in subdirectory\n",
    "#     # se sai che i dataset sono solo A–D direttamente sotto root_dir. Altrimenti commenta questa linea.\n",
    "#     # break  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics(train_losses, train_accuracies, val_losses, val_accuracies, save_dir):\n",
    "    \"\"\"\n",
    "    Plots and saves a single figure containing training/validation loss and accuracy over epochs.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Loss curves\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', linestyle='-')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss', linestyle='--')\n",
    "    # Accuracy curves\n",
    "    plt.plot(epochs, train_accuracies, label='Train Acc', linestyle='-.')\n",
    "    plt.plot(epochs, val_accuracies, label='Val Acc', linestyle=':')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'all_metrics.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"All metrics plot saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class SCELoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes, alpha=0.1, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        #CCE\n",
    "        ce  = F.cross_entropy(logits, targets, reduction='none')\n",
    "\n",
    "        #RCE\n",
    "        pred = F.softmax(logits, dim=1).clamp(min=1e-6, max=1-1e-6)\n",
    "        one_hot = F.one_hot(targets, self.num_classes).float()\n",
    "        rce = -(1 - one_hot) * torch.log(1 - pred)\n",
    "        rce = rce.sum(dim=1)\n",
    "        return (self.alpha * ce + self.beta * rce).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.57batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.30batch/s]\n",
      "Epoch 1/30, Val Acc: 0.3626, Val Macro-F1: 0.2495, Val Loss: 21.5169\n",
      "Epoch 1/30, Val Acc: 0.3626, Val Macro-F1: 0.2495, Val Loss: 21.5169\n",
      "Epoch 1/30, Loss: 0.2364, Train Acc: 0.9160, Val Acc: 0.3626\n",
      "Epoch 1/30, Loss: 0.2364, Train Acc: 0.9160, Val Acc: 0.3626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Acc: 0.9160 | Val Acc: 0.3626 | Val Macro-F1: 0.2495 | Val Loss: 21.5169\n",
      "Epoch 1/30, Loss: 0.2364, Train Acc: 0.9160, Val Acc: 0.3626\n",
      "Best model updated and saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/model_C_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.48batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.80batch/s]\n",
      "Epoch 2/30, Val Acc: 0.5039, Val Macro-F1: 0.3534, Val Loss: 7.2884\n",
      "Epoch 2/30, Val Acc: 0.5039, Val Macro-F1: 0.3534, Val Loss: 7.2884\n",
      "Epoch 2/30, Loss: 0.3915, Train Acc: 0.8678, Val Acc: 0.5039\n",
      "Epoch 2/30, Loss: 0.3915, Train Acc: 0.8678, Val Acc: 0.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Acc: 0.8678 | Val Acc: 0.5039 | Val Macro-F1: 0.3534 | Val Loss: 7.2884\n",
      "Epoch 2/30, Loss: 0.3915, Train Acc: 0.8678, Val Acc: 0.5039\n",
      "Best model updated and saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/model_C_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.50batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.73batch/s]\n",
      "Epoch 3/30, Val Acc: 0.5221, Val Macro-F1: 0.3652, Val Loss: 3.5737\n",
      "Epoch 3/30, Val Acc: 0.5221, Val Macro-F1: 0.3652, Val Loss: 3.5737\n",
      "Epoch 3/30, Loss: 0.2922, Train Acc: 0.8926, Val Acc: 0.5221\n",
      "Epoch 3/30, Loss: 0.2922, Train Acc: 0.8926, Val Acc: 0.5221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Acc: 0.8926 | Val Acc: 0.5221 | Val Macro-F1: 0.3652 | Val Loss: 3.5737\n",
      "Epoch 3/30, Loss: 0.2922, Train Acc: 0.8926, Val Acc: 0.5221\n",
      "Best model updated and saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/model_C_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.60batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.24batch/s]\n",
      "Epoch 4/30, Val Acc: 0.5241, Val Macro-F1: 0.4940, Val Loss: 3.8525\n",
      "Epoch 4/30, Val Acc: 0.5241, Val Macro-F1: 0.4940, Val Loss: 3.8525\n",
      "Epoch 4/30, Loss: 0.2681, Train Acc: 0.9038, Val Acc: 0.5241\n",
      "Epoch 4/30, Loss: 0.2681, Train Acc: 0.9038, Val Acc: 0.5241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Acc: 0.9038 | Val Acc: 0.5241 | Val Macro-F1: 0.4940 | Val Loss: 3.8525\n",
      "Epoch 4/30, Loss: 0.2681, Train Acc: 0.9038, Val Acc: 0.5241\n",
      "Best model updated and saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/model_C_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.61batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.67batch/s]\n",
      "Epoch 5/30, Val Acc: 0.7546, Val Macro-F1: 0.7223, Val Loss: 0.8661\n",
      "Epoch 5/30, Val Acc: 0.7546, Val Macro-F1: 0.7223, Val Loss: 0.8661\n",
      "Epoch 5/30, Loss: 0.2559, Train Acc: 0.9080, Val Acc: 0.7546\n",
      "Epoch 5/30, Loss: 0.2559, Train Acc: 0.9080, Val Acc: 0.7546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Acc: 0.9080 | Val Acc: 0.7546 | Val Macro-F1: 0.7223 | Val Loss: 0.8661\n",
      "Epoch 5/30, Loss: 0.2559, Train Acc: 0.9080, Val Acc: 0.7546\n",
      "Best model updated and saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/model_C_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.58batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.53batch/s]\n",
      "Epoch 6/30, Val Acc: 0.7643, Val Macro-F1: 0.7259, Val Loss: 0.7961\n",
      "Epoch 6/30, Val Acc: 0.7643, Val Macro-F1: 0.7259, Val Loss: 0.7961\n",
      "Epoch 6/30, Loss: 0.2381, Train Acc: 0.9144, Val Acc: 0.7643\n",
      "Epoch 6/30, Loss: 0.2381, Train Acc: 0.9144, Val Acc: 0.7643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Acc: 0.9144 | Val Acc: 0.7643 | Val Macro-F1: 0.7259 | Val Loss: 0.7961\n",
      "Epoch 6/30, Loss: 0.2381, Train Acc: 0.9144, Val Acc: 0.7643\n",
      "Best model updated and saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/model_C_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.56batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.29batch/s]\n",
      "Epoch 7/30, Val Acc: 0.6504, Val Macro-F1: 0.5244, Val Loss: 3.1469\n",
      "Epoch 7/30, Val Acc: 0.6504, Val Macro-F1: 0.5244, Val Loss: 3.1469\n",
      "Epoch 7/30, Loss: 0.2197, Train Acc: 0.9193, Val Acc: 0.6504\n",
      "Epoch 7/30, Loss: 0.2197, Train Acc: 0.9193, Val Acc: 0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Acc: 0.9193 | Val Acc: 0.6504 | Val Macro-F1: 0.5244 | Val Loss: 3.1469\n",
      "Epoch 7/30, Loss: 0.2197, Train Acc: 0.9193, Val Acc: 0.6504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.53batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.32batch/s]\n",
      "Epoch 8/30, Val Acc: 0.7240, Val Macro-F1: 0.5495, Val Loss: 1.3945\n",
      "Epoch 8/30, Val Acc: 0.7240, Val Macro-F1: 0.5495, Val Loss: 1.3945\n",
      "Epoch 8/30, Loss: 0.2546, Train Acc: 0.9059, Val Acc: 0.7240\n",
      "Epoch 8/30, Loss: 0.2546, Train Acc: 0.9059, Val Acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Acc: 0.9059 | Val Acc: 0.7240 | Val Macro-F1: 0.5495 | Val Loss: 1.3945\n",
      "Epoch 8/30, Loss: 0.2546, Train Acc: 0.9059, Val Acc: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.48batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.91batch/s]\n",
      "Epoch 9/30, Val Acc: 0.6178, Val Macro-F1: 0.4852, Val Loss: 1.3632\n",
      "Epoch 9/30, Val Acc: 0.6178, Val Macro-F1: 0.4852, Val Loss: 1.3632\n",
      "Epoch 9/30, Loss: 0.2203, Train Acc: 0.9152, Val Acc: 0.6178\n",
      "Epoch 9/30, Loss: 0.2203, Train Acc: 0.9152, Val Acc: 0.6178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Acc: 0.9152 | Val Acc: 0.6178 | Val Macro-F1: 0.4852 | Val Loss: 1.3632\n",
      "Epoch 9/30, Loss: 0.2203, Train Acc: 0.9152, Val Acc: 0.6178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/C/model_C_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.50batch/s]\n",
      "Epoch 10/30, Val Acc: 0.5332, Val Macro-F1: 0.3327, Val Loss: 2.1522\n",
      "Epoch 10/30, Val Acc: 0.5332, Val Macro-F1: 0.3327, Val Loss: 2.1522\n",
      "Epoch 10/30, Loss: 0.2011, Train Acc: 0.9271, Val Acc: 0.5332\n",
      "Epoch 10/30, Loss: 0.2011, Train Acc: 0.9271, Val Acc: 0.5332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Acc: 0.9271 | Val Acc: 0.5332 | Val Macro-F1: 0.3327 | Val Loss: 2.1522\n",
      "Epoch 10/30, Loss: 0.2011, Train Acc: 0.9271, Val Acc: 0.5332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.42batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.95batch/s]\n",
      "Epoch 11/30, Val Acc: 0.6074, Val Macro-F1: 0.4893, Val Loss: 4.2514\n",
      "Epoch 11/30, Val Acc: 0.6074, Val Macro-F1: 0.4893, Val Loss: 4.2514\n",
      "Epoch 11/30, Loss: 0.2508, Train Acc: 0.9110, Val Acc: 0.6074\n",
      "Epoch 11/30, Loss: 0.2508, Train Acc: 0.9110, Val Acc: 0.6074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Acc: 0.9110 | Val Acc: 0.6074 | Val Macro-F1: 0.4893 | Val Loss: 4.2514\n",
      "Epoch 11/30, Loss: 0.2508, Train Acc: 0.9110, Val Acc: 0.6074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.50batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.11batch/s]\n",
      "Epoch 12/30, Val Acc: 0.7070, Val Macro-F1: 0.5834, Val Loss: 1.8654\n",
      "Epoch 12/30, Val Acc: 0.7070, Val Macro-F1: 0.5834, Val Loss: 1.8654\n",
      "Epoch 12/30, Loss: 0.2054, Train Acc: 0.9246, Val Acc: 0.7070\n",
      "Epoch 12/30, Loss: 0.2054, Train Acc: 0.9246, Val Acc: 0.7070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Acc: 0.9246 | Val Acc: 0.7070 | Val Macro-F1: 0.5834 | Val Loss: 1.8654\n",
      "Epoch 12/30, Loss: 0.2054, Train Acc: 0.9246, Val Acc: 0.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.53batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.21batch/s]\n",
      "Epoch 13/30, Val Acc: 0.7337, Val Macro-F1: 0.6410, Val Loss: 0.9934\n",
      "Epoch 13/30, Val Acc: 0.7337, Val Macro-F1: 0.6410, Val Loss: 0.9934\n",
      "Epoch 13/30, Loss: 0.2123, Train Acc: 0.9225, Val Acc: 0.7337\n",
      "Epoch 13/30, Loss: 0.2123, Train Acc: 0.9225, Val Acc: 0.7337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Acc: 0.9225 | Val Acc: 0.7337 | Val Macro-F1: 0.6410 | Val Loss: 0.9934\n",
      "Epoch 13/30, Loss: 0.2123, Train Acc: 0.9225, Val Acc: 0.7337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.56batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.93batch/s]\n",
      "Epoch 14/30, Val Acc: 0.7337, Val Macro-F1: 0.6590, Val Loss: 0.9673\n",
      "Epoch 14/30, Val Acc: 0.7337, Val Macro-F1: 0.6590, Val Loss: 0.9673\n",
      "Epoch 14/30, Loss: 0.1903, Train Acc: 0.9320, Val Acc: 0.7337\n",
      "Epoch 14/30, Loss: 0.1903, Train Acc: 0.9320, Val Acc: 0.7337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Acc: 0.9320 | Val Acc: 0.7337 | Val Macro-F1: 0.6590 | Val Loss: 0.9673\n",
      "Epoch 14/30, Loss: 0.1903, Train Acc: 0.9320, Val Acc: 0.7337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.52batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.94batch/s]\n",
      "Epoch 15/30, Val Acc: 0.6576, Val Macro-F1: 0.5079, Val Loss: 2.5327\n",
      "Epoch 15/30, Val Acc: 0.6576, Val Macro-F1: 0.5079, Val Loss: 2.5327\n",
      "Epoch 15/30, Loss: 0.1930, Train Acc: 0.9313, Val Acc: 0.6576\n",
      "Epoch 15/30, Loss: 0.1930, Train Acc: 0.9313, Val Acc: 0.6576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Acc: 0.9313 | Val Acc: 0.6576 | Val Macro-F1: 0.5079 | Val Loss: 2.5327\n",
      "Epoch 15/30, Loss: 0.1930, Train Acc: 0.9313, Val Acc: 0.6576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.50batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.31batch/s]\n",
      "Epoch 16/30, Val Acc: 0.6628, Val Macro-F1: 0.4701, Val Loss: 1.5181\n",
      "Epoch 16/30, Val Acc: 0.6628, Val Macro-F1: 0.4701, Val Loss: 1.5181\n",
      "Epoch 16/30, Loss: 0.2053, Train Acc: 0.9227, Val Acc: 0.6628\n",
      "Epoch 16/30, Loss: 0.2053, Train Acc: 0.9227, Val Acc: 0.6628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Acc: 0.9227 | Val Acc: 0.6628 | Val Macro-F1: 0.4701 | Val Loss: 1.5181\n",
      "Epoch 16/30, Loss: 0.2053, Train Acc: 0.9227, Val Acc: 0.6628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.52batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.09batch/s]\n",
      "Epoch 17/30, Val Acc: 0.7318, Val Macro-F1: 0.6009, Val Loss: 1.2978\n",
      "Epoch 17/30, Val Acc: 0.7318, Val Macro-F1: 0.6009, Val Loss: 1.2978\n",
      "Epoch 17/30, Loss: 0.1814, Train Acc: 0.9310, Val Acc: 0.7318\n",
      "Epoch 17/30, Loss: 0.1814, Train Acc: 0.9310, Val Acc: 0.7318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Acc: 0.9310 | Val Acc: 0.7318 | Val Macro-F1: 0.6009 | Val Loss: 1.2978\n",
      "Epoch 17/30, Loss: 0.1814, Train Acc: 0.9310, Val Acc: 0.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.42batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.85batch/s]\n",
      "Epoch 18/30, Val Acc: 0.7070, Val Macro-F1: 0.5494, Val Loss: 1.3319\n",
      "Epoch 18/30, Val Acc: 0.7070, Val Macro-F1: 0.5494, Val Loss: 1.3319\n",
      "Epoch 18/30, Loss: 0.1758, Train Acc: 0.9347, Val Acc: 0.7070\n",
      "Epoch 18/30, Loss: 0.1758, Train Acc: 0.9347, Val Acc: 0.7070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Acc: 0.9347 | Val Acc: 0.7070 | Val Macro-F1: 0.5494 | Val Loss: 1.3319\n",
      "Epoch 18/30, Loss: 0.1758, Train Acc: 0.9347, Val Acc: 0.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.35batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.15batch/s]\n",
      "Epoch 19/30, Val Acc: 0.7103, Val Macro-F1: 0.5724, Val Loss: 1.5187\n",
      "Epoch 19/30, Val Acc: 0.7103, Val Macro-F1: 0.5724, Val Loss: 1.5187\n",
      "Epoch 19/30, Loss: 0.1750, Train Acc: 0.9372, Val Acc: 0.7103\n",
      "Epoch 19/30, Loss: 0.1750, Train Acc: 0.9372, Val Acc: 0.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Acc: 0.9372 | Val Acc: 0.7103 | Val Macro-F1: 0.5724 | Val Loss: 1.5187\n",
      "Epoch 19/30, Loss: 0.1750, Train Acc: 0.9372, Val Acc: 0.7103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/valerio/Desktop/noisy_labels/hackaton/checkpoints/C/model_C_epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.44batch/s]\n",
      "Epoch 20/30, Val Acc: 0.6667, Val Macro-F1: 0.5122, Val Loss: 1.8943\n",
      "Epoch 20/30, Val Acc: 0.6667, Val Macro-F1: 0.5122, Val Loss: 1.8943\n",
      "Epoch 20/30, Loss: 0.1680, Train Acc: 0.9385, Val Acc: 0.6667\n",
      "Epoch 20/30, Loss: 0.1680, Train Acc: 0.9385, Val Acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Acc: 0.9385 | Val Acc: 0.6667 | Val Macro-F1: 0.5122 | Val Loss: 1.8943\n",
      "Epoch 20/30, Loss: 0.1680, Train Acc: 0.9385, Val Acc: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.40batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.30batch/s]\n",
      "Epoch 21/30, Val Acc: 0.7383, Val Macro-F1: 0.6122, Val Loss: 1.2281\n",
      "Epoch 21/30, Val Acc: 0.7383, Val Macro-F1: 0.6122, Val Loss: 1.2281\n",
      "Epoch 21/30, Loss: 0.1654, Train Acc: 0.9437, Val Acc: 0.7383\n",
      "Epoch 21/30, Loss: 0.1654, Train Acc: 0.9437, Val Acc: 0.7383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Acc: 0.9437 | Val Acc: 0.7383 | Val Macro-F1: 0.6122 | Val Loss: 1.2281\n",
      "Epoch 21/30, Loss: 0.1654, Train Acc: 0.9437, Val Acc: 0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.31batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.06batch/s]\n",
      "Epoch 22/30, Val Acc: 0.6276, Val Macro-F1: 0.4640, Val Loss: 1.8387\n",
      "Epoch 22/30, Val Acc: 0.6276, Val Macro-F1: 0.4640, Val Loss: 1.8387\n",
      "Epoch 22/30, Loss: 0.1606, Train Acc: 0.9414, Val Acc: 0.6276\n",
      "Epoch 22/30, Loss: 0.1606, Train Acc: 0.9414, Val Acc: 0.6276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Acc: 0.9414 | Val Acc: 0.6276 | Val Macro-F1: 0.4640 | Val Loss: 1.8387\n",
      "Epoch 22/30, Loss: 0.1606, Train Acc: 0.9414, Val Acc: 0.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.35batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.67batch/s]\n",
      "Epoch 23/30, Val Acc: 0.7227, Val Macro-F1: 0.5898, Val Loss: 1.5625\n",
      "Epoch 23/30, Val Acc: 0.7227, Val Macro-F1: 0.5898, Val Loss: 1.5625\n",
      "Epoch 23/30, Loss: 0.1634, Train Acc: 0.9403, Val Acc: 0.7227\n",
      "Epoch 23/30, Loss: 0.1634, Train Acc: 0.9403, Val Acc: 0.7227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Acc: 0.9403 | Val Acc: 0.7227 | Val Macro-F1: 0.5898 | Val Loss: 1.5625\n",
      "Epoch 23/30, Loss: 0.1634, Train Acc: 0.9403, Val Acc: 0.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.58batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 10.93batch/s]\n",
      "Epoch 24/30, Val Acc: 0.7253, Val Macro-F1: 0.5886, Val Loss: 1.5188\n",
      "Epoch 24/30, Val Acc: 0.7253, Val Macro-F1: 0.5886, Val Loss: 1.5188\n",
      "Epoch 24/30, Loss: 0.1716, Train Acc: 0.9390, Val Acc: 0.7253\n",
      "Epoch 24/30, Loss: 0.1716, Train Acc: 0.9390, Val Acc: 0.7253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Acc: 0.9390 | Val Acc: 0.7253 | Val Macro-F1: 0.5886 | Val Loss: 1.5188\n",
      "Epoch 24/30, Loss: 0.1716, Train Acc: 0.9390, Val Acc: 0.7253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.45batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.41batch/s]\n",
      "Epoch 25/30, Val Acc: 0.6048, Val Macro-F1: 0.5596, Val Loss: 2.0471\n",
      "Epoch 25/30, Val Acc: 0.6048, Val Macro-F1: 0.5596, Val Loss: 2.0471\n",
      "Epoch 25/30, Loss: 0.1545, Train Acc: 0.9421, Val Acc: 0.6048\n",
      "Epoch 25/30, Loss: 0.1545, Train Acc: 0.9421, Val Acc: 0.6048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Acc: 0.9421 | Val Acc: 0.6048 | Val Macro-F1: 0.5596 | Val Loss: 2.0471\n",
      "Epoch 25/30, Loss: 0.1545, Train Acc: 0.9421, Val Acc: 0.6048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.21batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.61batch/s]\n",
      "Epoch 26/30, Val Acc: 0.6686, Val Macro-F1: 0.4891, Val Loss: 1.6308\n",
      "Epoch 26/30, Val Acc: 0.6686, Val Macro-F1: 0.4891, Val Loss: 1.6308\n",
      "Epoch 26/30, Loss: 0.1490, Train Acc: 0.9474, Val Acc: 0.6686\n",
      "Epoch 26/30, Loss: 0.1490, Train Acc: 0.9474, Val Acc: 0.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Acc: 0.9474 | Val Acc: 0.6686 | Val Macro-F1: 0.4891 | Val Loss: 1.6308\n",
      "Epoch 26/30, Loss: 0.1490, Train Acc: 0.9474, Val Acc: 0.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 192/192 [00:13<00:00, 14.34batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00, 11.14batch/s]\n",
      "Epoch 27/30, Val Acc: 0.6556, Val Macro-F1: 0.5099, Val Loss: 1.4027\n",
      "Epoch 27/30, Val Acc: 0.6556, Val Macro-F1: 0.5099, Val Loss: 1.4027\n",
      "Epoch 27/30, Loss: 0.1650, Train Acc: 0.9417, Val Acc: 0.6556\n",
      "Epoch 27/30, Loss: 0.1650, Train Acc: 0.9417, Val Acc: 0.6556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Acc: 0.9417 | Val Acc: 0.6556 | Val Macro-F1: 0.5099 | Val Loss: 1.4027\n",
      "Epoch 27/30, Loss: 0.1650, Train Acc: 0.9417, Val Acc: 0.6556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs:  82%|████████▏ | 157/192 [00:11<00:02, 13.66batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     checkpoint_intervals \u001b[38;5;241m=\u001b[39m [num_epochs]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 44\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_intervals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_dir_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m evaluate(val_loader, model, device, calculate_accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/noisy_labels/hackaton/src/models.py:57\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, batched_data)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_data):\n\u001b[0;32m---> 57\u001b[0m     h_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     h_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(h_node, batched_data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_pred_linear(h_graph)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/noisy_labels/hackaton/src/conv.py:219\u001b[0m, in \u001b[0;36mGNN_node_Virtualnode.forward\u001b[0;34m(self, batched_data)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m### update the virtual nodes\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layer \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m### add message from graph nodes to virtual nodes\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     virtualnode_embedding_temp \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_add_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m virtualnode_embedding\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m### transform virtual nodes using MLP\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/torch_geometric/nn/pool/glob.py:34\u001b[0m, in \u001b[0;36mglobal_add_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/hackaton/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py:53\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `dim` argument must lay between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# For now, we maintain various different code paths, based on whether\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# the input requires gradients and whether it lays on the CPU/GPU.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# For example, `torch_scatter` is usually faster than\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# indices, but is therefore way slower in its backward implementation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# More insights can be found in `test/utils/test_scatter.py`.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize()[:dim] \u001b[38;5;241m+\u001b[39m (dim_size, ) \u001b[38;5;241m+\u001b[39m src\u001b[38;5;241m.\u001b[39msize()[dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "if args.train_path:\n",
    "    # Carico l’intero dataset e lo splitto\n",
    "\n",
    "    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # labels = [d.y.item() for d in train_dataset]\n",
    "    # label_counts = Counter(labels)\n",
    "    # total = len(labels)\n",
    "\n",
    "\n",
    "    # weights = [1.0 / label_counts[d.y.item()] for d in train_dataset]\n",
    "\n",
    "    criterion = SCELoss(num_classes=6, alpha=0.1, beta=1.0) \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    num_epochs = 30\n",
    "    best_val_accuracy = 0.0   \n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    if num_checkpoints > 1:\n",
    "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "    else:\n",
    "        checkpoint_intervals = [num_epochs]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader, model, optimizer, criterion, device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
    "            current_epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc, val_f1 = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Acc: {val_acc:.4f} | \"\n",
    "            f\"Val Macro-F1: {val_f1:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "            f\"Val Acc: {val_acc:.4f}, \"\n",
    "            f\"Val Macro-F1: {val_f1:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # if val_f1 > best_f1 + 1e-4:\n",
    "        #     best_epoch = epoch + 1\n",
    "        #     best_f1 = val_f1\n",
    "            \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))\n",
    "    plot_all_metrics(train_losses, train_accuracies, val_losses, val_accuracies, os.path.join(logs_folder, \"plots_all_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_dataset\n",
    "del train_loader\n",
    "del full_dataset\n",
    "del val_dataset\n",
    "del val_loader\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "xsXZIj4Mdu3I",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = GraphDataset(args.test_path, transform=add_zeros)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "x1OnGq_nCmTr",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 48/48 [00:04<00:00,  9.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /home/valerio/Desktop/noisy_labels/hackaton/submission/testset_C.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     predictions = []\n",
    "#     total_loss = 0\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     with torch.no_grad():\n",
    "#         for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "#             print(data)\n",
    "#             data = data.to(device)\n",
    "#             output = model(data)\n",
    "#             pred = output.argmax(dim=1)\n",
    "            \n",
    "#             if calculate_accuracy:\n",
    "#                 print(f\"{pred=}, {data.y=}\")\n",
    "#                 print(f\"{type(pred)=}, {type(data.y)=}\")\n",
    "#                 # print(f\"{pred data. }\")\n",
    "\n",
    "#                 correct += (pred == data.y).sum().item()\n",
    "#                 total += data.y.size(0)\n",
    "#                 total_loss += criterion(output, data.y).item()\n",
    "#             else:\n",
    "#                 predictions.extend(pred.cpu().numpy())\n",
    "#     if calculate_accuracy:\n",
    "#         accuracy = correct / total\n",
    "#         return  total_loss / len(data_loader),accuracy\n",
    "#     return predictions\n",
    "\n",
    "# model.load_state_dict(torch.load(checkpoint_path))\n",
    "# predictions = evaluate(test_loader, model, device, calculate_accuracy=True)\n",
    "# save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
