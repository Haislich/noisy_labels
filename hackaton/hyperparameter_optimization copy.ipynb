{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from functools import partial\n",
    "import optuna\n",
    "import gc\n",
    "from typing import Literal\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, Dataset\n",
    "\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\"id\": test_graph_ids, \"pred\": predictions})\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy per Epoch\")\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedSubset(Dataset):\n",
    "    def __init__(self, subset: Subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.subset[i]\n",
    "        data.idx = torch.tensor(i, dtype=torch.long) # type: ignore\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCODLoss(nn.Module):\n",
    "    past_embeddings: torch.Tensor\n",
    "    centroids: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        embedding_dimensions: int = 300,\n",
    "        total_epochs: int = 150,\n",
    "        lambda_consistency: float = 1.0,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        dataset : iterable whose elements expose an integer label in `elem.y`\n",
    "        embedding_dimensions : size of the feature vectors\n",
    "        total_epochs : number of training epochs (used for centroid update schedule)\n",
    "        lambda_consistency : weight for the MSE consistency term\n",
    "        device : cuda / cpu device.  If None, picks CUDA if available.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device or torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "\n",
    "        labels = [int(elem.y) for elem in dataset]\n",
    "        self.num_elements = len(labels)\n",
    "        self.num_classes = max(labels) + 1  # robust to gaps (e.g. labels {1,3})\n",
    "        # self.register_buffer(\"bins\", torch.empty(self.num_classes, 0, dtype=torch.long))\n",
    "\n",
    "        # Convert bins to a list-of-lists for easy appends, then to tensors\n",
    "        tmp_bins: list[list[int]] = [[] for _ in range(self.num_classes)]\n",
    "        for idx, lab in enumerate(labels):\n",
    "            tmp_bins[lab].append(idx)\n",
    "        self.bins = [\n",
    "            torch.as_tensor(b, dtype=torch.long, device=self.device) for b in tmp_bins\n",
    "        ]\n",
    "\n",
    "        # Confidence parameter per sample (trainable!)\n",
    "        self.u = nn.Parameter(torch.empty(self.num_elements, 1, device=self.device))\n",
    "        nn.init.normal_(self.u, mean=1e-8, std=1e-9)\n",
    "\n",
    "        # Running memory of embeddings\n",
    "        self.register_buffer(\n",
    "            \"past_embeddings\",\n",
    "            torch.rand(\n",
    "                self.num_elements, self.embedding_dimensions, device=self.device\n",
    "            ),\n",
    "        )\n",
    "        # Class centroids\n",
    "        self.register_buffer(\n",
    "            \"centroids\",\n",
    "            torch.rand(self.num_classes, self.embedding_dimensions, device=self.device),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        logits: torch.Tensor,  # (B, C)\n",
    "        indexes: torch.Tensor,  # (B,) – dataset indices of the current batch\n",
    "        embeddings: torch.Tensor,  # (B, D)\n",
    "        targets: torch.Tensor,  # (B,)\n",
    "        epoch: int,\n",
    "    ) -> torch.Tensor:\n",
    "        eps = 1e-6\n",
    "\n",
    "        # Keep an L2-normalised copy of current embeddings\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        self.past_embeddings[indexes] = embeddings.detach()\n",
    "\n",
    "        # ---------------- Centroid update ------------------------------------\n",
    "        if epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                for c, idxs in enumerate(self.bins):\n",
    "                    if idxs.numel():\n",
    "                        self.centroids[c] = self.past_embeddings[idxs].mean(0)\n",
    "        else:\n",
    "            # Shrink the subset of samples that contribute to the centroid\n",
    "            percent = int(max(1, min(100, 50 + 50 * (1 - epoch / self.total_epochs))))\n",
    "            for c, idxs in enumerate(self.bins):\n",
    "                if idxs.numel() == 0:\n",
    "                    continue\n",
    "                # bottom-k u’s  (small u  ⇒ low confidence ⇒ smaller weight)\n",
    "                k = max(1, idxs.numel() * percent // 100)\n",
    "                u_batch = self.u[idxs].squeeze(1)\n",
    "                keep = torch.topk(u_batch, k, largest=False).indices  # (k,)\n",
    "                selected = idxs[keep]  # (k,)\n",
    "                self.centroids[c] = self.past_embeddings[selected].mean(0)\n",
    "\n",
    "        centroids = F.normalize(self.centroids, dim=1)  # (C, D)\n",
    "\n",
    "        # ---------------- Probability shaping --------------------------------\n",
    "        soft_labels = F.softmax(embeddings @ centroids.T, dim=1)  # (B, C)\n",
    "        probs = F.softmax(logits, dim=1)  # (B, C)\n",
    "        u_vals = torch.sigmoid(self.u[indexes]).squeeze(1)  # (B,)\n",
    "\n",
    "        adjusted = (probs + u_vals[:, None] * soft_labels).clamp(min=eps)\n",
    "        adjusted = adjusted / adjusted.sum(1, keepdim=True)\n",
    "\n",
    "        # ---------------- Loss terms -----------------------------------------\n",
    "        hard_ce = (\n",
    "            (1.0 - u_vals) * F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        ).mean()\n",
    "        soft_ce = -(soft_labels * torch.log(adjusted)).sum(1).mean()\n",
    "        consistency = F.mse_loss(adjusted, soft_labels)\n",
    "\n",
    "        return hard_ce + soft_ce + self.lambda_consistency * consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    optimizer_theta: torch.optim.Optimizer,\n",
    "    optimizer_u: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    checkpoint_path:str,\n",
    "    current_epoch: int,\n",
    "    save_checkpoints: bool = True,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        node_emb = model.gnn_node(batch)\n",
    "        graph_emb = model.pool(node_emb, batch.batch)\n",
    "        logits = model.graph_pred_linear(graph_emb)\n",
    "\n",
    "        if isinstance(criterion, NCODLoss):\n",
    "            loss = criterion(\n",
    "                logits=logits,\n",
    "                indexes=batch.idx.to(device),\n",
    "                embeddings=graph_emb,\n",
    "                targets=batch.y.to(device),\n",
    "                epoch=current_epoch,\n",
    "            )\n",
    "        else:\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "        optimizer_theta.zero_grad(set_to_none=True)\n",
    "        optimizer_u.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_theta.step()\n",
    "        optimizer_u.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            batch_size = batch.y.size(0)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), 1)).sum().item()\n",
    "            )\n",
    "\n",
    "    if save_checkpoints:\n",
    "        ckpt = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt)\n",
    "        print(f\"[checkpoint] saved: {ckpt}\")\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss, avg_confidence, accuracy, avg_entropy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            node_emb = model.gnn_node(batch)\n",
    "            graph_emb = model.pool(node_emb, batch.batch)\n",
    "            logits = model.graph_pred_linear(graph_emb)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "            batch_size = batch.y.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), dim=1)).sum().item()\n",
    "            )\n",
    "\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return loss, confidence, accuracy, entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial,\n",
    "    train_loader:DataLoader,\n",
    "    val_loader:DataLoader,\n",
    "    num_checkpoints:int,\n",
    "    checkpoints_dir:str,\n",
    "    run_name:str,\n",
    "    best_model_path:str,\n",
    "    logs_dir:str,\n",
    "    resume_training:bool,\n",
    "    *,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"confidence\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):  # -> float | Any:\n",
    "    logging.info(\"#\" * 80)\n",
    "    # Hyperparameter search space\n",
    "    logging.info(\"Start case study with parameters:\")\n",
    "    gnn_type = \"gin-virtual\"\n",
    "    # trial.suggest_categorical(\n",
    "    #     \"gnn_type\", [\"gin\", ]\n",
    "    # )\n",
    "    drop_ratio = 0.0  # trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "    num_layers = 5  # trial.suggest_int(\"num_layers\", 6, 12)\n",
    "    embedding_dim = 300  # trial.suggest_categorical(\"embedding_dim\", [64, 128, 300])\n",
    "    num_epochs = 100  # trial.suggest_int(\"num_epochs\", 10, 11, step=1)\n",
    "\n",
    "    logging.info(f\"{gnn_type=}\")\n",
    "    logging.info(f\"{drop_ratio=}\")\n",
    "    logging.info(f\"{num_layers=}\")\n",
    "    logging.info(f\"{embedding_dim=}\")\n",
    "    logging.info(f\"{num_epochs=}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNN(\n",
    "        gnn_type=\"gin\" if \"gin\" in gnn_type else \"gcn\",\n",
    "        num_class=6,\n",
    "        num_layer=num_layers,\n",
    "        emb_dim=embedding_dim,\n",
    "        drop_ratio=drop_ratio,\n",
    "        virtual_node=\"virtual\" in gnn_type,\n",
    "    ).to(device)\n",
    "    if resume_training:\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "    # NCOD by default — switch to CE for testing if needed\n",
    "    train_criterion = NCODLoss(\n",
    "        train_loader.dataset,\n",
    "        embedding_dimensions=embedding_dim,\n",
    "        total_epochs=num_epochs,\n",
    "    ).to(device)\n",
    "    val_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_theta = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "    optimizer_u = torch.optim.SGD(train_criterion.parameters(), lr=1e-3)\n",
    "\n",
    "    # Checkpoint logic\n",
    "    checkpoint_epochs = [\n",
    "        int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)\n",
    "    ]\n",
    "\n",
    "    best_val_score = -float(\"inf\")\n",
    "    train_losses, train_confs, train_accs, train_entropies, train_scores = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "    val_losses, val_confs, val_accs, val_entropies, val_scores = [], [], [], [], []\n",
    "\n",
    "    progress_bar = tqdm(range(num_epochs), desc=\"Training...\", leave=False)\n",
    "    for epoch in progress_bar:\n",
    "        train_loss, train_conf, train_acc, train_entropy = train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer_theta,\n",
    "            optimizer_u,\n",
    "            train_criterion,\n",
    "            device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_epochs),\n",
    "            checkpoint_path=os.path.join(checkpoints_dir, f\"model_{run_name}\"),\n",
    "            current_epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_loss, val_conf, val_acc, val_entropy = evaluate(\n",
    "            val_loader,\n",
    "            model,\n",
    "            val_criterion,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"Epoch {epoch}/{num_epochs}\\n\"\n",
    "            f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}\\n\"\n",
    "            f\"Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\\n\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch}/{num_epochs}| \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}| \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\"\n",
    "        )\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_confs.append(train_conf)\n",
    "        train_entropies.append(train_entropy)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_confs.append(val_conf)\n",
    "        val_entropies.append(val_entropy)\n",
    "\n",
    "        if score_type == \"loss\":\n",
    "            train_score = train_loss\n",
    "            val_score = val_loss\n",
    "        elif score_type == \"entropy\":\n",
    "            train_score = train_entropy\n",
    "            val_score = val_entropy\n",
    "        elif score_type == \"confidence\":\n",
    "            train_score = train_conf\n",
    "            val_score = val_conf\n",
    "        else:\n",
    "            train_score = train_acc\n",
    "            val_score = val_acc\n",
    "\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            logging.info(f\"[{run_name}] Best model updated at {best_model_path}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plot_training_progress(\n",
    "        train_losses, train_scores, os.path.join(logs_dir, \"train_plots\")\n",
    "    )\n",
    "    plot_training_progress(val_losses, val_scores, os.path.join(logs_dir, \"val_plots\"))\n",
    "\n",
    "    logging.info(f\"Case study end, {best_val_score}\")\n",
    "    logging.info(\"#\" * 80)\n",
    "    logging.info(\"\\n\")\n",
    "\n",
    "    return best_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_study(\n",
    "    dataset_name: Literal[\"A\", \"B\", \"C\", \"D\"],\n",
    "    resume_training: bool,\n",
    "    n_trials: int = 30,\n",
    "    num_checkpoints: int = 10,\n",
    "    default_batch_size: int = 32,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"confidence\",\n",
    "    full_dataset=None,\n",
    "):\n",
    "    script_root = os.getcwd()\n",
    "    train_path = f\"./datasets/{dataset_name}/train.json.gz\"\n",
    "    run_name = dataset_name\n",
    "\n",
    "    logs_dir = os.path.join(script_root, \"logs\", run_name)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_dir, \"training.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    # logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "    checkpoints_dir = os.path.join(script_root, \"checkpoints\", run_name)\n",
    "    best_model_path = os.path.join(checkpoints_dir, f\"model_{run_name}_best.pth\")\n",
    "    summary_csv_path = os.path.join(logs_dir, f\"optuna_summary_{dataset_name}.csv\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    if full_dataset is None:\n",
    "        full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    train_dataset = IndexedSubset(train_dataset)\n",
    "    val_dataset = IndexedSubset(val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,  # type:ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,  # type:ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Starting Optuna optimization for dataset {dataset_name}\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=run_name,\n",
    "        direction=\"minimize\" if score_type == \"loss\" or score_type == \"entropy\" else \"maximize\",\n",
    "    )\n",
    "\n",
    "    obj = partial(\n",
    "        objective,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_checkpoints=num_checkpoints,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        run_name=run_name,\n",
    "        best_model_path=best_model_path,\n",
    "        logs_dir=logs_dir,\n",
    "        resume_training=resume_training,\n",
    "        score_type=score_type,\n",
    "    )\n",
    "    study.optimize(obj, n_trials=n_trials)\n",
    "\n",
    "    all_trials = []\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            row = {score_type: trial.value}\n",
    "            row.update(trial.params)\n",
    "            all_trials.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_trials)\n",
    "    results_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nAll trials saved to: {summary_csv_path}\")\n",
    "    print(f\"\\nBest result for dataset {dataset_name}:\")\n",
    "    display(results_df.sort_values(score_type, ascending=(score_type == \"loss\" or score_type == \"entropy\")))\n",
    "    print(f\"\\nBest Params for {dataset_name}:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_a = GraphDataset(\"./datasets/A/train.json.gz\", transform=add_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-26 19:01:04,164] A new study created in memory with name: A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna optimization for dataset A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e964daa0da5c412f99bbf288e614ae5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "Train Loss: 2.6584, Accuracy: 0.3339, Confidence: 0.2462, Entropy: 1.7470\n",
      "Val Loss  : 1.7846, Accuracy: 0.2540, Confidence: 0.2438, Entropy: 1.7399\n",
      "\n",
      "Epoch 1/100\n",
      "Train Loss: 2.6101, Accuracy: 0.3629, Confidence: 0.2835, Entropy: 1.7088\n",
      "Val Loss  : 1.7406, Accuracy: 0.2779, Confidence: 0.2431, Entropy: 1.7369\n",
      "\n",
      "Epoch 2/100\n",
      "Train Loss: 2.5422, Accuracy: 0.3700, Confidence: 0.3111, Entropy: 1.6701\n",
      "Val Loss  : 1.7496, Accuracy: 0.3320, Confidence: 0.2936, Entropy: 1.6710\n",
      "\n",
      "Epoch 3/100\n",
      "Train Loss: 2.4570, Accuracy: 0.3462, Confidence: 0.3242, Entropy: 1.6437\n",
      "Val Loss  : 1.7293, Accuracy: 0.2877, Confidence: 0.2904, Entropy: 1.6612\n",
      "\n",
      "Epoch 4/100\n",
      "Train Loss: 2.4060, Accuracy: 0.3147, Confidence: 0.3172, Entropy: 1.6338\n",
      "Val Loss  : 1.6764, Accuracy: 0.3152, Confidence: 0.3158, Entropy: 1.6642\n",
      "\n",
      "Epoch 5/100\n",
      "Train Loss: 2.4243, Accuracy: 0.3100, Confidence: 0.3124, Entropy: 1.6447\n",
      "Val Loss  : 1.7544, Accuracy: 0.2531, Confidence: 0.3029, Entropy: 1.6449\n",
      "\n",
      "Epoch 6/100\n",
      "Train Loss: 2.3943, Accuracy: 0.3321, Confidence: 0.3155, Entropy: 1.6272\n",
      "Val Loss  : 1.6354, Accuracy: 0.3635, Confidence: 0.3042, Entropy: 1.6276\n",
      "\n",
      "Epoch 7/100\n",
      "Train Loss: 2.3923, Accuracy: 0.3320, Confidence: 0.3248, Entropy: 1.6228\n",
      "Val Loss  : 1.8002, Accuracy: 0.2615, Confidence: 0.3538, Entropy: 1.5910\n",
      "\n",
      "Epoch 8/100\n",
      "Train Loss: 2.3806, Accuracy: 0.3198, Confidence: 0.3245, Entropy: 1.6255\n",
      "Val Loss  : 1.6635, Accuracy: 0.3005, Confidence: 0.3213, Entropy: 1.6451\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_10.pth\n",
      "Epoch 9/100\n",
      "Train Loss: 2.3680, Accuracy: 0.3457, Confidence: 0.3322, Entropy: 1.6128\n",
      "Val Loss  : 1.6243, Accuracy: 0.3187, Confidence: 0.3176, Entropy: 1.6198\n",
      "\n",
      "Epoch 10/100\n",
      "Train Loss: 2.3928, Accuracy: 0.3131, Confidence: 0.3186, Entropy: 1.6293\n",
      "Val Loss  : 1.6630, Accuracy: 0.3311, Confidence: 0.2942, Entropy: 1.6712\n",
      "\n",
      "Epoch 11/100\n",
      "Train Loss: 2.3653, Accuracy: 0.3408, Confidence: 0.3284, Entropy: 1.6159\n",
      "Val Loss  : 1.6813, Accuracy: 0.3125, Confidence: 0.3403, Entropy: 1.6192\n",
      "\n",
      "Epoch 12/100\n",
      "Train Loss: 2.3494, Accuracy: 0.3387, Confidence: 0.3316, Entropy: 1.6081\n",
      "Val Loss  : 1.6714, Accuracy: 0.3285, Confidence: 0.3213, Entropy: 1.6197\n",
      "\n",
      "Epoch 13/100\n",
      "Train Loss: 2.3607, Accuracy: 0.3302, Confidence: 0.3235, Entropy: 1.6182\n",
      "Val Loss  : 1.6545, Accuracy: 0.3422, Confidence: 0.2960, Entropy: 1.6523\n",
      "\n",
      "Epoch 14/100\n",
      "Train Loss: 2.3675, Accuracy: 0.3413, Confidence: 0.3226, Entropy: 1.6183\n",
      "Val Loss  : 1.6294, Accuracy: 0.3511, Confidence: 0.3270, Entropy: 1.6059\n",
      "\n",
      "Epoch 15/100\n",
      "Train Loss: 2.3516, Accuracy: 0.3460, Confidence: 0.3260, Entropy: 1.6058\n",
      "Val Loss  : 1.6672, Accuracy: 0.3129, Confidence: 0.3060, Entropy: 1.6300\n",
      "\n",
      "Epoch 16/100\n",
      "Train Loss: 2.3373, Accuracy: 0.3309, Confidence: 0.3254, Entropy: 1.6067\n",
      "Val Loss  : 1.8046, Accuracy: 0.2509, Confidence: 0.3707, Entropy: 1.5480\n",
      "\n",
      "Epoch 17/100\n",
      "Train Loss: 2.3381, Accuracy: 0.3502, Confidence: 0.3334, Entropy: 1.6007\n",
      "Val Loss  : 1.6246, Accuracy: 0.3630, Confidence: 0.2930, Entropy: 1.6487\n",
      "\n",
      "Epoch 18/100\n",
      "Train Loss: 2.3572, Accuracy: 0.3471, Confidence: 0.3323, Entropy: 1.6076\n",
      "Val Loss  : 1.6666, Accuracy: 0.2815, Confidence: 0.3334, Entropy: 1.6326\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_20.pth\n",
      "Epoch 19/100\n",
      "Train Loss: 2.3429, Accuracy: 0.3685, Confidence: 0.3406, Entropy: 1.5941\n",
      "Val Loss  : 1.5643, Accuracy: 0.3710, Confidence: 0.3613, Entropy: 1.5857\n",
      "\n",
      "Epoch 20/100\n",
      "Train Loss: 2.3452, Accuracy: 0.3646, Confidence: 0.3346, Entropy: 1.5966\n",
      "Val Loss  : 1.6079, Accuracy: 0.3754, Confidence: 0.3350, Entropy: 1.5948\n",
      "\n",
      "Epoch 21/100\n",
      "Train Loss: 2.3433, Accuracy: 0.3675, Confidence: 0.3380, Entropy: 1.5927\n",
      "Val Loss  : 1.6881, Accuracy: 0.2881, Confidence: 0.2952, Entropy: 1.6457\n",
      "\n",
      "Epoch 22/100\n",
      "Train Loss: 2.3397, Accuracy: 0.3553, Confidence: 0.3328, Entropy: 1.6017\n",
      "Val Loss  : 1.5581, Accuracy: 0.3945, Confidence: 0.3507, Entropy: 1.5770\n",
      "\n",
      "Epoch 23/100\n",
      "Train Loss: 2.3328, Accuracy: 0.3599, Confidence: 0.3364, Entropy: 1.5958\n",
      "Val Loss  : 1.6639, Accuracy: 0.3218, Confidence: 0.3018, Entropy: 1.6506\n",
      "\n",
      "Epoch 24/100\n",
      "Train Loss: 2.3404, Accuracy: 0.3599, Confidence: 0.3386, Entropy: 1.5946\n",
      "Val Loss  : 1.5386, Accuracy: 0.3989, Confidence: 0.3397, Entropy: 1.5890\n",
      "\n",
      "Epoch 25/100\n",
      "Train Loss: 2.3460, Accuracy: 0.3731, Confidence: 0.3445, Entropy: 1.5908\n",
      "Val Loss  : 1.5523, Accuracy: 0.3524, Confidence: 0.3497, Entropy: 1.6041\n",
      "\n",
      "Epoch 26/100\n",
      "Train Loss: 2.3323, Accuracy: 0.3885, Confidence: 0.3471, Entropy: 1.5831\n",
      "Val Loss  : 1.5865, Accuracy: 0.3528, Confidence: 0.3573, Entropy: 1.5732\n",
      "\n",
      "Epoch 27/100\n",
      "Train Loss: 2.3323, Accuracy: 0.3691, Confidence: 0.3393, Entropy: 1.5897\n",
      "Val Loss  : 1.5787, Accuracy: 0.3666, Confidence: 0.3260, Entropy: 1.6181\n",
      "\n",
      "Epoch 28/100\n",
      "Train Loss: 2.3397, Accuracy: 0.3686, Confidence: 0.3386, Entropy: 1.5947\n",
      "Val Loss  : 1.5801, Accuracy: 0.3839, Confidence: 0.3259, Entropy: 1.5958\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_30.pth\n",
      "Epoch 29/100\n",
      "Train Loss: 2.3288, Accuracy: 0.3830, Confidence: 0.3459, Entropy: 1.5843\n",
      "Val Loss  : 1.5617, Accuracy: 0.3701, Confidence: 0.3466, Entropy: 1.5904\n",
      "\n",
      "Epoch 30/100\n",
      "Train Loss: 2.3303, Accuracy: 0.3748, Confidence: 0.3420, Entropy: 1.5880\n",
      "Val Loss  : 1.6016, Accuracy: 0.3675, Confidence: 0.3097, Entropy: 1.6152\n",
      "\n",
      "Epoch 31/100\n",
      "Train Loss: 2.3324, Accuracy: 0.3742, Confidence: 0.3404, Entropy: 1.5916\n",
      "Val Loss  : 1.5789, Accuracy: 0.3652, Confidence: 0.3442, Entropy: 1.5941\n",
      "\n",
      "Epoch 32/100\n",
      "Train Loss: 2.3220, Accuracy: 0.3855, Confidence: 0.3493, Entropy: 1.5797\n",
      "Val Loss  : 1.5560, Accuracy: 0.3754, Confidence: 0.3499, Entropy: 1.5967\n",
      "\n",
      "Epoch 33/100\n",
      "Train Loss: 2.3195, Accuracy: 0.3908, Confidence: 0.3471, Entropy: 1.5792\n",
      "Val Loss  : 1.5348, Accuracy: 0.3949, Confidence: 0.3561, Entropy: 1.5602\n",
      "\n",
      "Epoch 34/100\n",
      "Train Loss: 2.3159, Accuracy: 0.3958, Confidence: 0.3518, Entropy: 1.5732\n",
      "Val Loss  : 1.7121, Accuracy: 0.2766, Confidence: 0.3535, Entropy: 1.5838\n",
      "\n",
      "Epoch 35/100\n",
      "Train Loss: 2.3191, Accuracy: 0.3932, Confidence: 0.3485, Entropy: 1.5775\n",
      "Val Loss  : 1.6129, Accuracy: 0.3378, Confidence: 0.3167, Entropy: 1.6192\n",
      "\n",
      "Epoch 36/100\n",
      "Train Loss: 2.3401, Accuracy: 0.3687, Confidence: 0.3346, Entropy: 1.5998\n",
      "Val Loss  : 1.5755, Accuracy: 0.3639, Confidence: 0.3533, Entropy: 1.5924\n",
      "\n",
      "Epoch 37/100\n",
      "Train Loss: 2.3212, Accuracy: 0.3931, Confidence: 0.3488, Entropy: 1.5785\n",
      "Val Loss  : 1.5575, Accuracy: 0.3901, Confidence: 0.3286, Entropy: 1.5979\n",
      "\n",
      "Epoch 38/100\n",
      "Train Loss: 2.3177, Accuracy: 0.3922, Confidence: 0.3491, Entropy: 1.5762\n",
      "Val Loss  : 1.6336, Accuracy: 0.3542, Confidence: 0.3150, Entropy: 1.6032\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_40.pth\n",
      "Epoch 39/100\n",
      "Train Loss: 2.3260, Accuracy: 0.3850, Confidence: 0.3428, Entropy: 1.5842\n",
      "Val Loss  : 1.6003, Accuracy: 0.3537, Confidence: 0.3135, Entropy: 1.6217\n",
      "\n",
      "Epoch 40/100\n",
      "Train Loss: 2.3219, Accuracy: 0.3957, Confidence: 0.3458, Entropy: 1.5807\n",
      "Val Loss  : 1.5658, Accuracy: 0.3848, Confidence: 0.3535, Entropy: 1.5821\n",
      "\n",
      "Epoch 41/100\n",
      "Train Loss: 2.3215, Accuracy: 0.3992, Confidence: 0.3476, Entropy: 1.5808\n",
      "Val Loss  : 1.7712, Accuracy: 0.2473, Confidence: 0.3721, Entropy: 1.5780\n",
      "\n",
      "Epoch 42/100\n",
      "Train Loss: 2.3175, Accuracy: 0.3954, Confidence: 0.3488, Entropy: 1.5794\n",
      "Val Loss  : 1.6645, Accuracy: 0.3027, Confidence: 0.3402, Entropy: 1.5876\n",
      "\n",
      "Epoch 43/100\n",
      "Train Loss: 2.3206, Accuracy: 0.3979, Confidence: 0.3468, Entropy: 1.5812\n",
      "Val Loss  : 1.5820, Accuracy: 0.3737, Confidence: 0.3301, Entropy: 1.6077\n",
      "\n",
      "Epoch 44/100\n",
      "Train Loss: 2.3202, Accuracy: 0.3946, Confidence: 0.3454, Entropy: 1.5800\n",
      "Val Loss  : 1.5449, Accuracy: 0.4277, Confidence: 0.3162, Entropy: 1.5979\n",
      "\n",
      "Epoch 45/100\n",
      "Train Loss: 2.3103, Accuracy: 0.4091, Confidence: 0.3545, Entropy: 1.5690\n",
      "Val Loss  : 1.5822, Accuracy: 0.3772, Confidence: 0.3575, Entropy: 1.5597\n",
      "\n",
      "Epoch 46/100\n",
      "Train Loss: 2.3121, Accuracy: 0.4059, Confidence: 0.3533, Entropy: 1.5722\n",
      "Val Loss  : 1.6858, Accuracy: 0.3152, Confidence: 0.3357, Entropy: 1.5982\n",
      "\n",
      "Epoch 47/100\n",
      "Train Loss: 2.3152, Accuracy: 0.4033, Confidence: 0.3506, Entropy: 1.5779\n",
      "Val Loss  : 1.5409, Accuracy: 0.4291, Confidence: 0.3414, Entropy: 1.5734\n",
      "\n",
      "Epoch 48/100\n",
      "Train Loss: 2.3028, Accuracy: 0.4160, Confidence: 0.3566, Entropy: 1.5639\n",
      "Val Loss  : 1.5271, Accuracy: 0.4437, Confidence: 0.3308, Entropy: 1.6023\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_50.pth\n",
      "Epoch 49/100\n",
      "Train Loss: 2.3160, Accuracy: 0.4135, Confidence: 0.3528, Entropy: 1.5685\n",
      "Val Loss  : 1.5834, Accuracy: 0.3741, Confidence: 0.3251, Entropy: 1.5952\n",
      "\n",
      "Epoch 50/100\n",
      "Train Loss: 2.3195, Accuracy: 0.4128, Confidence: 0.3514, Entropy: 1.5731\n",
      "Val Loss  : 1.6267, Accuracy: 0.3573, Confidence: 0.3312, Entropy: 1.5975\n",
      "\n",
      "Epoch 51/100\n",
      "Train Loss: 2.3156, Accuracy: 0.4123, Confidence: 0.3532, Entropy: 1.5705\n",
      "Val Loss  : 1.7807, Accuracy: 0.2837, Confidence: 0.3447, Entropy: 1.5915\n",
      "\n",
      "Epoch 52/100\n",
      "Train Loss: 2.3130, Accuracy: 0.4128, Confidence: 0.3503, Entropy: 1.5748\n",
      "Val Loss  : 1.5691, Accuracy: 0.4136, Confidence: 0.3350, Entropy: 1.5776\n",
      "\n",
      "Epoch 53/100\n",
      "Train Loss: 2.3149, Accuracy: 0.4161, Confidence: 0.3527, Entropy: 1.5737\n",
      "Val Loss  : 1.4814, Accuracy: 0.4566, Confidence: 0.3648, Entropy: 1.5575\n",
      "\n",
      "Epoch 54/100\n",
      "Train Loss: 2.3169, Accuracy: 0.4377, Confidence: 0.3609, Entropy: 1.5611\n",
      "Val Loss  : 1.4857, Accuracy: 0.4521, Confidence: 0.3547, Entropy: 1.5470\n",
      "\n",
      "Epoch 55/100\n",
      "Train Loss: 2.3070, Accuracy: 0.4263, Confidence: 0.3585, Entropy: 1.5601\n",
      "Val Loss  : 1.5108, Accuracy: 0.3998, Confidence: 0.3777, Entropy: 1.5507\n",
      "\n",
      "Epoch 56/100\n",
      "Train Loss: 2.3027, Accuracy: 0.4152, Confidence: 0.3533, Entropy: 1.5644\n",
      "Val Loss  : 1.5378, Accuracy: 0.3896, Confidence: 0.3426, Entropy: 1.6057\n",
      "\n",
      "Epoch 57/100\n",
      "Train Loss: 2.2996, Accuracy: 0.4301, Confidence: 0.3630, Entropy: 1.5574\n",
      "Val Loss  : 1.4891, Accuracy: 0.4335, Confidence: 0.3701, Entropy: 1.5468\n",
      "\n",
      "Epoch 58/100\n",
      "Train Loss: 2.2947, Accuracy: 0.4397, Confidence: 0.3640, Entropy: 1.5537\n",
      "Val Loss  : 1.4810, Accuracy: 0.4300, Confidence: 0.4029, Entropy: 1.5108\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_60.pth\n",
      "Epoch 59/100\n",
      "Train Loss: 2.3041, Accuracy: 0.4291, Confidence: 0.3606, Entropy: 1.5584\n",
      "Val Loss  : 1.6331, Accuracy: 0.3511, Confidence: 0.3318, Entropy: 1.6128\n",
      "\n",
      "Epoch 60/100\n",
      "Train Loss: 2.3103, Accuracy: 0.4079, Confidence: 0.3507, Entropy: 1.5738\n",
      "Val Loss  : 1.5319, Accuracy: 0.4322, Confidence: 0.3370, Entropy: 1.5880\n",
      "\n",
      "Epoch 61/100\n",
      "Train Loss: 2.3038, Accuracy: 0.4173, Confidence: 0.3517, Entropy: 1.5717\n",
      "Val Loss  : 1.5773, Accuracy: 0.3551, Confidence: 0.3637, Entropy: 1.5657\n",
      "\n",
      "Epoch 62/100\n",
      "Train Loss: 2.3006, Accuracy: 0.4214, Confidence: 0.3566, Entropy: 1.5660\n",
      "Val Loss  : 1.4961, Accuracy: 0.4570, Confidence: 0.3493, Entropy: 1.5716\n",
      "\n",
      "Epoch 63/100\n",
      "Train Loss: 2.2984, Accuracy: 0.4266, Confidence: 0.3564, Entropy: 1.5655\n",
      "Val Loss  : 1.5316, Accuracy: 0.3843, Confidence: 0.3707, Entropy: 1.5385\n",
      "\n",
      "Epoch 64/100\n",
      "Train Loss: 2.3079, Accuracy: 0.4205, Confidence: 0.3534, Entropy: 1.5695\n",
      "Val Loss  : 1.5359, Accuracy: 0.4238, Confidence: 0.3219, Entropy: 1.6022\n",
      "\n",
      "Epoch 65/100\n",
      "Train Loss: 2.2992, Accuracy: 0.4292, Confidence: 0.3584, Entropy: 1.5616\n",
      "Val Loss  : 1.4908, Accuracy: 0.4282, Confidence: 0.3615, Entropy: 1.5720\n",
      "\n",
      "Epoch 66/100\n",
      "Train Loss: 2.2924, Accuracy: 0.4366, Confidence: 0.3645, Entropy: 1.5537\n",
      "Val Loss  : 1.4933, Accuracy: 0.4464, Confidence: 0.3488, Entropy: 1.5665\n",
      "\n",
      "Epoch 67/100\n",
      "Train Loss: 2.2947, Accuracy: 0.4395, Confidence: 0.3633, Entropy: 1.5562\n",
      "Val Loss  : 1.6182, Accuracy: 0.3661, Confidence: 0.3429, Entropy: 1.5932\n",
      "\n",
      "Epoch 68/100\n",
      "Train Loss: 2.2893, Accuracy: 0.4439, Confidence: 0.3650, Entropy: 1.5520\n",
      "Val Loss  : 1.4705, Accuracy: 0.4645, Confidence: 0.3579, Entropy: 1.5684\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_70.pth\n",
      "Epoch 69/100\n",
      "Train Loss: 2.2933, Accuracy: 0.4388, Confidence: 0.3627, Entropy: 1.5562\n",
      "Val Loss  : 1.5688, Accuracy: 0.3839, Confidence: 0.3260, Entropy: 1.6085\n",
      "\n",
      "Epoch 70/100\n",
      "Train Loss: 2.2924, Accuracy: 0.4469, Confidence: 0.3632, Entropy: 1.5535\n",
      "Val Loss  : 1.4840, Accuracy: 0.4809, Confidence: 0.3462, Entropy: 1.5845\n",
      "\n",
      "Epoch 71/100\n",
      "Train Loss: 2.2951, Accuracy: 0.4500, Confidence: 0.3629, Entropy: 1.5535\n",
      "Val Loss  : 1.6114, Accuracy: 0.3249, Confidence: 0.3355, Entropy: 1.5926\n",
      "\n",
      "Epoch 72/100\n",
      "Train Loss: 2.2878, Accuracy: 0.4430, Confidence: 0.3679, Entropy: 1.5488\n",
      "Val Loss  : 1.7116, Accuracy: 0.3262, Confidence: 0.3459, Entropy: 1.5889\n",
      "\n",
      "Epoch 73/100\n",
      "Train Loss: 2.2901, Accuracy: 0.4443, Confidence: 0.3661, Entropy: 1.5495\n",
      "Val Loss  : 1.4910, Accuracy: 0.4353, Confidence: 0.3612, Entropy: 1.5628\n",
      "\n",
      "Epoch 74/100\n",
      "Train Loss: 2.2935, Accuracy: 0.4420, Confidence: 0.3648, Entropy: 1.5516\n",
      "Val Loss  : 1.7698, Accuracy: 0.3249, Confidence: 0.3779, Entropy: 1.5422\n",
      "\n",
      "Epoch 75/100\n",
      "Train Loss: 2.2866, Accuracy: 0.4721, Confidence: 0.3729, Entropy: 1.5380\n",
      "Val Loss  : 1.4955, Accuracy: 0.4180, Confidence: 0.3851, Entropy: 1.5340\n",
      "\n",
      "Epoch 76/100\n",
      "Train Loss: 2.2780, Accuracy: 0.4811, Confidence: 0.3795, Entropy: 1.5310\n",
      "Val Loss  : 1.4859, Accuracy: 0.4645, Confidence: 0.3771, Entropy: 1.5195\n",
      "\n",
      "Epoch 77/100\n",
      "Train Loss: 2.2770, Accuracy: 0.5022, Confidence: 0.3853, Entropy: 1.5203\n",
      "Val Loss  : 1.4686, Accuracy: 0.4765, Confidence: 0.3846, Entropy: 1.4974\n",
      "\n",
      "Epoch 78/100\n",
      "Train Loss: 2.2843, Accuracy: 0.4897, Confidence: 0.3858, Entropy: 1.5211\n",
      "Val Loss  : 1.4191, Accuracy: 0.5062, Confidence: 0.3766, Entropy: 1.5274\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_80.pth\n",
      "Epoch 79/100\n",
      "Train Loss: 2.2884, Accuracy: 0.4750, Confidence: 0.3775, Entropy: 1.5311\n",
      "Val Loss  : 1.4790, Accuracy: 0.4548, Confidence: 0.3832, Entropy: 1.5014\n",
      "\n",
      "Epoch 80/100\n",
      "Train Loss: 2.2924, Accuracy: 0.4788, Confidence: 0.3768, Entropy: 1.5314\n",
      "Val Loss  : 1.4632, Accuracy: 0.4530, Confidence: 0.4024, Entropy: 1.4666\n",
      "\n",
      "Epoch 81/100\n",
      "Train Loss: 2.2850, Accuracy: 0.4847, Confidence: 0.3817, Entropy: 1.5265\n",
      "Val Loss  : 1.7346, Accuracy: 0.3830, Confidence: 0.4063, Entropy: 1.5035\n",
      "\n",
      "Epoch 82/100\n",
      "Train Loss: 2.2815, Accuracy: 0.4817, Confidence: 0.3819, Entropy: 1.5276\n",
      "Val Loss  : 1.4526, Accuracy: 0.4725, Confidence: 0.3626, Entropy: 1.5471\n",
      "\n",
      "Epoch 83/100\n",
      "Train Loss: 2.2865, Accuracy: 0.4674, Confidence: 0.3737, Entropy: 1.5378\n",
      "Val Loss  : 1.4675, Accuracy: 0.4614, Confidence: 0.3738, Entropy: 1.5450\n",
      "\n",
      "Epoch 84/100\n",
      "Train Loss: 2.2796, Accuracy: 0.4617, Confidence: 0.3739, Entropy: 1.5409\n",
      "Val Loss  : 1.5841, Accuracy: 0.3794, Confidence: 0.4043, Entropy: 1.5078\n",
      "\n",
      "Epoch 85/100\n",
      "Train Loss: 2.2865, Accuracy: 0.4725, Confidence: 0.3756, Entropy: 1.5397\n",
      "Val Loss  : 1.4715, Accuracy: 0.4623, Confidence: 0.3787, Entropy: 1.5299\n",
      "\n",
      "Epoch 86/100\n",
      "Train Loss: 2.2781, Accuracy: 0.4776, Confidence: 0.3781, Entropy: 1.5302\n",
      "Val Loss  : 1.4605, Accuracy: 0.4792, Confidence: 0.3560, Entropy: 1.5622\n",
      "\n",
      "Epoch 87/100\n",
      "Train Loss: 2.2777, Accuracy: 0.4951, Confidence: 0.3870, Entropy: 1.5164\n",
      "Val Loss  : 1.4642, Accuracy: 0.4415, Confidence: 0.3960, Entropy: 1.5186\n",
      "\n",
      "Epoch 88/100\n",
      "Train Loss: 2.2878, Accuracy: 0.4920, Confidence: 0.3805, Entropy: 1.5270\n",
      "Val Loss  : 1.4367, Accuracy: 0.4911, Confidence: 0.3833, Entropy: 1.5329\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_90.pth\n",
      "Epoch 89/100\n",
      "Train Loss: 2.2829, Accuracy: 0.4930, Confidence: 0.3810, Entropy: 1.5249\n",
      "Val Loss  : 1.6208, Accuracy: 0.3586, Confidence: 0.3939, Entropy: 1.5057\n",
      "\n",
      "Epoch 90/100\n",
      "Train Loss: 2.2765, Accuracy: 0.5153, Confidence: 0.3884, Entropy: 1.5180\n",
      "Val Loss  : 1.4110, Accuracy: 0.5146, Confidence: 0.3942, Entropy: 1.5142\n",
      "\n",
      "Epoch 91/100\n",
      "Train Loss: 2.2770, Accuracy: 0.5115, Confidence: 0.3872, Entropy: 1.5162\n",
      "Val Loss  : 1.4300, Accuracy: 0.4969, Confidence: 0.3640, Entropy: 1.5586\n",
      "\n",
      "Epoch 92/100\n",
      "Train Loss: 2.2702, Accuracy: 0.5054, Confidence: 0.3856, Entropy: 1.5174\n",
      "Val Loss  : 1.4066, Accuracy: 0.5262, Confidence: 0.3830, Entropy: 1.5225\n",
      "\n",
      "Epoch 93/100\n",
      "Train Loss: 2.2723, Accuracy: 0.5001, Confidence: 0.3829, Entropy: 1.5206\n",
      "Val Loss  : 1.4136, Accuracy: 0.5177, Confidence: 0.3816, Entropy: 1.5170\n",
      "\n",
      "Epoch 94/100\n",
      "Train Loss: 2.2681, Accuracy: 0.5130, Confidence: 0.3930, Entropy: 1.5070\n",
      "Val Loss  : 1.4124, Accuracy: 0.5080, Confidence: 0.3989, Entropy: 1.4911\n",
      "\n",
      "Epoch 95/100\n",
      "Train Loss: 2.2628, Accuracy: 0.5234, Confidence: 0.3977, Entropy: 1.4989\n",
      "Val Loss  : 1.4033, Accuracy: 0.4902, Confidence: 0.4019, Entropy: 1.4885\n",
      "\n",
      "Epoch 96/100\n",
      "Train Loss: 2.2675, Accuracy: 0.5043, Confidence: 0.3909, Entropy: 1.5133\n",
      "Val Loss  : 1.4532, Accuracy: 0.4495, Confidence: 0.3904, Entropy: 1.5075\n",
      "\n",
      "Epoch 97/100\n",
      "Train Loss: 2.2650, Accuracy: 0.5001, Confidence: 0.3882, Entropy: 1.5188\n",
      "Val Loss  : 1.4205, Accuracy: 0.4734, Confidence: 0.4102, Entropy: 1.4980\n",
      "\n",
      "Epoch 98/100\n",
      "Train Loss: 2.2667, Accuracy: 0.4956, Confidence: 0.3843, Entropy: 1.5252\n",
      "Val Loss  : 1.4784, Accuracy: 0.4592, Confidence: 0.3647, Entropy: 1.5481\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_100.pth\n",
      "Epoch 99/100\n",
      "Train Loss: 2.2670, Accuracy: 0.4941, Confidence: 0.3832, Entropy: 1.5262\n",
      "Val Loss  : 1.4855, Accuracy: 0.4619, Confidence: 0.3758, Entropy: 1.5294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-26 20:00:14,483] Trial 0 finished with value: 0.41017251868619986 and parameters: {}. Best is trial 0 with value: 0.41017251868619986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All trials saved to: /home/haislich/Documents/noisy_labels/hackaton/logs/A/optuna_summary_A.csv\n",
      "\n",
      "Best result for dataset A:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.410173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   confidence\n",
       "0    0.410173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-26 20:01:01,053] A new study created in memory with name: B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna optimization for dataset B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a3aab24a494fbf820ffa52005889b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "Train Loss: 2.6888, Accuracy: 0.2846, Confidence: 0.2281, Entropy: 1.7605\n",
      "Val Loss  : 1.7616, Accuracy: 0.2750, Confidence: 0.2341, Entropy: 1.7606\n",
      "\n",
      "Epoch 1/100\n",
      "Train Loss: 2.6605, Accuracy: 0.3342, Confidence: 0.2467, Entropy: 1.7566\n",
      "Val Loss  : 1.7776, Accuracy: 0.2286, Confidence: 0.2369, Entropy: 1.7556\n",
      "\n",
      "Epoch 2/100\n",
      "Train Loss: 2.6556, Accuracy: 0.3301, Confidence: 0.2570, Entropy: 1.7502\n",
      "Val Loss  : 1.7466, Accuracy: 0.2884, Confidence: 0.2241, Entropy: 1.7629\n",
      "\n",
      "Epoch 3/100\n",
      "Train Loss: 2.6467, Accuracy: 0.3469, Confidence: 0.2647, Entropy: 1.7439\n",
      "Val Loss  : 1.7338, Accuracy: 0.2902, Confidence: 0.2463, Entropy: 1.7554\n",
      "\n",
      "Epoch 4/100\n",
      "Train Loss: 2.6384, Accuracy: 0.3549, Confidence: 0.2759, Entropy: 1.7338\n",
      "Val Loss  : 1.8265, Accuracy: 0.2116, Confidence: 0.2989, Entropy: 1.6912\n",
      "\n",
      "Epoch 5/100\n",
      "Train Loss: 2.6235, Accuracy: 0.3650, Confidence: 0.2854, Entropy: 1.7239\n",
      "Val Loss  : 1.7258, Accuracy: 0.2875, Confidence: 0.2513, Entropy: 1.7474\n",
      "\n",
      "Epoch 6/100\n",
      "Train Loss: 2.6002, Accuracy: 0.3632, Confidence: 0.2979, Entropy: 1.7094\n",
      "Val Loss  : 1.7334, Accuracy: 0.3018, Confidence: 0.2815, Entropy: 1.7204\n",
      "\n",
      "Epoch 7/100\n",
      "Train Loss: 2.5457, Accuracy: 0.3480, Confidence: 0.3191, Entropy: 1.6826\n",
      "Val Loss  : 1.8905, Accuracy: 0.2188, Confidence: 0.3893, Entropy: 1.5664\n",
      "\n",
      "Epoch 8/100\n",
      "Train Loss: 2.4786, Accuracy: 0.3442, Confidence: 0.3301, Entropy: 1.6612\n",
      "Val Loss  : 1.7093, Accuracy: 0.3018, Confidence: 0.3162, Entropy: 1.6833\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/B/model_B_epoch_10.pth\n",
      "Epoch 9/100\n",
      "Train Loss: 2.4441, Accuracy: 0.3402, Confidence: 0.3268, Entropy: 1.6517\n",
      "Val Loss  : 1.7393, Accuracy: 0.3036, Confidence: 0.3057, Entropy: 1.6780\n",
      "\n",
      "Epoch 10/100\n",
      "Train Loss: 2.4403, Accuracy: 0.3429, Confidence: 0.3285, Entropy: 1.6483\n",
      "Val Loss  : 1.6714, Accuracy: 0.3375, Confidence: 0.3324, Entropy: 1.6396\n",
      "\n",
      "Epoch 11/100\n",
      "Train Loss: 2.4311, Accuracy: 0.3560, Confidence: 0.3354, Entropy: 1.6392\n",
      "Val Loss  : 1.8127, Accuracy: 0.2830, Confidence: 0.3264, Entropy: 1.6198\n",
      "\n",
      "Epoch 12/100\n",
      "Train Loss: 2.4158, Accuracy: 0.3578, Confidence: 0.3352, Entropy: 1.6364\n",
      "Val Loss  : 1.6614, Accuracy: 0.3768, Confidence: 0.3545, Entropy: 1.6174\n",
      "\n",
      "Epoch 13/100\n",
      "Train Loss: 2.4107, Accuracy: 0.3656, Confidence: 0.3380, Entropy: 1.6307\n",
      "Val Loss  : 1.7875, Accuracy: 0.2938, Confidence: 0.3382, Entropy: 1.6124\n",
      "\n",
      "Epoch 14/100\n",
      "Train Loss: 2.4110, Accuracy: 0.3471, Confidence: 0.3323, Entropy: 1.6348\n",
      "Val Loss  : 1.6732, Accuracy: 0.3688, Confidence: 0.3311, Entropy: 1.6355\n",
      "\n",
      "Epoch 15/100\n",
      "Train Loss: 2.4178, Accuracy: 0.3629, Confidence: 0.3320, Entropy: 1.6399\n",
      "Val Loss  : 1.7102, Accuracy: 0.3348, Confidence: 0.3359, Entropy: 1.6278\n",
      "\n",
      "Epoch 16/100\n",
      "Train Loss: 2.4041, Accuracy: 0.3580, Confidence: 0.3293, Entropy: 1.6359\n",
      "Val Loss  : 1.7865, Accuracy: 0.2929, Confidence: 0.3120, Entropy: 1.6702\n",
      "\n",
      "Epoch 17/100\n",
      "Train Loss: 2.4149, Accuracy: 0.3471, Confidence: 0.3245, Entropy: 1.6447\n",
      "Val Loss  : 1.7373, Accuracy: 0.3125, Confidence: 0.3141, Entropy: 1.6416\n",
      "\n",
      "Epoch 18/100\n",
      "Train Loss: 2.3926, Accuracy: 0.3440, Confidence: 0.3237, Entropy: 1.6369\n",
      "Val Loss  : 1.7444, Accuracy: 0.3161, Confidence: 0.2976, Entropy: 1.6742\n",
      "\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/B/model_B_epoch_20.pth\n",
      "Epoch 19/100\n",
      "Train Loss: 2.3781, Accuracy: 0.3484, Confidence: 0.3200, Entropy: 1.6335\n",
      "Val Loss  : 1.7376, Accuracy: 0.3205, Confidence: 0.3142, Entropy: 1.6578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_study(\"A\", False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"B\", False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"C\", False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"D\", False, 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
