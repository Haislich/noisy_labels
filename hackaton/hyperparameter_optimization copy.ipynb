{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from functools import partial\n",
    "import optuna\n",
    "import gc\n",
    "from typing import Literal\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, Dataset\n",
    "\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\"id\": test_graph_ids, \"pred\": predictions})\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy per Epoch\")\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedSubset(Dataset):\n",
    "    def __init__(self, subset: Subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.subset[i]\n",
    "        data.idx = torch.tensor(i, dtype=torch.long)  # type: ignore\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCODLoss(nn.Module):\n",
    "    past_embeddings: torch.Tensor\n",
    "    centroids: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        embedding_dimensions: int = 300,\n",
    "        total_epochs: int = 150,\n",
    "        lambda_consistency: float = 1.0,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        dataset : iterable whose elements expose an integer label in `elem.y`\n",
    "        embedding_dimensions : size of the feature vectors\n",
    "        total_epochs : number of training epochs (used for centroid update schedule)\n",
    "        lambda_consistency : weight for the MSE consistency term\n",
    "        device : cuda / cpu device.  If None, picks CUDA if available.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device or torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "\n",
    "        labels = [int(elem.y) for elem in dataset]\n",
    "        self.num_elements = len(labels)\n",
    "        self.num_classes = max(labels) + 1  # robust to gaps (e.g. labels {1,3})\n",
    "        # self.register_buffer(\"bins\", torch.empty(self.num_classes, 0, dtype=torch.long))\n",
    "\n",
    "        # Convert bins to a list-of-lists for easy appends, then to tensors\n",
    "        tmp_bins: list[list[int]] = [[] for _ in range(self.num_classes)]\n",
    "        for idx, lab in enumerate(labels):\n",
    "            tmp_bins[lab].append(idx)\n",
    "        self.bins = [\n",
    "            torch.as_tensor(b, dtype=torch.long, device=self.device) for b in tmp_bins\n",
    "        ]\n",
    "\n",
    "        # Confidence parameter per sample (trainable!)\n",
    "        self.u = nn.Parameter(torch.empty(self.num_elements, 1, device=self.device))\n",
    "        nn.init.normal_(self.u, mean=1e-8, std=1e-9)\n",
    "\n",
    "        # Running memory of embeddings\n",
    "        self.register_buffer(\n",
    "            \"past_embeddings\",\n",
    "            torch.rand(\n",
    "                self.num_elements, self.embedding_dimensions, device=self.device\n",
    "            ),\n",
    "        )\n",
    "        # Class centroids\n",
    "        self.register_buffer(\n",
    "            \"centroids\",\n",
    "            torch.rand(self.num_classes, self.embedding_dimensions, device=self.device),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        logits: torch.Tensor,  # (B, C)\n",
    "        indexes: torch.Tensor,  # (B,) – dataset indices of the current batch\n",
    "        embeddings: torch.Tensor,  # (B, D)\n",
    "        targets: torch.Tensor,  # (B,)\n",
    "        epoch: int,\n",
    "    ) -> torch.Tensor:\n",
    "        eps = 1e-6\n",
    "\n",
    "        # Keep an L2-normalised copy of current embeddings\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        self.past_embeddings[indexes] = embeddings.detach()\n",
    "\n",
    "        # ---------------- Centroid update ------------------------------------\n",
    "        if epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                for c, idxs in enumerate(self.bins):\n",
    "                    if idxs.numel():\n",
    "                        self.centroids[c] = self.past_embeddings[idxs].mean(0)\n",
    "        else:\n",
    "            # Shrink the subset of samples that contribute to the centroid\n",
    "            percent = int(max(1, min(100, 50 + 50 * (1 - epoch / self.total_epochs))))\n",
    "            for c, idxs in enumerate(self.bins):\n",
    "                if idxs.numel() == 0:\n",
    "                    continue\n",
    "                # bottom-k u’s  (small u  ⇒ low confidence ⇒ smaller weight)\n",
    "                k = max(1, idxs.numel() * percent // 100)\n",
    "                u_batch = self.u[idxs].squeeze(1)\n",
    "                keep = torch.topk(u_batch, k, largest=False).indices  # (k,)\n",
    "                selected = idxs[keep]  # (k,)\n",
    "                self.centroids[c] = self.past_embeddings[selected].mean(0)\n",
    "\n",
    "        centroids = F.normalize(self.centroids, dim=1)  # (C, D)\n",
    "\n",
    "        # ---------------- Probability shaping --------------------------------\n",
    "        soft_labels = F.softmax(embeddings @ centroids.T, dim=1)  # (B, C)\n",
    "        probs = F.softmax(logits, dim=1)  # (B, C)\n",
    "        u_vals = torch.sigmoid(self.u[indexes]).squeeze(1)  # (B,)\n",
    "\n",
    "        adjusted = (probs + u_vals[:, None] * soft_labels).clamp(min=eps)\n",
    "        adjusted = adjusted / adjusted.sum(1, keepdim=True)\n",
    "\n",
    "        # ---------------- Loss terms -----------------------------------------\n",
    "        hard_ce = (\n",
    "            (1.0 - u_vals) * F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        ).mean()\n",
    "        soft_ce = -(soft_labels * torch.log(adjusted)).sum(1).mean()\n",
    "        consistency = F.mse_loss(adjusted, soft_labels)\n",
    "\n",
    "        return hard_ce + soft_ce + self.lambda_consistency * consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (\n",
    "            1\n",
    "            - torch.nn.functional.one_hot(targets, num_classes=logits.size(1))\n",
    "            .float()\n",
    "            .sum(dim=1)\n",
    "        )\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCELoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 6, alpha: float = 0.1, beta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # CCE\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "\n",
    "        # RCE\n",
    "        pred = F.softmax(logits, dim=1).clamp(min=1e-6, max=1 - 1e-6)\n",
    "        one_hot = F.one_hot(targets, self.num_classes).float()\n",
    "        rce = -(1 - one_hot) * torch.log(1 - pred)\n",
    "        rce = rce.sum(dim=1)\n",
    "        return (self.alpha * ce + self.beta * rce).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    optimizer_theta: torch.optim.Optimizer,\n",
    "    optimizer_u: torch.optim.Optimizer | None,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    checkpoint_path: str,\n",
    "    current_epoch: int,\n",
    "    save_checkpoints: bool = True,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        node_emb = model.gnn_node(batch)\n",
    "        graph_emb = model.pool(node_emb, batch.batch)\n",
    "        logits = model.graph_pred_linear(graph_emb)\n",
    "\n",
    "        if isinstance(criterion, NCODLoss):\n",
    "            loss = criterion(\n",
    "                logits=logits,\n",
    "                indexes=batch.idx.to(device),\n",
    "                embeddings=graph_emb,\n",
    "                targets=batch.y.to(device),\n",
    "                epoch=current_epoch,\n",
    "            )\n",
    "        else:\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "        optimizer_theta.zero_grad(set_to_none=True)\n",
    "        if optimizer_u is not None:\n",
    "            optimizer_u.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_theta.step()\n",
    "        if optimizer_u is not None:\n",
    "            optimizer_u.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            batch_size = batch.y.size(0)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), 1)).sum().item()\n",
    "            )\n",
    "\n",
    "    if save_checkpoints:\n",
    "        ckpt = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt)\n",
    "        print(f\"[checkpoint] saved: {ckpt}\")\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss, avg_confidence, accuracy, avg_entropy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            node_emb = model.gnn_node(batch)\n",
    "            graph_emb = model.pool(node_emb, batch.batch)\n",
    "            logits = model.graph_pred_linear(graph_emb)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "            batch_size = batch.y.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), dim=1)).sum().item()\n",
    "            )\n",
    "\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_checkpoints: int,\n",
    "    checkpoints_dir: str,\n",
    "    run_name: str,\n",
    "    best_model_path: str,\n",
    "    logs_dir: str,\n",
    "    resume_training: bool,\n",
    "    *,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"confidence\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):  # -> float | Any:\n",
    "    try:\n",
    "        logging.info(\"#\" * 80)\n",
    "        # Hyperparameter search space\n",
    "        logging.info(\"Start case study with parameters:\")\n",
    "        gnn_type = trial.suggest_categorical(\n",
    "            \"gnn_type\", [\"gin\", \"gin-virtual\", \"gcn\", \"gcn-virtual\"]\n",
    "        )\n",
    "        loss_type = trial.suggest_categorical(\n",
    "            \"loss_type\", [\"ce\"] + [\"ncod\"] * 5 + [\"noisy_ce\"] * 5 + [\"sce\"]\n",
    "        )\n",
    "        graph_pooling = trial.suggest_categorical(\n",
    "            \"graph_pooling\",\n",
    "            [\"sum\"] * 2 + [\"mean\"] * 4 + [\"max\"] * 2 + [\"attention\"] * 2 + [\"set2set\"],\n",
    "        )\n",
    "        drop_ratio = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "        num_layers = 5  # trial.suggest_int(\"num_layers\", 6, 12)\n",
    "        embedding_dim = (\n",
    "            300  # trial.suggest_categorical(\"embedding_dim\", [64, 128, 300])\n",
    "        )\n",
    "        num_epochs = 50  # trial.suggest_int(\"num_epochs\", 10, 11, step=1)\n",
    "\n",
    "        print(\n",
    "            f\"{gnn_type=}\\n{loss_type=}\\n{graph_pooling=}\\n{drop_ratio=}\\n{num_layers=}\\n{embedding_dim=}\\n{num_epochs=}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"{gnn_type=} {loss_type=} {graph_pooling=} {drop_ratio=} {num_layers=} {embedding_dim=} {num_epochs=}\"\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model = GNN(\n",
    "            gnn_type=\"gin\" if \"gin\" in gnn_type else \"gcn\",\n",
    "            num_class=6,\n",
    "            num_layer=num_layers,\n",
    "            emb_dim=embedding_dim,\n",
    "            drop_ratio=drop_ratio,\n",
    "            virtual_node=\"virtual\" in gnn_type,\n",
    "            graph_pooling=graph_pooling,\n",
    "        ).to(device)\n",
    "        if resume_training:\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "        optimizer_theta = torch.optim.Adam(\n",
    "            model.parameters(), lr=3e-4, weight_decay=1e-5\n",
    "        )\n",
    "        optimizer_u = None\n",
    "        if loss_type == \"ce\":\n",
    "            train_criterion = nn.CrossEntropyLoss()\n",
    "        elif loss_type == \"ncod\":\n",
    "            train_criterion = NCODLoss(\n",
    "                train_loader.dataset,\n",
    "                embedding_dimensions=embedding_dim,\n",
    "                total_epochs=num_epochs,\n",
    "                device=device,\n",
    "            )\n",
    "            optimizer_u = torch.optim.SGD(train_criterion.parameters(), lr=1e-3)\n",
    "        elif loss_type == \"sce\":\n",
    "            train_criterion = SCELoss()\n",
    "        else:\n",
    "            train_criterion = NoisyCrossEntropyLoss(0.2)\n",
    "\n",
    "        val_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Checkpoint logic\n",
    "        checkpoint_epochs = [\n",
    "            int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)\n",
    "        ]\n",
    "\n",
    "        best_val_score = -float(\"inf\")\n",
    "        train_losses, train_confs, train_accs, train_entropies, train_scores = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        val_losses, val_confs, val_accs, val_entropies, val_scores = [], [], [], [], []\n",
    "\n",
    "        progress_bar = tqdm(range(num_epochs), desc=\"Training...\", leave=False)\n",
    "        for epoch in progress_bar:\n",
    "            train_loss, train_conf, train_acc, train_entropy = train(\n",
    "                train_loader,\n",
    "                model,\n",
    "                optimizer_theta,\n",
    "                optimizer_u,\n",
    "                train_criterion,\n",
    "                device,\n",
    "                save_checkpoints=(epoch + 1 in checkpoint_epochs),\n",
    "                checkpoint_path=os.path.join(checkpoints_dir, f\"model_{run_name}\"),\n",
    "                current_epoch=epoch,\n",
    "            )\n",
    "\n",
    "            val_loss, val_conf, val_acc, val_entropy = evaluate(\n",
    "                val_loader,\n",
    "                model,\n",
    "                val_criterion,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            progress_bar.set_postfix_str(\n",
    "                f\"\\nTrain Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}\\n\"\n",
    "                f\"Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\\n\"\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"Epoch {epoch}/{num_epochs}| \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}| \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\"\n",
    "            )\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            train_confs.append(train_conf)\n",
    "            train_entropies.append(train_entropy)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            val_confs.append(val_conf)\n",
    "            val_entropies.append(val_entropy)\n",
    "\n",
    "            if score_type == \"loss\":\n",
    "                train_score = train_loss\n",
    "                val_score = -val_loss\n",
    "            elif score_type == \"entropy\":\n",
    "                train_score = train_entropy\n",
    "                val_score = -val_entropy\n",
    "            elif score_type == \"confidence\":\n",
    "                train_score = train_conf\n",
    "                val_score = val_conf\n",
    "            else:\n",
    "                train_score = train_acc\n",
    "                val_score = val_acc\n",
    "\n",
    "            train_scores.append(train_score)\n",
    "            val_scores.append(val_score)\n",
    "            trial.report(val_score, step=epoch)\n",
    "            if trial.should_prune():\n",
    "                logging.warning(f\"Trial was pruned at epoch {epoch}\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                logging.info(f\"[{run_name}] Best model updated at {best_model_path}\")\n",
    "\n",
    "        # Plot training curves\n",
    "        plot_training_progress(\n",
    "            train_losses, train_scores, os.path.join(logs_dir, \"train_plots\")\n",
    "        )\n",
    "        plot_training_progress(\n",
    "            val_losses, val_scores, os.path.join(logs_dir, \"val_plots\")\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Case study end, {best_val_score}\")\n",
    "        logging.info(\"#\" * 80)\n",
    "        logging.info(\"\\n\")\n",
    "\n",
    "        return best_val_score\n",
    "    except Exception as e:\n",
    "        print(\"Unhandled exception \", e)\n",
    "        return -float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "import numpy as np\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "\n",
    "def case_study(\n",
    "    dataset_name: Literal[\"A\", \"B\", \"C\", \"D\"],\n",
    "    resume_training: bool,\n",
    "    n_trials: int = 30,\n",
    "    num_checkpoints: int = 10,\n",
    "    default_batch_size: int = 64,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"confidence\",\n",
    "    full_dataset=None,\n",
    "):\n",
    "    script_root = os.getcwd()\n",
    "    train_path = f\"./datasets/{dataset_name}/train.json.gz\"\n",
    "    run_name = dataset_name\n",
    "\n",
    "    logs_dir = os.path.join(script_root, \"logs\", run_name)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_dir, \"training.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "\n",
    "    checkpoints_dir = os.path.join(script_root, \"checkpoints\", run_name)\n",
    "    best_model_path = os.path.join(checkpoints_dir, f\"model_{run_name}_best.pth\")\n",
    "    summary_csv_path = os.path.join(logs_dir, f\"optuna_summary_{dataset_name}.csv\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    if full_dataset is None:\n",
    "        full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    train_dataset = IndexedSubset(train_dataset)\n",
    "    val_dataset = IndexedSubset(val_dataset)\n",
    "\n",
    "    # ---- WeightedRandomSampler for class balancing ----\n",
    "    labels = [int(full_dataset[i].y) for i in train_dataset.subset.indices]\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=default_batch_size,\n",
    "        sampler=sampler,  # replaces shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Starting Optuna optimization for dataset {dataset_name}\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=run_name,\n",
    "        direction=\"maximize\",\n",
    "        pruner = MedianPruner(n_warmup_steps=5)\n",
    "    )\n",
    "\n",
    "    obj = partial(\n",
    "        objective,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_checkpoints=num_checkpoints,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        run_name=run_name,\n",
    "        best_model_path=best_model_path,\n",
    "        logs_dir=logs_dir,\n",
    "        resume_training=resume_training,\n",
    "        score_type=score_type,\n",
    "    )\n",
    "    study.optimize(obj, n_trials=n_trials,show_progress_bar=True)\n",
    "\n",
    "    all_trials = []\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            row = {score_type: trial.value}\n",
    "            row.update(trial.params)\n",
    "            all_trials.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_trials)\n",
    "    results_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nAll trials saved to: {summary_csv_path}\")\n",
    "    print(f\"\\nBest result for dataset {dataset_name}:\")\n",
    "    display(results_df.sort_values(score_type, ascending=False))\n",
    "    print(f\"\\nBest Params for {dataset_name}:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_a = GraphDataset(\"./datasets/A/train.json.gz\", transform=add_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-27 15:22:15,166] A new study created in memory with name: A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna optimization for dataset A\n",
      "gnn_type='gin-virtual'\n",
      "loss_type='ncod'\n",
      "graph_pooling='attention'\n",
      "drop_ratio=0.20073187301579293\n",
      "num_layers=5\n",
      "embedding_dim=300\n",
      "num_epochs=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d4323845b641fe8fc215c314f11ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_5.pth\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_10.pth\n",
      "[checkpoint] saved: /home/haislich/Documents/noisy_labels/hackaton/checkpoints/A/model_A_epoch_15.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-27 15:31:35,330] Trial 0 failed with parameters: {'gnn_type': 'gin-virtual', 'loss_type': 'ncod', 'graph_pooling': 'attention', 'dropout': 0.20073187301579293} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_93765/391235273.py\", line 93, in objective\n",
      "    train_loss, train_conf, train_acc, train_entropy = train(\n",
      "                                                       ^^^^^^\n",
      "  File \"/tmp/ipykernel_93765/1993452241.py\", line 17, in train\n",
      "    for batch in data_loader:\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 733, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 789, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/tmp/ipykernel_93765/3813083377.py\", line 9, in __getitem__\n",
      "    data = self.subset[i]\n",
      "           ~~~~~~~~~~~^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 408, in __getitem__\n",
      "    return self.dataset[self.indices[idx]]\n",
      "           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch_geometric/data/dataset.py\", line 291, in __getitem__\n",
      "    data = self.get(self.indices()[idx])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/hackaton/src/loadData.py\", line 59, in get\n",
      "    return dictToGraphObject(self.graphs_dicts[idx])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/hackaton/src/loadData.py\", line 70, in dictToGraphObject\n",
      "    edge_index = torch.tensor(graph_dict[\"edge_index\"], dtype=torch.long)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-27 15:31:35,331] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcase_study\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mcase_study\u001b[39m\u001b[34m(dataset_name, resume_training, n_trials, num_checkpoints, default_batch_size, score_type, full_dataset)\u001b[39m\n\u001b[32m     64\u001b[39m study = optuna.create_study(\n\u001b[32m     65\u001b[39m     study_name=run_name,\n\u001b[32m     66\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m )\n\u001b[32m     69\u001b[39m obj = partial(\n\u001b[32m     70\u001b[39m     objective,\n\u001b[32m     71\u001b[39m     train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     score_type=score_type,\n\u001b[32m     80\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m all_trials = []\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m study.trials:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial, train_loader, val_loader, num_checkpoints, checkpoints_dir, run_name, best_model_path, logs_dir, resume_training, score_type, device)\u001b[39m\n\u001b[32m     91\u001b[39m progress_bar = tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining...\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     train_loss, train_conf, train_acc, train_entropy = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_u\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_epochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     val_loss, val_conf, val_acc, val_entropy = evaluate(\n\u001b[32m    106\u001b[39m         val_loader,\n\u001b[32m    107\u001b[39m         model,\n\u001b[32m    108\u001b[39m         val_criterion,\n\u001b[32m    109\u001b[39m         device,\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    111\u001b[39m     progress_bar.set_postfix_str(\n\u001b[32m    112\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_conf\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_entropy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal Loss  : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_conf\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_entropy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(data_loader, model, optimizer_theta, optimizer_u, criterion, device, checkpoint_path, current_epoch, save_checkpoints)\u001b[39m\n\u001b[32m     14\u001b[39m total_loss = total_conf = total_entropy = \u001b[32m0.0\u001b[39m\n\u001b[32m     15\u001b[39m correct = num_samples = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_emb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgnn_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mIndexedSubset.__getitem__\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m     data.idx = torch.tensor(i, dtype=torch.long)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:408\u001b[39m, in \u001b[36mSubset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset[[\u001b[38;5;28mself\u001b[39m.indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch_geometric/data/dataset.py:291\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03mpresent).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m \u001b[33;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np.integer))\n\u001b[32m    288\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx.dim() == \u001b[32m0\u001b[39m)\n\u001b[32m    289\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m np.isscalar(idx))):\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     data = data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(data)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/loadData.py:59\u001b[39m, in \u001b[36mGraphDataset.get\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdictToGraphObject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraphs_dicts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/loadData.py:70\u001b[39m, in \u001b[36mdictToGraphObject\u001b[39m\u001b[34m(graph_dict)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdictToGraphObject\u001b[39m(graph_dict):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     edge_index = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43medge_index\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     edge_attr = (\n\u001b[32m     72\u001b[39m         torch.tensor(graph_dict[\u001b[33m\"\u001b[39m\u001b[33medge_attr\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float)\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m graph_dict[\u001b[33m\"\u001b[39m\u001b[33medge_attr\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     76\u001b[39m     num_nodes = graph_dict[\u001b[33m\"\u001b[39m\u001b[33mnum_nodes\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "case_study(\"A\", False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"B\", False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"C\", False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"D\", False, 10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
