{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from functools import partial\n",
    "import optuna\n",
    "import gc\n",
    "from typing import Literal, Optional\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, Dataset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import numpy as np\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\"id\": test_graph_ids, \"pred\": predictions})\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy per Epoch\")\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class IndexedSubset(Dataset):\n",
    "    def __init__(self, subset: Subset[Data]):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.subset[i]\n",
    "        data.idx = torch.tensor(i, dtype=torch.long)  # type: ignore\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCODLoss(nn.Module):\n",
    "    past_embeddings: torch.Tensor\n",
    "    centroids: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        embedding_dimensions: int = 300,\n",
    "        total_epochs: int = 150,\n",
    "        lambda_consistency: float = 1.0,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        dataset : iterable whose elements expose an integer label in `elem.y`\n",
    "        embedding_dimensions : size of the feature vectors\n",
    "        total_epochs : number of training epochs (used for centroid update schedule)\n",
    "        lambda_consistency : weight for the MSE consistency term\n",
    "        device : cuda / cpu device.  If None, picks CUDA if available.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device or torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "\n",
    "        labels = [int(elem.y) for elem in dataset]\n",
    "        self.num_elements = len(labels)\n",
    "        self.num_classes = max(labels) + 1  # robust to gaps (e.g. labels {1,3})\n",
    "        # self.register_buffer(\"bins\", torch.empty(self.num_classes, 0, dtype=torch.long))\n",
    "\n",
    "        # Convert bins to a list-of-lists for easy appends, then to tensors\n",
    "        tmp_bins: list[list[int]] = [[] for _ in range(self.num_classes)]\n",
    "        for idx, lab in enumerate(labels):\n",
    "            tmp_bins[lab].append(idx)\n",
    "        self.bins = [\n",
    "            torch.as_tensor(b, dtype=torch.long, device=self.device) for b in tmp_bins\n",
    "        ]\n",
    "\n",
    "        # Confidence parameter per sample (trainable!)\n",
    "        self.u = nn.Parameter(torch.empty(self.num_elements, 1, device=self.device))\n",
    "        nn.init.normal_(self.u, mean=1e-8, std=1e-9)\n",
    "\n",
    "        # Running memory of embeddings\n",
    "        self.register_buffer(\n",
    "            \"past_embeddings\",\n",
    "            torch.rand(\n",
    "                self.num_elements, self.embedding_dimensions, device=self.device\n",
    "            ),\n",
    "        )\n",
    "        # Class centroids\n",
    "        self.register_buffer(\n",
    "            \"centroids\",\n",
    "            torch.rand(self.num_classes, self.embedding_dimensions, device=self.device),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        logits: torch.Tensor,  # (B, C)\n",
    "        indexes: torch.Tensor,  # (B,) – dataset indices of the current batch\n",
    "        embeddings: torch.Tensor,  # (B, D)\n",
    "        targets: torch.Tensor,  # (B,)\n",
    "        epoch: int,\n",
    "    ) -> torch.Tensor:\n",
    "        eps = 1e-6\n",
    "\n",
    "        # Keep an L2-normalised copy of current embeddings\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        self.past_embeddings[indexes] = embeddings.detach()\n",
    "\n",
    "        # ---------------- Centroid update ------------------------------------\n",
    "        if epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                for c, idxs in enumerate(self.bins):\n",
    "                    if idxs.numel():\n",
    "                        self.centroids[c] = self.past_embeddings[idxs].mean(0)\n",
    "        else:\n",
    "            # Shrink the subset of samples that contribute to the centroid\n",
    "            percent = int(max(1, min(100, 50 + 50 * (1 - epoch / self.total_epochs))))\n",
    "            for c, idxs in enumerate(self.bins):\n",
    "                if idxs.numel() == 0:\n",
    "                    continue\n",
    "                # bottom-k u’s  (small u  ⇒ low confidence ⇒ smaller weight)\n",
    "                k = max(1, idxs.numel() * percent // 100)\n",
    "                u_batch = self.u[idxs].squeeze(1)\n",
    "                keep = torch.topk(u_batch, k, largest=False).indices  # (k,)\n",
    "                selected = idxs[keep]  # (k,)\n",
    "                self.centroids[c] = self.past_embeddings[selected].mean(0)\n",
    "\n",
    "        centroids = F.normalize(self.centroids, dim=1)  # (C, D)\n",
    "\n",
    "        # ---------------- Probability shaping --------------------------------\n",
    "        soft_labels = F.softmax(embeddings @ centroids.T, dim=1)  # (B, C)\n",
    "        probs = F.softmax(logits, dim=1)  # (B, C)\n",
    "        u_vals = torch.sigmoid(self.u[indexes]).squeeze(1)  # (B,)\n",
    "\n",
    "        adjusted = (probs + u_vals[:, None] * soft_labels).clamp(min=eps)\n",
    "        adjusted = adjusted / adjusted.sum(1, keepdim=True)\n",
    "\n",
    "        # ---------------- Loss terms -----------------------------------------\n",
    "        hard_ce = (\n",
    "            (1.0 - u_vals) * F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        ).mean()\n",
    "        soft_ce = -(soft_labels * torch.log(adjusted)).sum(1).mean()\n",
    "        consistency = F.mse_loss(adjusted, soft_labels)\n",
    "\n",
    "        return hard_ce + soft_ce + self.lambda_consistency * consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (\n",
    "            1\n",
    "            - torch.nn.functional.one_hot(targets, num_classes=logits.size(1))\n",
    "            .float()\n",
    "            .sum(dim=1)\n",
    "        )\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCELoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 6, alpha: float = 0.1, beta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # CCE\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "\n",
    "        # RCE\n",
    "        pred = F.softmax(logits, dim=1).clamp(min=1e-6, max=1 - 1e-6)\n",
    "        one_hot = F.one_hot(targets, self.num_classes).float()\n",
    "        rce = -(1 - one_hot) * torch.log(1 - pred)\n",
    "        rce = rce.sum(dim=1)\n",
    "        return (self.alpha * ce + self.beta * rce).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    optimizer_theta: torch.optim.Optimizer,\n",
    "    optimizer_u: torch.optim.Optimizer | None,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    checkpoint_path: str,\n",
    "    current_epoch: int,\n",
    "    save_checkpoints: bool = True,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        node_emb = model.gnn_node(batch)\n",
    "        graph_emb = model.pool(node_emb, batch.batch)\n",
    "        logits = model.graph_pred_linear(graph_emb)\n",
    "\n",
    "        if isinstance(criterion, NCODLoss):\n",
    "            loss = criterion(\n",
    "                logits=logits,\n",
    "                indexes=batch.idx.to(device),\n",
    "                embeddings=graph_emb,\n",
    "                targets=batch.y.to(device),\n",
    "                epoch=current_epoch,\n",
    "            )\n",
    "        else:\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "        optimizer_theta.zero_grad(set_to_none=True)\n",
    "        if optimizer_u is not None:\n",
    "            optimizer_u.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_theta.step()\n",
    "        if optimizer_u is not None:\n",
    "            optimizer_u.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            batch_size = batch.y.size(0)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), 1)).sum().item()\n",
    "            )\n",
    "\n",
    "    if save_checkpoints:\n",
    "        ckpt = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt)\n",
    "        print(f\"[checkpoint] saved: {ckpt}\")\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss, avg_confidence, accuracy, avg_entropy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            node_emb = model.gnn_node(batch)\n",
    "            graph_emb = model.pool(node_emb, batch.batch)\n",
    "            logits = model.graph_pred_linear(graph_emb)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "            batch_size = batch.y.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), dim=1)).sum().item()\n",
    "            )\n",
    "\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_setup(dataset_name):\n",
    "    script_root = os.getcwd()\n",
    "    logs_dir = os.path.join(script_root, \"logs\", dataset_name)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_dir, \"training.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    checkpoints_dir = os.path.join(script_root, \"checkpoints\", dataset_name)\n",
    "    best_model_path = os.path.join(checkpoints_dir, f\"model_{dataset_name}_best.pth\")\n",
    "    return logs_dir, checkpoints_dir, best_model_path\n",
    "\n",
    "\n",
    "def dataloader_setup(dataset_name, batch_size):\n",
    "    train_path = f\"./datasets/{dataset_name}/train.json.gz\"\n",
    "    full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    train_dataset = IndexedSubset(train_dataset)\n",
    "    val_dataset = IndexedSubset(val_dataset)\n",
    "\n",
    "    # ---- WeightedRandomSampler for class balancing ----\n",
    "    labels = [int(data.y[0]) for data in train_dataset]\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        sample_weights, num_samples=len(sample_weights), replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,  # replaces shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, full_dataset, train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial: Optional[optuna.trial.Trial] = None,\n",
    "    *,\n",
    "    dataset_name: str,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"accuracy\",\n",
    "    checkpoints: int = 5,\n",
    "    epochs: int = 150,\n",
    "    train_loader: Optional[DataLoader] = None,\n",
    "    val_loader: Optional[DataLoader] = None,\n",
    "    full_dataset = None,\n",
    "    train_dataset = None,\n",
    "    val_dataset = None,\n",
    "    batch_size: Optional[int] = None,\n",
    "    logs_dir: Optional[str] = None,\n",
    "    checkpoints_dir: Optional[str] = None,\n",
    "    best_model_path: Optional[str],\n",
    "    gnn_type: Optional[str] = None,\n",
    "    loss_type: Optional[str] = None,\n",
    "    graph_pooling: Optional[str] = None,\n",
    "    drop_ratio: Optional[float] = None,\n",
    "    num_layers: Optional[int] = None,\n",
    "    embedding_dim: Optional[int] = None,\n",
    "    model: Optional[GNN] = None,\n",
    "    optimizer_theta: Optional[torch.optim.Optimizer] = None,\n",
    "    optimizer_u: Optional[torch.optim.Optimizer] = None,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"#\" * 80)\n",
    "        # Hyperparameter search space\n",
    "        logging.info(\"Start case study with parameters:\")\n",
    "\n",
    "        batch_size = batch_size or 64\n",
    "        if logs_dir is None or checkpoints_dir is None or best_model_path is None:\n",
    "            logs_dir, checkpoints_dir, best_model_path = directory_setup(dataset_name)\n",
    "\n",
    "        if train_loader is None or val_loader is None:\n",
    "            train_loader, val_loader, full_dataset, train_dataset, val_dataset = (\n",
    "                dataloader_setup(dataset_name, batch_size)\n",
    "            )\n",
    "        if trial is None:\n",
    "            gnn_type = gnn_type or \"gcn\"\n",
    "            graph_pooling = graph_pooling or \"mean\"\n",
    "            loss_type = loss_type or \"noisy_ce\"\n",
    "            drop_ratio = drop_ratio or 0.2\n",
    "            num_layers = num_layers or 11\n",
    "            embedding_dim = embedding_dim or 300\n",
    "\n",
    "        else:\n",
    "            gnn_type = trial.suggest_categorical(\n",
    "                \"gnn_type\", [\"gin\", \"gin-virtual\", \"gcn\", \"gcn-virtual\"]\n",
    "            )\n",
    "            loss_type = trial.suggest_categorical(\n",
    "                \"loss_type\", [\"ce\"] + [\"ncod\"] * 5 + [\"noisy_ce\"] * 5 + [\"sce\"]\n",
    "            )\n",
    "            graph_pooling = trial.suggest_categorical(\n",
    "                \"graph_pooling\",\n",
    "                [\"sum\"] * 2\n",
    "                + [\"mean\"] * 4\n",
    "                + [\"max\"] * 2\n",
    "                + [\"attention\"] * 2\n",
    "                + [\"set2set\"],\n",
    "            )\n",
    "            drop_ratio = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "            num_layers = trial.suggest_int(\"num_layers\", 6, 12)\n",
    "            embedding_dim = trial.suggest_categorical(\n",
    "                \"embedding_dim\", [64, 128, 300, 600]\n",
    "            )\n",
    "        embedding_dim = (\n",
    "            2 * embedding_dim\n",
    "            if graph_pooling == \"set2set\" and loss_type == \"ncod\"\n",
    "            else embedding_dim\n",
    "        )\n",
    "        \n",
    "        if model is None:\n",
    "            model = GNN(\n",
    "                gnn_type=\"gin\" if \"gin\" in gnn_type else \"gcn\",\n",
    "                num_class=6,\n",
    "                num_layer=num_layers,\n",
    "                emb_dim=embedding_dim,\n",
    "                drop_ratio=drop_ratio,\n",
    "                virtual_node=\"virtual\" in gnn_type,\n",
    "                graph_pooling=graph_pooling,\n",
    "            )\n",
    "        # Initialize model\n",
    "        model = model.to(device)\n",
    "        print(f\"{model.__dict__}\\n{epochs=}\")\n",
    "        logging.info(f\"{model.__dict__} {epochs=}\")\n",
    "        if optimizer_theta is None:\n",
    "            optimizer_theta = torch.optim.Adam(\n",
    "                model.parameters(), lr=3e-4, weight_decay=1e-5\n",
    "            )\n",
    "\n",
    "        optimizer_u = None\n",
    "        if loss_type == \"ce\":\n",
    "            train_criterion = nn.CrossEntropyLoss()\n",
    "        elif loss_type == \"ncod\":\n",
    "            train_criterion = NCODLoss(\n",
    "                train_loader.dataset,\n",
    "                embedding_dimensions=embedding_dim,\n",
    "                total_epochs=epochs,\n",
    "                device=device,\n",
    "            )\n",
    "            if optimizer_u is None:\n",
    "                optimizer_u = torch.optim.SGD(train_criterion.parameters(), lr=1e-3)\n",
    "        elif loss_type == \"sce\":\n",
    "            train_criterion = SCELoss()\n",
    "        else:\n",
    "            train_criterion = NoisyCrossEntropyLoss(0.2)\n",
    "\n",
    "        val_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Checkpoint logic\n",
    "        checkpoint_epochs = [\n",
    "            int((i + 1) * epochs / checkpoints) for i in range(checkpoints)\n",
    "        ]\n",
    "\n",
    "        train_losses, train_confs, train_accs, train_entropies, train_scores = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        best_val_score = -float(\"inf\")\n",
    "        val_losses, val_confs, val_accs, val_entropies, val_scores = [], [], [], [], []\n",
    "\n",
    "        progress_bar = tqdm(range(epochs), desc=\"Training...\", leave=False)\n",
    "        for epoch in progress_bar:\n",
    "            train_loss, train_conf, train_acc, train_entropy = train(\n",
    "                train_loader,\n",
    "                model,\n",
    "                optimizer_theta,\n",
    "                optimizer_u,\n",
    "                train_criterion,\n",
    "                device,\n",
    "                save_checkpoints=(epoch + 1 in checkpoint_epochs),\n",
    "                checkpoint_path=os.path.join(checkpoints_dir, f\"model_{dataset_name}\"),\n",
    "                current_epoch=epoch,\n",
    "            )\n",
    "\n",
    "            val_loss, val_conf, val_acc, val_entropy = evaluate(\n",
    "                val_loader,\n",
    "                model,\n",
    "                val_criterion,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            train_confs.append(train_conf)\n",
    "            train_entropies.append(train_entropy)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            val_confs.append(val_conf)\n",
    "            val_entropies.append(val_entropy)\n",
    "\n",
    "            if score_type == \"loss\":\n",
    "                train_score = train_loss\n",
    "                val_score = -val_loss\n",
    "            elif score_type == \"entropy\":\n",
    "                train_score = train_entropy\n",
    "                val_score = -val_entropy\n",
    "            elif score_type == \"confidence\":\n",
    "                train_score = train_conf\n",
    "                val_score = val_conf\n",
    "            else:\n",
    "                train_score = train_acc\n",
    "                val_score = val_acc\n",
    "\n",
    "            train_scores.append(train_score)\n",
    "            val_scores.append(val_score)\n",
    "            if trial is not None:\n",
    "                trial.report(val_score, step=epoch)\n",
    "\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                logging.info(\n",
    "                    f\"[{dataset_name}] Best model updated at {best_model_path}\"\n",
    "                )\n",
    "            progress_bar.set_postfix_str(\n",
    "                f\"Train Score: {train_score:.4f}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}| \"\n",
    "                f\"Val Score: {val_score:.4f}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\"\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"Epoch {epoch}/{epochs}| \"\n",
    "                f\"Train Score: {train_score:.4f}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}| \"\n",
    "                f\"Val Score: {val_score:.4f}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\"\n",
    "            )\n",
    "            if trial is not None and trial.should_prune():\n",
    "                logging.warning(f\"Trial was pruned at epoch {epoch}\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        logging.info(f\"Case study end, {best_val_score}\")\n",
    "        # Plot training curves\n",
    "        plot_training_progress(\n",
    "            train_losses, train_scores, os.path.join(logs_dir, \"train_plots\")\n",
    "        )\n",
    "        plot_training_progress(\n",
    "            val_losses, val_scores, os.path.join(logs_dir, \"val_plots\")\n",
    "        )\n",
    "\n",
    "        return best_val_score\n",
    "    except optuna.exceptions.TrialPruned as e:\n",
    "        raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Unhandled exception \", e)\n",
    "        raise e\n",
    "        return -float(\"inf\")\n",
    "\n",
    "    finally:\n",
    "        \n",
    "        logging.info(\"#\" * 80)\n",
    "        logging.info(\"\\n\")\n",
    "        if trial is None:\n",
    "            del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "            gc.collect()\n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_study(\n",
    "    dataset_name: Literal[\"A\", \"B\", \"C\", \"D\"],\n",
    "    n_trials: int = 30,\n",
    "    checkpoints: int = 5,\n",
    "    batch_size: int = 64,\n",
    "    epochs=150,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"accuracy\",\n",
    "):\n",
    "    logs_dir, checkpoints_dir, best_model_path = directory_setup(dataset_name)\n",
    "    summary_csv_path = os.path.join(logs_dir, f\"optuna_summary_{dataset_name}.csv\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    train_loader, val_loader, full_dataset, train_dataset, val_dataset = (\n",
    "        dataloader_setup(dataset_name, batch_size)\n",
    "    )\n",
    "    print(f\"Starting Optuna optimization for dataset {dataset_name}\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=dataset_name,\n",
    "        direction=\"maximize\",\n",
    "        pruner=MedianPruner(n_warmup_steps=10),\n",
    "        sampler=optuna.samplers.TPESampler(n_startup_trials=0),\n",
    "    )\n",
    "\n",
    "    obj = partial(\n",
    "        objective,\n",
    "        dataset_name=dataset_name,\n",
    "        score_type=score_type,\n",
    "        epochs=epochs,\n",
    "        checkpoints=checkpoints,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        batch_size=batch_size,\n",
    "        logs_dir=logs_dir,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        best_model_path=best_model_path,\n",
    "    )\n",
    "    study.optimize(obj, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    all_trials = []\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            row = {score_type: trial.value}\n",
    "            row.update(trial.params)\n",
    "            all_trials.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_trials).sort_values(score_type, ascending=False)\n",
    "    results_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nAll trials saved to: {summary_csv_path}\")\n",
    "    print(f\"\\nBest result for dataset {dataset_name}:\")\n",
    "    display(results_df)\n",
    "    print(f\"\\nBest Params for {dataset_name}:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "    gc.collect()\n",
    "    return study.best_trial.params, study.best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_on_dataset(\n",
    "#     model,\n",
    "#     optimizer_theta,\n",
    "#     dataset_name,\n",
    "#     logs_dir,\n",
    "#     checkpoints_dir,\n",
    "#     best_model_path,\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "# ):\n",
    "#     train_loader, val_loader, full_dataset, train_dataset, val_dataset = (\n",
    "#         dataloader_setup(dataset_name, 64)\n",
    "#     )\n",
    "#     objective(\n",
    "#         dataset_name=\"aggregated\",\n",
    "#         score_type=\"accuracy\",\n",
    "#         checkpoints=5,\n",
    "#         epochs=50,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         batch_size=64,\n",
    "#         logs_dir=logs_dir,\n",
    "#         checkpoints_dir=checkpoints_dir,\n",
    "#         best_model_path=best_model_path,\n",
    "#         model=model,\n",
    "#         optimizer_theta = optimizer_theta,\n",
    "#         device=device,\n",
    "#     )\n",
    "#     # del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "#     # gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_type = \"gcn\"\n",
    "# graph_pooling = \"mean\"\n",
    "# loss_type = \"noisy_ce\"\n",
    "# drop_ratio = 0.2\n",
    "# num_layers = 11\n",
    "# embedding_dim = 300\n",
    "\n",
    "# model = GNN(\n",
    "#     gnn_type=gnn_type,\n",
    "#     num_class=6,\n",
    "#     num_layer=num_layers,\n",
    "#     emb_dim=embedding_dim,\n",
    "#     drop_ratio=drop_ratio,\n",
    "#     virtual_node=\"virtual\" in gnn_type,\n",
    "#     graph_pooling=graph_pooling,\n",
    "# )\n",
    "# optimizer_theta = torch.optim.Adam(\n",
    "#             model.parameters(), lr=3e-4, weight_decay=1e-5\n",
    "#         )\n",
    "# logs_dir, checkpoints_dir, best_model_path = directory_setup(\"aggregated\")\n",
    "# train_loader, val_loader, full_dataset, train_dataset, val_dataset = dataloader_setup(\n",
    "#     \"C\", 64\n",
    "# )\n",
    "# for name in [\"D\",\"C\",  \"B\", \"A\"]:\n",
    "#     train_on_dataset(model, optimizer_theta,name,logs_dir,checkpoints_dir,best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 20:33:55,755] A new study created in memory with name: A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna optimization for dataset A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a499f3ef781a4c4ab910c9c704ce6332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': True, '_parameters': {}, '_buffers': {}, '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': {'gnn_node': GNN_node_Virtualnode(\n",
      "  (node_encoder): Embedding(1, 300)\n",
      "  (virtualnode_embedding): Embedding(1, 300)\n",
      "  (convs): ModuleList(\n",
      "    (0-9): 10 x GINConv()\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0-9): 10 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (mlp_virtualnode_list): ModuleList(\n",
      "    (0-8): 9 x Sequential(\n",
      "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
      "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=600, out_features=300, bias=True)\n",
      "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "), 'graph_pred_linear': Linear(in_features=300, out_features=6, bias=True)}, 'num_layer': 10, 'drop_ratio': 0.28425527459347893, 'JK': 'last', 'emb_dim': 300, 'num_class': 6, 'graph_pooling': 'max', 'pool': <function global_max_pool at 0x79d933dcc680>}\n",
      "epochs=20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5bee57ac8d48a6bc0e845d3cbd4268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unhandled exception  CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 391.06 MiB is free. Process 2160 has 7.59 MiB memory in use. Process 10052 has 23.84 MiB memory in use. Process 11053 has 3.98 GiB memory in use. Including non-PyTorch memory, this process has 6.46 GiB memory in use. Of the allocated memory 5.85 GiB is allocated by PyTorch, and 397.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[W 2025-05-29 20:33:57,378] Trial 0 failed with parameters: {'gnn_type': 'gin-virtual', 'loss_type': 'sce', 'graph_pooling': 'max', 'dropout': 0.28425527459347893, 'num_layers': 10, 'embedding_dim': 300} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 391.06 MiB is free. Process 2160 has 7.59 MiB memory in use. Process 10052 has 23.84 MiB memory in use. Process 11053 has 3.98 GiB memory in use. Including non-PyTorch memory, this process has 6.46 GiB memory in use. Of the allocated memory 5.85 GiB is allocated by PyTorch, and 397.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_17938/948464587.py\", line 210, in objective\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_17938/948464587.py\", line 130, in objective\n",
      "    train_loss, train_conf, train_acc, train_entropy = train(\n",
      "                                                       ^^^^^^\n",
      "  File \"/tmp/ipykernel_17938/1993452241.py\", line 20, in train\n",
      "    node_emb = model.gnn_node(batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/hackaton/src/conv.py\", line 202, in forward\n",
      "    h = self.convs[layer](h_list[layer], edge_index, edge_attr)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/hackaton/src/conv.py\", line 25, in forward\n",
      "    out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/src.conv_GINConv_propagate_csi8gfoq.py\", line 183, in propagate\n",
      "    out = self.message(\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/hackaton/src/conv.py\", line 30, in message\n",
      "    return F.relu(x_j + edge_attr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 1704, in relu\n",
      "    result = torch.relu(input)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 391.06 MiB is free. Process 2160 has 7.59 MiB memory in use. Process 10052 has 23.84 MiB memory in use. Process 11053 has 3.98 GiB memory in use. Including non-PyTorch memory, this process has 6.46 GiB memory in use. Of the allocated memory 5.85 GiB is allocated by PyTorch, and 397.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[W 2025-05-29 20:33:57,379] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 391.06 MiB is free. Process 2160 has 7.59 MiB memory in use. Process 10052 has 23.84 MiB memory in use. Process 11053 has 3.98 GiB memory in use. Including non-PyTorch memory, this process has 6.46 GiB memory in use. Of the allocated memory 5.85 GiB is allocated by PyTorch, and 397.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcase_study\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mcase_study\u001b[39m\u001b[34m(dataset_name, n_trials, checkpoints, batch_size, epochs, score_type)\u001b[39m\n\u001b[32m     17\u001b[39m study = optuna.create_study(\n\u001b[32m     18\u001b[39m     study_name=dataset_name,\n\u001b[32m     19\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     pruner=MedianPruner(n_warmup_steps=\u001b[32m10\u001b[39m),\n\u001b[32m     21\u001b[39m     sampler=optuna.samplers.TPESampler(n_startup_trials=\u001b[32m0\u001b[39m),\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m obj = partial(\n\u001b[32m     25\u001b[39m     objective,\n\u001b[32m     26\u001b[39m     dataset_name=dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     best_model_path=best_model_path,\n\u001b[32m     36\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m all_trials = []\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m study.trials:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 210\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial, dataset_name, score_type, checkpoints, epochs, train_loader, val_loader, full_dataset, train_dataset, val_dataset, batch_size, logs_dir, checkpoints_dir, best_model_path, gnn_type, loss_type, graph_pooling, drop_ratio, num_layers, embedding_dim, model, optimizer_theta, optimizer_u, device)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnhandled exception \u001b[39m\u001b[33m\"\u001b[39m, e)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial, dataset_name, score_type, checkpoints, epochs, train_loader, val_loader, full_dataset, train_dataset, val_dataset, batch_size, logs_dir, checkpoints_dir, best_model_path, gnn_type, loss_type, graph_pooling, drop_ratio, num_layers, embedding_dim, model, optimizer_theta, optimizer_u, device)\u001b[39m\n\u001b[32m    128\u001b[39m progress_bar = tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining...\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     train_loss, train_conf, train_acc, train_entropy = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_u\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_epochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     val_loss, val_conf, val_acc, val_entropy = evaluate(\n\u001b[32m    143\u001b[39m         val_loader,\n\u001b[32m    144\u001b[39m         model,\n\u001b[32m    145\u001b[39m         val_criterion,\n\u001b[32m    146\u001b[39m         device,\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    149\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(data_loader, model, optimizer_theta, optimizer_u, criterion, device, checkpoint_path, current_epoch, save_checkpoints)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[32m     18\u001b[39m     batch = batch.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     node_emb = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgnn_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     graph_emb = model.pool(node_emb, batch.batch)\n\u001b[32m     22\u001b[39m     logits = model.graph_pred_linear(graph_emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/conv.py:202\u001b[39m, in \u001b[36mGNN_node_Virtualnode.forward\u001b[39m\u001b[34m(self, batched_data)\u001b[39m\n\u001b[32m    199\u001b[39m h_list[layer] = h_list[layer] + virtualnode_embedding[batch]\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m### Message passing among graph nodes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m h = \u001b[38;5;28mself\u001b[39m.batch_norms[layer](h)\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m layer == \u001b[38;5;28mself\u001b[39m.num_layer - \u001b[32m1\u001b[39m:\n\u001b[32m    206\u001b[39m     \u001b[38;5;66;03m#remove relu for the last layer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/conv.py:25\u001b[39m, in \u001b[36mGINConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[32m     24\u001b[39m     edge_embedding = \u001b[38;5;28mself\u001b[39m.edge_encoder(edge_attr)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.mlp((\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.eps) *x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_embedding\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/src.conv_GINConv_propagate_csi8gfoq.py:183\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, edge_attr, size)\u001b[39m\n\u001b[32m    174\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    175\u001b[39m                 x_j=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_j\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    176\u001b[39m                 edge_attr=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33medge_attr\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m                 dim_size=kwargs.dim_size,\n\u001b[32m    180\u001b[39m             )\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/conv.py:30\u001b[39m, in \u001b[36mGINConv.message\u001b[39m\u001b[34m(self, x_j, edge_attr)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j, edge_attr):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 391.06 MiB is free. Process 2160 has 7.59 MiB memory in use. Process 10052 has 23.84 MiB memory in use. Process 11053 has 3.98 GiB memory in use. Including non-PyTorch memory, this process has 6.46 GiB memory in use. Of the allocated memory 5.85 GiB is allocated by PyTorch, and 397.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "case_study(\n",
    "    \"A\",\n",
    "    60,\n",
    "    batch_size=64,\n",
    "    epochs = 20,\n",
    "    score_type=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_study(\n",
    "#     \"C\", n_trials=100, num_checkpoints=5, default_batch_size=64, score_type=\"composite\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_study(\n",
    "#     \"D\", n_trials=100, num_checkpoints=5, default_batch_size=64, score_type=\"composite\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
