{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from functools import partial\n",
    "import optuna\n",
    "import gc\n",
    "from typing import Literal\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, Dataset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\"id\": test_graph_ids, \"pred\": predictions})\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy per Epoch\")\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedSubset(Dataset):\n",
    "    def __init__(self, subset: Subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = self.subset[i]\n",
    "        data.idx = torch.tensor(i, dtype=torch.long)  # type: ignore\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCODLoss(nn.Module):\n",
    "    past_embeddings: torch.Tensor\n",
    "    centroids: torch.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        embedding_dimensions: int = 300,\n",
    "        total_epochs: int = 150,\n",
    "        lambda_consistency: float = 1.0,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        dataset : iterable whose elements expose an integer label in `elem.y`\n",
    "        embedding_dimensions : size of the feature vectors\n",
    "        total_epochs : number of training epochs (used for centroid update schedule)\n",
    "        lambda_consistency : weight for the MSE consistency term\n",
    "        device : cuda / cpu device.  If None, picks CUDA if available.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device or torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "\n",
    "        labels = [int(elem.y) for elem in dataset]\n",
    "        self.num_elements = len(labels)\n",
    "        self.num_classes = max(labels) + 1  # robust to gaps (e.g. labels {1,3})\n",
    "        # self.register_buffer(\"bins\", torch.empty(self.num_classes, 0, dtype=torch.long))\n",
    "\n",
    "        # Convert bins to a list-of-lists for easy appends, then to tensors\n",
    "        tmp_bins: list[list[int]] = [[] for _ in range(self.num_classes)]\n",
    "        for idx, lab in enumerate(labels):\n",
    "            tmp_bins[lab].append(idx)\n",
    "        self.bins = [\n",
    "            torch.as_tensor(b, dtype=torch.long, device=self.device) for b in tmp_bins\n",
    "        ]\n",
    "\n",
    "        # Confidence parameter per sample (trainable!)\n",
    "        self.u = nn.Parameter(torch.empty(self.num_elements, 1, device=self.device))\n",
    "        nn.init.normal_(self.u, mean=1e-8, std=1e-9)\n",
    "\n",
    "        # Running memory of embeddings\n",
    "        self.register_buffer(\n",
    "            \"past_embeddings\",\n",
    "            torch.rand(\n",
    "                self.num_elements, self.embedding_dimensions, device=self.device\n",
    "            ),\n",
    "        )\n",
    "        # Class centroids\n",
    "        self.register_buffer(\n",
    "            \"centroids\",\n",
    "            torch.rand(self.num_classes, self.embedding_dimensions, device=self.device),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        logits: torch.Tensor,  # (B, C)\n",
    "        indexes: torch.Tensor,  # (B,) – dataset indices of the current batch\n",
    "        embeddings: torch.Tensor,  # (B, D)\n",
    "        targets: torch.Tensor,  # (B,)\n",
    "        epoch: int,\n",
    "    ) -> torch.Tensor:\n",
    "        eps = 1e-6\n",
    "\n",
    "        # Keep an L2-normalised copy of current embeddings\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        self.past_embeddings[indexes] = embeddings.detach()\n",
    "\n",
    "        # ---------------- Centroid update ------------------------------------\n",
    "        if epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                for c, idxs in enumerate(self.bins):\n",
    "                    if idxs.numel():\n",
    "                        self.centroids[c] = self.past_embeddings[idxs].mean(0)\n",
    "        else:\n",
    "            # Shrink the subset of samples that contribute to the centroid\n",
    "            percent = int(max(1, min(100, 50 + 50 * (1 - epoch / self.total_epochs))))\n",
    "            for c, idxs in enumerate(self.bins):\n",
    "                if idxs.numel() == 0:\n",
    "                    continue\n",
    "                # bottom-k u’s  (small u  ⇒ low confidence ⇒ smaller weight)\n",
    "                k = max(1, idxs.numel() * percent // 100)\n",
    "                u_batch = self.u[idxs].squeeze(1)\n",
    "                keep = torch.topk(u_batch, k, largest=False).indices  # (k,)\n",
    "                selected = idxs[keep]  # (k,)\n",
    "                self.centroids[c] = self.past_embeddings[selected].mean(0)\n",
    "\n",
    "        centroids = F.normalize(self.centroids, dim=1)  # (C, D)\n",
    "\n",
    "        # ---------------- Probability shaping --------------------------------\n",
    "        soft_labels = F.softmax(embeddings @ centroids.T, dim=1)  # (B, C)\n",
    "        probs = F.softmax(logits, dim=1)  # (B, C)\n",
    "        u_vals = torch.sigmoid(self.u[indexes]).squeeze(1)  # (B,)\n",
    "\n",
    "        adjusted = (probs + u_vals[:, None] * soft_labels).clamp(min=eps)\n",
    "        adjusted = adjusted / adjusted.sum(1, keepdim=True)\n",
    "\n",
    "        # ---------------- Loss terms -----------------------------------------\n",
    "        hard_ce = (\n",
    "            (1.0 - u_vals) * F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        ).mean()\n",
    "        soft_ce = -(soft_labels * torch.log(adjusted)).sum(1).mean()\n",
    "        consistency = F.mse_loss(adjusted, soft_labels)\n",
    "\n",
    "        return hard_ce + soft_ce + self.lambda_consistency * consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (\n",
    "            1\n",
    "            - torch.nn.functional.one_hot(targets, num_classes=logits.size(1))\n",
    "            .float()\n",
    "            .sum(dim=1)\n",
    "        )\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCELoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 6, alpha: float = 0.1, beta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # CCE\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "\n",
    "        # RCE\n",
    "        pred = F.softmax(logits, dim=1).clamp(min=1e-6, max=1 - 1e-6)\n",
    "        one_hot = F.one_hot(targets, self.num_classes).float()\n",
    "        rce = -(1 - one_hot) * torch.log(1 - pred)\n",
    "        rce = rce.sum(dim=1)\n",
    "        return (self.alpha * ce + self.beta * rce).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    optimizer_theta: torch.optim.Optimizer,\n",
    "    optimizer_u: torch.optim.Optimizer | None,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    checkpoint_path: str,\n",
    "    current_epoch: int,\n",
    "    save_checkpoints: bool = True,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        node_emb = model.gnn_node(batch)\n",
    "        graph_emb = model.pool(node_emb, batch.batch)\n",
    "        logits = model.graph_pred_linear(graph_emb)\n",
    "\n",
    "        if isinstance(criterion, NCODLoss):\n",
    "            loss = criterion(\n",
    "                logits=logits,\n",
    "                indexes=batch.idx.to(device),\n",
    "                embeddings=graph_emb,\n",
    "                targets=batch.y.to(device),\n",
    "                epoch=current_epoch,\n",
    "            )\n",
    "        else:\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "        optimizer_theta.zero_grad(set_to_none=True)\n",
    "        if optimizer_u is not None:\n",
    "            optimizer_u.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_theta.step()\n",
    "        if optimizer_u is not None:\n",
    "            optimizer_u.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            batch_size = batch.y.size(0)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), 1)).sum().item()\n",
    "            )\n",
    "\n",
    "    if save_checkpoints:\n",
    "        ckpt = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt)\n",
    "        print(f\"[checkpoint] saved: {ckpt}\")\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_loader: DataLoader,\n",
    "    model: GNN,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss, avg_confidence, accuracy, avg_entropy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = total_conf = total_entropy = 0.0\n",
    "    correct = num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            node_emb = model.gnn_node(batch)\n",
    "            graph_emb = model.pool(node_emb, batch.batch)\n",
    "            logits = model.graph_pred_linear(graph_emb)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            loss = criterion(logits, batch.y)\n",
    "\n",
    "            batch_size = batch.y.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            total_conf += probs.max(dim=1).values.sum().item()\n",
    "            total_entropy += (\n",
    "                (-torch.sum(probs * torch.log(probs + 1e-10), dim=1)).sum().item()\n",
    "            )\n",
    "\n",
    "            pred = probs.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "    loss = total_loss / num_samples\n",
    "    confidence = total_conf / num_samples\n",
    "    entropy = total_entropy / num_samples\n",
    "    accuracy = correct / num_samples\n",
    "\n",
    "    return loss, confidence, accuracy, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_checkpoints: int,\n",
    "    checkpoints_dir: str,\n",
    "    run_name: str,\n",
    "    best_model_path: str,\n",
    "    logs_dir: str,\n",
    "    resume_training: bool,\n",
    "    *,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"confidence\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "):  # -> float | Any:\n",
    "    try:\n",
    "        logging.info(\"#\" * 80)\n",
    "        # Hyperparameter search space\n",
    "        logging.info(\"Start case study with parameters:\")\n",
    "        gnn_type = trial.suggest_categorical(\n",
    "            \"gnn_type\", [\"gin\", \"gin-virtual\", \"gcn\", \"gcn-virtual\"]\n",
    "        )\n",
    "        loss_type = trial.suggest_categorical(\n",
    "            \"loss_type\", [\"ce\", \"ncod\", \"noisy_ce\", \"sce\"]\n",
    "        )\n",
    "        graph_pooling = trial.suggest_categorical(\"graph_pooling\", [\"sum\",\"mean\",\"max\",\"attention\",\"set2set\"])\n",
    "        drop_ratio = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "        num_layers = 6 #trial.suggest_int(\"num_layers\", 6, 12)\n",
    "        embedding_dim = 300  # trial.suggest_categorical(\"embedding_dim\", [64, 128, 300])\n",
    "        num_epochs = 30  # trial.suggest_int(\"num_epochs\", 10, 11, step=1)\n",
    "\n",
    "        print(f\"{gnn_type=}\\n{loss_type=}\\n{graph_pooling=}\\n{drop_ratio=}\\n{num_layers=}\\n{embedding_dim=}\\n{num_epochs=}\")\n",
    "        logging.info(f\"{gnn_type=} {loss_type=} {graph_pooling=} {drop_ratio=} {num_layers=} {embedding_dim=} {num_epochs=}\")\n",
    "\n",
    "        # Initialize model\n",
    "        model = GNN(\n",
    "            gnn_type=\"gin\" if \"gin\" in gnn_type else \"gcn\",\n",
    "            num_class=6,\n",
    "            num_layer=num_layers,\n",
    "            emb_dim=embedding_dim,\n",
    "            drop_ratio=drop_ratio,\n",
    "            virtual_node=\"virtual\" in gnn_type,\n",
    "            graph_pooling=graph_pooling\n",
    "        ).to(device)\n",
    "        if resume_training:\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "        optimizer_theta = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "        optimizer_u = None\n",
    "        if loss_type == \"ce\":\n",
    "            train_criterion = nn.CrossEntropyLoss()\n",
    "        elif loss_type == \"ncod\":\n",
    "            train_criterion = NCODLoss(\n",
    "                train_loader.dataset,\n",
    "                embedding_dimensions=embedding_dim,\n",
    "                total_epochs=num_epochs,\n",
    "                device=device,\n",
    "            )\n",
    "            optimizer_u = torch.optim.SGD(train_criterion.parameters(), lr=1e-3)\n",
    "        elif loss_type == \"sce\":\n",
    "            train_criterion = SCELoss()\n",
    "        else:\n",
    "            train_criterion = NoisyCrossEntropyLoss(0.2)\n",
    "\n",
    "        val_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Checkpoint logic\n",
    "        checkpoint_epochs = [\n",
    "            int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)\n",
    "        ]\n",
    "\n",
    "        best_val_score = -float(\"inf\")\n",
    "        train_losses, train_confs, train_accs, train_entropies, train_scores = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        val_losses, val_confs, val_accs, val_entropies, val_scores = [], [], [], [], []\n",
    "\n",
    "        progress_bar = tqdm(range(num_epochs), desc=\"Training...\", leave=False)\n",
    "        for epoch in progress_bar:\n",
    "            train_loss, train_conf, train_acc, train_entropy = train(\n",
    "                train_loader,\n",
    "                model,\n",
    "                optimizer_theta,\n",
    "                optimizer_u,\n",
    "                train_criterion,\n",
    "                device,\n",
    "                save_checkpoints=(epoch + 1 in checkpoint_epochs),\n",
    "                checkpoint_path=os.path.join(checkpoints_dir, f\"model_{run_name}\"),\n",
    "                current_epoch=epoch,\n",
    "            )\n",
    "\n",
    "            val_loss, val_conf, val_acc, val_entropy = evaluate(\n",
    "                val_loader,\n",
    "                model,\n",
    "                val_criterion,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            tqdm.write(\n",
    "                f\"Epoch {epoch}/{num_epochs}\\n\"\n",
    "                f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}\\n\"\n",
    "                f\"Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\\n\"\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"Epoch {epoch}/{num_epochs}| \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Confidence: {train_conf:.4f}, Entropy: {train_entropy:.4f}| \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Confidence: {val_conf:.4f}, Entropy: {val_entropy:.4f}\"\n",
    "            )\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            train_confs.append(train_conf)\n",
    "            train_entropies.append(train_entropy)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            val_confs.append(val_conf)\n",
    "            val_entropies.append(val_entropy)\n",
    "\n",
    "            if score_type == \"loss\":\n",
    "                train_score = train_loss\n",
    "                val_score = val_loss\n",
    "            elif score_type == \"entropy\":\n",
    "                train_score = train_entropy\n",
    "                val_score = val_entropy\n",
    "            elif score_type == \"confidence\":\n",
    "                train_score = train_conf\n",
    "                val_score = val_conf\n",
    "            else:\n",
    "                train_score = train_acc\n",
    "                val_score = val_acc\n",
    "\n",
    "            train_scores.append(train_score)\n",
    "            val_scores.append(val_score)\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                logging.info(f\"[{run_name}] Best model updated at {best_model_path}\")\n",
    "\n",
    "        # Plot training curves\n",
    "        plot_training_progress(\n",
    "            train_losses, train_scores, os.path.join(logs_dir, \"train_plots\")\n",
    "        )\n",
    "        plot_training_progress(val_losses, val_scores, os.path.join(logs_dir, \"val_plots\"))\n",
    "\n",
    "        logging.info(f\"Case study end, {best_val_score}\")\n",
    "        logging.info(\"#\" * 80)\n",
    "        logging.info(\"\\n\")\n",
    "\n",
    "        return best_val_score\n",
    "    except Exception as e:\n",
    "        print(\"Unhandled exception \",e)\n",
    "        return - float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_study(\n",
    "    version_number,\n",
    "    resume_training: bool,\n",
    "    n_trials: int = 30,\n",
    "    num_checkpoints: int = 10,\n",
    "    default_batch_size: int = 32,\n",
    "    score_type: Literal[\"loss\", \"accuracy\", \"entropy\", \"confidence\"] = \"confidence\",\n",
    "    full_dataset=None,\n",
    "):\n",
    "    script_root = os.getcwd()\n",
    "    train_path = (\n",
    "        f\"./datasets/filtered_aggregated/filtered_aggregated_{version_number}.json.gz\"\n",
    "    )\n",
    "    run_name = f\"filtered_aggregated_{version_number}\"\n",
    "\n",
    "    logs_dir = os.path.join(script_root, \"logs\", run_name)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_dir, \"training.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "\n",
    "    checkpoints_dir = os.path.join(script_root, \"checkpoints\", run_name)\n",
    "    best_model_path = os.path.join(\n",
    "        checkpoints_dir, f\"baseline_{version_number}_best.pth\"\n",
    "    )\n",
    "    summary_csv_path = os.path.join(logs_dir, f\"optuna_summary_{version_number}.csv\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    if full_dataset is None:\n",
    "        full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "    \n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    train_dataset = IndexedSubset(train_dataset)\n",
    "    val_dataset = IndexedSubset(val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,  # type:ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,  # type:ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Starting Optuna optimization for dataset {version_number}\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=run_name,\n",
    "        direction=\"minimize\"\n",
    "        if score_type == \"loss\" or score_type == \"entropy\"\n",
    "        else \"maximize\",\n",
    "    )\n",
    "\n",
    "    obj = partial(\n",
    "        objective,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_checkpoints=num_checkpoints,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        run_name=run_name,\n",
    "        best_model_path=best_model_path,\n",
    "        logs_dir=logs_dir,\n",
    "        resume_training=resume_training,\n",
    "        score_type=score_type,\n",
    "    )\n",
    "    study.optimize(obj, n_trials=n_trials)\n",
    "\n",
    "    all_trials = []\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            row = {score_type: trial.value}\n",
    "            row.update(trial.params)\n",
    "            all_trials.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_trials)\n",
    "    results_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nAll trials saved to: {summary_csv_path}\")\n",
    "    print(f\"\\nBest result for dataset {version_number}:\")\n",
    "    display(\n",
    "        results_df.sort_values(\n",
    "            score_type, ascending=(score_type == \"loss\" or score_type == \"entropy\")\n",
    "        )\n",
    "    )\n",
    "    print(f\"\\nBest Params for baseline {version_number}:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m filtered_aggregated = \u001b[43mGraphDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./datasets/merged/merged_dataset_v2.json.gz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_zeros\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/loadData.py:58\u001b[39m, in \u001b[36mGraphDataset.__init__\u001b[39m\u001b[34m(self, filename, transform, pre_transform, load_in_memory)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_graphs, \u001b[38;5;28mself\u001b[39m.graphs_dicts = \u001b[38;5;28mself\u001b[39m._load_all_graphs()\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_graphs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28mself\u001b[39m.graphs_dicts = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, transform, pre_transform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/loadData.py:82\u001b[39m, in \u001b[36mGraphDataset._count_graphs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_count_graphs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m gzip.open(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mrt\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mijson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mitem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/hackaton/src/loadData.py:82\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_count_graphs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m gzip.open(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mrt\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mijson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mitem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/noisy_labels/.venv/lib/python3.12/site-packages/ijson/compat.py:16\u001b[39m, in \u001b[36mutf8reader.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/gzip.py:337\u001b[39m, in \u001b[36mGzipFile.read1\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size < \u001b[32m0\u001b[39m:\n\u001b[32m    336\u001b[39m     size = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/_compression.py:68\u001b[39m, in \u001b[36mDecompressReader.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view.cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/gzip.py:550\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buf == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    547\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCompressed file ended before the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    548\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mend-of-stream marker was reached\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m \u001b[38;5;28mself\u001b[39m._crc = \u001b[43mzlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrc32\u001b[49m\u001b[43m(\u001b[49m\u001b[43muncompress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28mself\u001b[39m._stream_size += \u001b[38;5;28mlen\u001b[39m(uncompress)\n\u001b[32m    552\u001b[39m \u001b[38;5;28mself\u001b[39m._pos += \u001b[38;5;28mlen\u001b[39m(uncompress)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "filtered_aggregated = GraphDataset(\n",
    "    \"./datasets/merged/merged_dataset_v2.json.gz\", transform=add_zeros,load_in_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(filtered_aggregated,batch_size=64)\n",
    "for data in dataloader:\n",
    "    print(data.to(torch.device(\"cuda\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_study(1, False, n_trials=50,score_type=\"accuracy\", full_dataset=filtered_aggregated)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
