{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAQuCuIoBbq5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haislich/Documents/noisy_labels/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from functools import partial\n",
    "import optuna\n",
    "import gc\n",
    "from typing import Literal\n",
    "\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Dyf0I2-t9IcW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3jKvoQYI9Zbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    save_checkpoints,\n",
    "    checkpoint_path,\n",
    "    current_epoch,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8peFiIS19ZpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return total_loss / len(data_loader), accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WanuZKxy9Zs-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "\n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "\n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\"id\": test_graph_ids, \"pred\": predictions})\n",
    "\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uyHIJS5U9ZzB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy per Epoch\")\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (\n",
    "            1\n",
    "            - torch.nn.functional.one_hot(targets, num_classes=logits.size(1))\n",
    "            .float()\n",
    "            .sum(dim=1)\n",
    "        )\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_checkpoints,\n",
    "    checkpoints_dir,\n",
    "    run_name,\n",
    "    best_model_path,\n",
    "    logs_dir,\n",
    "):  # -> float | Any:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(\"#\" * 80)\n",
    "    # Hyperparameter search space\n",
    "    logging.info(\"Start case study with parameters:\")\n",
    "    gnn_type = trial.suggest_categorical(\n",
    "        \"gnn_type\", [\"gin\", \"gin-virtual\", \"gcn\", \"gcn-virtual\"]\n",
    "    )\n",
    "    drop_ratio = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 3, 6)\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 300, 600])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 50, 200, step=50)\n",
    "\n",
    "    logging.info(f\"{gnn_type=}\")\n",
    "    logging.info(f\"{drop_ratio=}\")\n",
    "    logging.info(f\"{num_layers=}\")\n",
    "    logging.info(f\"{embedding_dim=}\")\n",
    "    logging.info(f\"{num_epochs=}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNN(\n",
    "        gnn_type=\"gin\" if \"gin\" in gnn_type else \"gcn\",\n",
    "        num_class=6,\n",
    "        num_layer=num_layers,\n",
    "        emb_dim=embedding_dim,\n",
    "        drop_ratio=drop_ratio,\n",
    "        virtual_node=\"virtual\" in gnn_type,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Prepare checkpoints\n",
    "    checkpoint_epochs = [\n",
    "        int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)\n",
    "    ]\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            save_checkpoints=(epoch + 1 in checkpoint_epochs),\n",
    "            checkpoint_path=os.path.join(checkpoints_dir, f\"model_{run_name}\"),\n",
    "            current_epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "\n",
    "        msg = (\n",
    "            f\"[{run_name}] Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "        print(msg)\n",
    "        logging.info(msg)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            logging.info(f\"[{run_name}] Best model updated at {best_model_path}\")\n",
    "\n",
    "    plot_training_progress(\n",
    "        train_losses, train_accuracies, os.path.join(logs_dir, \"train_plots\")\n",
    "    )\n",
    "    plot_training_progress(\n",
    "        val_losses, val_accuracies, os.path.join(logs_dir, \"val_plots\")\n",
    "    )\n",
    "    logging.info(f\"Case study end, {best_val_accuracy}\")\n",
    "    logging.info(\"#\" * 80)\n",
    "    logging.info(\"\\n\")\n",
    "\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected default value expression (170094642.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdataset_name: Literal[\"A\",\"B\",\"C\",\"\"]=,\u001b[39m\n                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected default value expression\n"
     ]
    }
   ],
   "source": [
    "def case_study(\n",
    "    dataset_name: Literal[\"A\",\"B\",\"C\",\"D\"],\n",
    "    n_trials: int = 30,\n",
    "    resume_if_exists: bool = True,\n",
    "    num_checkpoints: int = 10,\n",
    "    default_batch_size: int = 32,\n",
    "    summary_csv_path: str = \"optuna_summary.csv\",\n",
    "):\n",
    "    script_root = os.getcwd()\n",
    "    train_path = f\"./datasets/{dataset_name}/train.json.gz\"\n",
    "    run_name = dataset_name\n",
    "\n",
    "    logs_dir = os.path.join(script_root, \"logs\", run_name)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(logs_dir, \"training.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    logging.getLogger().addHandler(logging.StreamHandler())\n",
    "\n",
    "    checkpoints_dir = os.path.join(script_root, \"checkpoints\", run_name)\n",
    "    best_model_path = os.path.join(checkpoints_dir, f\"model_{run_name}_best.pth\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    # Load or initialize results summary\n",
    "    if os.path.exists(summary_csv_path):\n",
    "        results_df = pd.read_csv(summary_csv_path)\n",
    "    else:\n",
    "        results_df = pd.DataFrame()\n",
    "\n",
    "    # Skip if already present and resume flag is set\n",
    "    if resume_if_exists and dataset_name in results_df.get(\"dataset\", []):\n",
    "        print(f\"âœ… Skipping {dataset_name}, already completed in summary.\")\n",
    "        display(results_df)\n",
    "        return\n",
    "\n",
    "    # Dataset loading\n",
    "    full_dataset = GraphDataset(train_path, transform=add_zeros)\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    generator = torch.Generator().manual_seed(12)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, # type: ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,# type: ignore\n",
    "        batch_size=default_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # Run Optuna study\n",
    "    print(f\"\\n--- Optimizing for dataset {dataset_name} ---\")\n",
    "    logging.info(f\"--- Starting Optuna optimization for dataset {dataset_name} ---\")\n",
    "\n",
    "    study = optuna.create_study(study_name=run_name, direction=\"maximize\")\n",
    "\n",
    "    obj = partial(\n",
    "        objective,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_checkpoints=num_checkpoints,\n",
    "        checkpoints_dir=checkpoints_dir,\n",
    "        run_name=run_name,\n",
    "        best_model_path=best_model_path,\n",
    "        logs_dir=logs_dir,\n",
    "    )\n",
    "    study.optimize(obj, n_trials=n_trials)\n",
    "\n",
    "    # Record best result\n",
    "    row = {\"dataset\": dataset_name, \"best_accuracy\": study.best_value}\n",
    "    row.update(study.best_params)\n",
    "    results_df = pd.concat(\n",
    "        [results_df[results_df[\"dataset\"] != dataset_name], pd.DataFrame([row])],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    results_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\n Best result for dataset {dataset_name}:\")\n",
    "    display(results_df)\n",
    "    print(f\"\\n Best Params for {dataset_name}:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del train_loader, val_loader, full_dataset, train_dataset, val_dataset\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa777efc784449509295335a8b342b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/4 |\n",
      "\tTrain - Loss: 1.0000, Acc: 3.0000, Conf: 2.0000, Ent: 4.0000 |\n",
      "\tVal - Loss: 5.0000, Acc: 7.0000, Conf: 6.0000, Ent: 8.0000\n",
      "\n",
      "Epoch 02/4 |\n",
      "\tTrain - Loss: 1.0000, Acc: 3.0000, Conf: 2.0000, Ent: 4.0000 |\n",
      "\tVal - Loss: 5.0000, Acc: 7.0000, Conf: 6.0000, Ent: 8.0000\n",
      "\n",
      "Epoch 03/4 |\n",
      "\tTrain - Loss: 1.0000, Acc: 3.0000, Conf: 2.0000, Ent: 4.0000 |\n",
      "\tVal - Loss: 5.0000, Acc: 7.0000, Conf: 6.0000, Ent: 8.0000\n",
      "\n",
      "Epoch 04/4 |\n",
      "\tTrain - Loss: 1.0000, Acc: 3.0000, Conf: 2.0000, Ent: 4.0000 |\n",
      "\tVal - Loss: 5.0000, Acc: 7.0000, Conf: 6.0000, Ent: 8.0000\n",
      "\n",
      "\u001b[F\u001b[K"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "num_epochs =  4\n",
    "progress_bar = tqdm(range(num_epochs), leave=False)\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    train_loss, train_conf, train_acc, train_entropy = 1,2,3,4#train(...)\n",
    "    val_loss, val_conf, val_acc, val_entropy = 5,6,7,8 #evaluate(...)\n",
    "\n",
    "    tqdm.write(  # prints *below* the bar without disrupting it\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}:\\n\"\n",
    "        f\"\\tTrain - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Conf: {train_conf:.4f}, Ent: {train_entropy:.4f}\\n\"\n",
    "        f\"\\tVal -   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Conf: {val_conf:.4f}, Ent: {val_entropy:.4f}\\n\"\n",
    "    )\n",
    "    sleep(2)\n",
    "    # logging.info(\n",
    "    #     f\"Epoch {epoch + 1}/{num_epochs}\\n\"\n",
    "    #     f\"\\tTrain Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Conf: {train_conf:.4f}, Entropy: {train_entropy:.4f}\\n\"\n",
    "    #     f\"\\tVal   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Conf: {val_conf:.4f}, Entropy: {val_entropy:.4f}\"\n",
    "    # )\n",
    "\n",
    "progress_bar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(\"A\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
